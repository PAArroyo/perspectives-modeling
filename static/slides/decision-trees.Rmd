---
title: "Decision trees"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(tidymodels)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(patchwork)
library(titanic)
library(rcfss)
library(pROC)
library(gbm)
library(gganimate)
library(magrittr)
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
library(iml)

# to get the tree graphs with the labels and values, use the forked
# version of ggdendro
# devtools::install_github("bensoltoff/ggdendro")
library(ggdendro)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Decision trees

.center[

![:scale 75%](https://s-media-cache-ak0.pinimg.com/originals/7a/89/ff/7a89ff67b4ce34204c23135cbf35acfa.jpg)

]


---

# Decision trees

.center[

![](https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700)

]

---

# Decision trees

.center[

![:scale 38%](https://s-media-cache-ak0.pinimg.com/564x/0b/87/df/0b87df1a54474716384f8ec94b52eab9.jpg)

]

---

# Decision trees

* Regression and classification
* Split observations into regions of feature space
* Predictions based on the mean or mode of the training observations in that region

---

# Single predictor

```{r auto-lm, echo = FALSE}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(data = Auto, aes(y = mpg), alpha = .2) +
  geom_smooth(method = "lm") +
  labs(title = "Linear model of highway mileage",
       x = "Horsepower",
       y = "Highway mileage")
```

---

# Stratification

1. Divide the predictor space $X_1, X_2, \dots, X_p$ into $J$ distinct and non-overlapping regions $R_1, R_2, \dots, R_J$.
1. For every observation in region $R_j$, make the same prediction which is the mean of the response variable $Y$ for all observations in $R_j$

* Iterative process

---

# Stratification

```{r part-tree-data, include = FALSE}
# hackish function to get line segment coordinates for ggplot
partition.tree.data <- function (tree, label = "yval", add = FALSE, ordvars, ...) 
{
  ptXlines <- function(x, v, xrange, xcoord = NULL, ycoord = NULL, 
                       tvar, i = 1L) {
    if (v[i] == "<leaf>") {
      y1 <- (xrange[1L] + xrange[3L])/2
      y2 <- (xrange[2L] + xrange[4L])/2
      return(list(xcoord = xcoord, ycoord = c(ycoord, y1, 
                                              y2), i = i))
    }
    if (v[i] == tvar[1L]) {
      xcoord <- c(xcoord, x[i], xrange[2L], x[i], xrange[4L])
      xr <- xrange
      xr[3L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[1L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else if (v[i] == tvar[2L]) {
      xcoord <- c(xcoord, xrange[1L], x[i], xrange[3L], 
                  x[i])
      xr <- xrange
      xr[4L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[2L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else stop("wrong variable numbers in tree.")
  }
  if (inherits(tree, "singlenode")) 
    stop("cannot plot singlenode tree")
  if (!inherits(tree, "tree")) 
    stop("not legitimate tree")
  frame <- tree$frame
  leaves <- frame$var == "<leaf>"
  var <- unique(as.character(frame$var[!leaves]))
  if (length(var) > 2L || length(var) < 1L) 
    stop("tree can only have one or two predictors")
  nlevels <- sapply(attr(tree, "xlevels"), length)
  if (any(nlevels[var] > 0L)) 
    stop("tree can only have continuous predictors")
  x <- rep(NA, length(leaves))
  x[!leaves] <- as.double(substring(frame$splits[!leaves, "cutleft"], 
                                    2L, 100L))
  m <- model.frame(tree)
  if (length(var) == 1L) {
    x <- sort(c(range(m[[var]]), x[!leaves]))
    if (is.null(attr(tree, "ylevels"))) 
      y <- frame$yval[leaves]
    else y <- frame$yprob[, 1L]
    y <- c(y, y[length(y)])
    if (add) {
      # lines(x, y, type = "s", ...)
    }
    else {
      a <- attributes(attr(m, "terms"))
      yvar <- as.character(a$variables[1 + a$response])
      xo <- m[[yvar]]
      if (is.factor(xo)) 
        ylim <- c(0, 1)
      else ylim <- range(xo)
      # plot(x, y, ylab = yvar, xlab = var, type = "s", ylim = ylim,
      #      xaxs = "i", ...)
    }
    tibble(x = x, y = y)
  }
  else {
    if (!missing(ordvars)) {
      ind <- match(var, ordvars)
      if (any(is.na(ind))) 
        stop("unmatched names in vars")
      var <- ordvars[sort(ind)]
    }
    lab <- frame$yval[leaves]
    if (is.null(frame$yprob)) 
      lab <- format(signif(lab, 3L))
    else if (match(label, attr(tree, "ylevels"), nomatch = 0L)) 
      lab <- format(signif(frame$yprob[leaves, label], 
                           3L))
    rx <- range(m[[var[1L]]])
    rx <- rx + c(-0.025, 0.025) * diff(rx)
    rz <- range(m[[var[2L]]])
    rz <- rz + c(-0.025, 0.025) * diff(rz)
    xrange <- c(rx, rz)[c(1, 3, 2, 4)]
    xcoord <- NULL
    ycoord <- NULL
    xy <- ptXlines(x, frame$var, xrange, xcoord, ycoord, 
                   var)
    xx <- matrix(xy$xcoord, nrow = 4L)
    yy <- matrix(xy$ycoord, nrow = 2L)

    return(list(tibble(xmin = xx[1L,],
                           ymin = xx[2L,],
                           xmax = xx[3L,],
                           ymax = xx[4L,]),
                tibble(x = yy[1L,],
                           y = yy[2L,],
                           label = lab)))
    # if (!add) 
    #   plot(rx, rz, xlab = var[1L], ylab = var[2L], type = "n", 
    #        xaxs = "i", yaxs = "i", ...)
    # segments(xx[1L, ], xx[2L, ], xx[3L, ], xx[4L, ])
    # text(yy[1L, ], yy[2L, ], as.character(lab), ...)
  }
}
```

```{r auto-tree2, echo = FALSE, fig.height = 6}
# estimate model
auto_tree <- tree(mpg ~ horsepower, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))

mod <- prune.tree(auto_tree, best = 2)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_vline(data = partition.tree.data(mod), aes(xintercept = x), linetype = 2) +
  coord_cartesian(xlim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  ylim = c(min(Auto$mpg), max(Auto$mpg)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
ptree +
  preg +
  plot_annotation(title = str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)))
```

* Terminal node/leaf
* Internal node
* Branch

---

# Stratification

```{r auto-tree3, echo = FALSE}
mod <- prune.tree(auto_tree, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_vline(data = partition.tree.data(mod), aes(xintercept = x), linetype = 2) +
  coord_cartesian(xlim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  ylim = c(min(Auto$mpg), max(Auto$mpg)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
ptree +
  preg +
  plot_annotation(title = str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)))
```

---

# Stratification

```{r auto-treeall, echo = FALSE, message = FALSE}
mod <- auto_tree

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_smooth(data = partition.tree.data(mod), aes(x, y), se = FALSE) +
  coord_cartesian(xlim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  ylim = c(min(Auto$mpg), max(Auto$mpg)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
ptree +
  preg +
  plot_annotation(title = str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)))
```

```{r auto-treeall-mse, dependson = "auto-tree2", include = FALSE}
mse_lm <- glm(mpg ~ horsepower, data = Auto) %>%
  augment() %>%
  mse(truth = mpg, estimate = .fitted) %$%
  .estimate[[1]]

mse_tree <- Auto %>%
  mutate(.fitted = predict(auto_tree)) %>%
  mse(truth = mpg, estimate = .fitted) %$%
  .estimate[[1]]
```

---

# Multiple predictors

```{r auto-tree-weight, echo = FALSE}
auto_tree <- tree(mpg ~ horsepower + weight, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))

mod <- prune.tree(auto_tree, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
ptree +
  preg +
  plot_annotation(title = str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)))
```

---

# Stratification

.pull-left[

```{r auto-tree-weight-i-tree, echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
tree_mods <- tibble(
  n_nodes = seq(from = 3, to = 30)
) %>%
  mutate(mod = map(n_nodes, ~ prune.tree(auto_tree, best = .x)),
         tree_data = map(mod, dendro_data),
         segment = map(tree_data, segment),
         label = map(tree_data, label),
         leaf_label = map(tree_data, leaf_label),
         tree_data = map(mod, partition.tree.data),
         tree_data1 = map(tree_data, ~ .x[[1]]),
         tree_data2 = map(tree_data, ~ .x[[2]]))

# plot the tree
## extract all the necessary elements
tree_segments <- unnest(tree_mods, segment)
tree_labels <- unnest(tree_mods, label)
tree_leaf_labels <- unnest(tree_mods, leaf_label)

ggplot(tree_segments, aes(group = n_nodes)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = tree_labels, 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = tree_leaf_labels, 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  ggtitle("Terminal nodes = {previous_state}") +
  theme_dendro() +
  transition_states(n_nodes,
                    transition_length = .05,
                    state_length = .2) +
  enter_fade() +
  exit_fade()
```

]

.pull-right[

```{r auto-tree-weight-i-feat-space, dependson = "auto-tree-weight-i-tree", echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
# plot the feature space
## extract all the necessary elements
tree_data1 <- unnest(tree_mods, tree_data1)
tree_data2 <- unnest(tree_mods, tree_data2)

ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = tree_data1,
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax, group = n_nodes)) +
  geom_text(data = tree_data2,
            aes(x = x, y = y, label = label, group = n_nodes)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1)) +
  ggtitle("Terminal nodes = {previous_state}") +
  transition_states(n_nodes,
                    transition_length = .05,
                    state_length = .2) +
  enter_fade() +
  exit_fade()
```

]

---

# Estimation procedure

$$\min \left\{ \sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2 \right\}$$

* Classification and regression trees (CART)
* Recursive binary strategy
* Top-down approach
* Greedy partitioning

$$R_1(j, s) = \{ X | X_j < s \} \, \text{and} \, R_2(j,s) = \{X | X_j \geq s \}$$

$$\min \left\{ SSE = \sum_{i \in R_1} (y_i - \hat{y}_{R_1})^2 + \sum_{i \in R_2} (y_i - \hat{y}_{R_2})^2 \right\}$$

* Proceed until stopping criteria reached
* Generating predictions

---

# Pruning the tree

.center[

![](https://na.rdcpix.com/1162715347/e7baeacffee094b5d79c2ec1708c500ew-c0xd-w685_h860_q80.jpg)

]

---

# Pruning the tree

* Tree size = tuning parameter
* Cost complexity pruning
    * Grow a large tree $T_0$
    * Select an optimal subtree

$$\min \left\{ \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_M})^2 + \alpha |T| \right\}$$

* $L_1$ norm penalty

---

# `horsepower + weight`

```{r auto-tree-default, echo = FALSE}
auto_tree <- tree(mpg ~ horsepower + weight, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))
mod <- auto_tree

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
ptree +
  preg +
  plot_annotation(title = str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)))
```

---

# `horsepower + weight`

```{r auto-tree-default-prune, echo = FALSE}
# generate 10-fold CV trees
auto_cv <- vfold_cv(Auto, v = 10) %>%
  mutate(tree = map(splits, ~ tree(mpg ~ horsepower + weight, data = analysis(.x),
                                  control = tree.control(nobs = nrow(Auto),
                                                         mindev = 0))))

# calculate each possible prune result for each fold
auto_cv_prune <- expand.grid(auto_cv$id, 2:30) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(id = Var1,
         k = Var2) %>%
  left_join(auto_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         estimate = map2(prune, splits, ~ predict(.x, newdata = assessment(.y))),
         truth = map(splits, ~ assessment(.x)$mpg)) %>%
  unnest(estimate, truth) %>%
  group_by(k) %>%
  mse(truth = truth, estimate = estimate)

ggplot(auto_cv_prune, aes(k, .estimate)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = auto_cv_prune$k[[which.min(auto_cv_prune$.estimate)]],
             linetype = 2) +
  labs(x = "Number of terminal nodes",
       y = "10-fold CV MSE")
```

---

# `horsepower + weight`

```{r auto-tree-best, dependson = "auto-tree-default-prune", echo = FALSE}
mod <- prune.tree(auto_tree, best = auto_cv_prune$k[[which.min(auto_cv_prune$.estimate)]])

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
ptree +
  preg +
  plot_annotation(title = str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)))
```

---

# Classification trees

* Same thing, different outcome
* Predictions
* Class proportions
* Cost functions

---

# Cost functions

* Classification error rate

    $$E = 1 - \max_{k}(\hat{p}_{mk})$$

* Gini index

    $$G = \sum_{k = 1}^k \hat{p}_{mk} (1 - \hat{p}_{mk})$$

* Cross-entropy

    $$D = - \sum_{k = 1}^K \hat{p}_{mk} \log(\hat{p}_{mk})$$

---

# Titanic example

```{r titanic_tree, echo = FALSE}
titanic <- titanic_train %>%
  as_tibble() %>%
  mutate(Survived = factor(Survived, levels = 0:1, labels = c("Died", "Survived")),
         Female = factor(Sex, levels = c("male", "female")))

# estimate model
titanic_tree <- tree(Survived ~ Age + Female, data = titanic,
                     control = tree.control(nobs = nrow(titanic),
                            mindev = .001))

# plot unpruned tree
mod <- titanic_tree

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender")
```

---

# Titanic example

```{r titanic-tree-prune, dependson="titanic-tree", echo = FALSE, warning = FALSE, message = FALSE}
set.seed(11)
# generate 10-fold CV trees
titanic_cv <- titanic %>%
  na.omit(Survived, Age, Female) %>%
  vfold_cv(v = 10) %>%
  mutate(tree = map(splits, ~ tree(Survived ~ Age + Female, data = analysis(.x),
                                   control = tree.control(nobs = nrow(titanic),
                                                          mindev = .001))))

# calculate each possible prune result for each fold
titanic_cv_prune <- expand.grid(titanic_cv$id, 2:30) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(id = Var1,
         k = Var2) %>%
  left_join(titanic_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         estimate = map2(prune, splits, ~ predict(.x, newdata = assessment(.y))[,2]),
         truth = map(splits, ~ assessment(.x)$Survived)) %>%
  unnest(estimate, truth) %>%
  group_by(k) %>%
  accuracy(truth = truth, estimate = factor(round(estimate), levels = 0:1, labels = c("Died", "Survived")))

ggplot(titanic_cv_prune, aes(k, 1 - .estimate)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = titanic_cv_prune$k[[which.max(titanic_cv_prune$.estimate)]],
             linetype = 2) +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender",
       x = "Number of terminal nodes",
       y = "10-fold CV error rate")
```

---

# Titanic example

```{r titanic-tree-best, dependson="titanic-tree-prune"}
mod <- prune.tree(titanic_tree, best = titanic_cv_prune$k[[which.max(titanic_cv_prune$.estimate)]])

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender")
```

---

# Node purity

## Males older than or equal to 13 on the Titanic

```{r titanic-m-13}
titanic_m13 <- titanic %>%
  filter(Female == "male", Age >= 13) %>%
  count(Survived)

knitr::kable(titanic_m13,
             format = "html",
             col.names = c("Outcome", "Number of training observations"))
```

--

## Males older than or equal to 13 on the Titanic

```{r titanic-m-13-split}
titanic %>%
  filter(Female == "male", Age >= 13) %>%
  mutate(age25 = Age < 24.75) %>%
  count(age25, Survived) %>%
  complete(age25, Survived, fill = list(n = 0)) %>%
  knitr::kable(format = "html",
               col.names = c("Less than 24.75 years old", "Outcome",
                             "Number of training observations"))
```

---

# Trees vs. regression

* Linear regression

    $$f(X) = \beta_0 + \sum_{j = 1}^p X_j \beta_j$$

* Decision trees

    $$f(X) = \sum_{m = 1}^M c_m \cdot 1_{X \in R_m}$$

---

# Categorical predictors

* Splitting categorical predictors
* $q$ unordered values
    * $2^{q - 1} - 1$ possible partitions
* No one-hot encoding

--

* Order the predictor classes according to the proportion falling into outcome class 1
* Split the predictor as if it were an ordered predictor
* Only works for binary classification

---

# Missing values

* With regression models
    1. Discard the observation
    1. Impute the missing value
* With decision trees
    1. Treat missing values as if they are another unique category
    1. Construct surrogate variables

---

# Connection to MARS

* Modification of MARS procedure
* Replace the piecewise linear basis functions with step functions $I(x - t > 0)$ and $I(x - t \leq 0)$
* When a model term is involved in a multiplication by a candidate term, it gets replaced by the interaction and is not available for further interactions

---

# Decision trees

## Benefits

* Easy to explain
* Easy to visualize
* Adept handling of categorical predictors

## Drawbacks

* Cannot accurately capture additive structures
* Lower accuracy rates
* Low-bias, but high-variance

---

# Decision trees

```{r auto-tree-val, echo = FALSE, message = FALSE}
auto_val_test <- function(){
  # split data
  auto_split <- initial_split(Auto, prop = 0.7)
  
  # estimate model
  val <- tree(mpg ~ horsepower + weight, data = analysis(auto_split))
  
  # estimate test mse
  assessment(auto_split) %>%
    mutate(estimate = predict(val, newdata = assessment(auto_split))) %>%
    mse(truth = mpg, estimate = estimate)
}

# repeat the procedure 1000 times
auto_val_mse <- rerun(.n = 1000, auto_val_test()) %>%
  bind_rows

ggplot(auto_val_mse, aes(.estimate)) +
  geom_histogram() +
  geom_vline(xintercept = mean(auto_val_mse$.estimate)) +
  labs(title = "1000 random training/validation set partitions",
       x = "Validation set MSE",
       y = "Frequency count")
```

---

# House sale price

```{r ames, include = FALSE}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

```{r ames-basic, dependson = "ames"}
m1 <- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = "anova"
  )

rpart.plot(m1)
```

---

# Tuning the number of nodes

```{r ames-tree-tune, dependson = "ames-basic"}
plotcp(m1)
```

---

# House sale price

```{r ames-tree-tune-full, dependson = "ames-basic"}
m2 <- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(m2)
abline(v = 12, lty = "dashed")
```

---

# Other tuning parameters

* Cost complexity
* Minimum number of data points to attempt a split
* Maximum number of internal nodes

---

# Other tuning parameters

```{r hypergrid}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)
```

```{r ames-tuning-grid, dependson = c("ames", "hypergrid")}
models <- list()

for (i in 1:nrow(hyper_grid)) {
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  # train a model and store in the list
  models[[i]] <- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}
```

```{r ames-tuning-extract-errors, dependson = "ames-tuning-grid"}
# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}

hyper_grid %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
    ) %>%
  arrange(error) %>%
  top_n(-10, wt = error)
```

--

## Test RMSE

```{r ames-tree-optimal, dependson = "ames"}
optimal_tree <- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova",
    control = list(minsplit = 11, maxdepth = 8, cp = 0.01)
    )

pred <- predict(optimal_tree, newdata = ames_test)
RMSE(pred = pred, obs = ames_test$Sale_Price)
```

---

# Bagging

* High variance estimates and consistency
* Low variance estimates and consistency
* Bootstrap aggregating (bagging)

---

# Bootstrap

* Repeatedly sample with replacement from a sample
* Estimate a parameter from each bootstrap sample
* Average accross all the bootstrap samples
* Reduces the variance $\sigma^2$
--

* Estimate $\hat{f}^1(x), \hat{f}^2(x), \dots, \hat{f}^B(x)$ using $B$ separate training sets

    $$\hat{f}_{\text{avg}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^b(x)$$

    $$\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^b(x)$$
    
* Trees grown without pruning
* Assigning predictions

---

# Out-of-bag observations

```{r boot-prop}
# generate sample index
samp <- tibble(x = seq.int(1000))

# generate bootstrap sample and count proportion of observations in each draw
prop_drawn <- bootstraps(samp, times = nrow(samp)) %>%
  mutate(strap = map(splits, analysis)) %>%
  unnest(strap) %>%
  mutate(drawn = TRUE) %>%
  complete(id, x, fill = list(drawn = FALSE)) %>%
  distinct %>%
  group_by(x) %>%
  mutate(n_drawn = cumsum(drawn),
         id = parse_number(id),
         n_prop = n_drawn / id)

ggplot(prop_drawn, aes(id, n_prop, group = x)) +
  geom_line(alpha = .05) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "b-th bootstrap sample ",
       y = "Proportion i-th observation in samples 1:b")
```

---

# Out-of-bag error estimate

* Generate bagged predictions for each observation $i$ using only its OOB estimates
* Average across all $i$ observations
* Valid estimate of the test error

---

# Out-of-bag error estimate

```{r titanic}
titanic <- titanic_train %>%
  as_tibble() %>%
  mutate(Survived = factor(Survived, levels = 0:1, labels = c("Died", "Survived")),
         Female = factor(Sex, levels = c("male", "female")))
```

```{r titanic-bag-oob, dependson = "titanic"}
titanic_rf_data <- titanic %>%
    select(-Name, -Ticket, -Cabin, -Sex, -PassengerId) %>%
    mutate_each(list(~ as.factor(.)), Pclass, Embarked) %>%
    na.omit

(titanic_bag <- randomForest(Survived ~ ., data = titanic_rf_data,
                             mtry = 7, ntree = 500))
```

---

# Out-of-bag error estimate

## Estimation time for OOB error rate

```{r titanic-bag-oob-time}
system.time({
  randomForest(Survived ~ ., data = titanic_rf_data,
                              mtry = 7, ntree = 500)
})
```

## Estimation time for $10$-fold CV error rate

```{r titanic-bag-cv-time}
system.time({
  vfold_cv(titanic_rf_data, v = 10) %>%
    mutate(model = map(splits, ~ randomForest(x = select(analysis(.x), -Survived),
                                              y = analysis(.x)$Survived,
                                              mtry = 7, ntree = 500)),
           estimate = map2(model, splits, ~predict(.x, newdata = assessment(.y))),
           truth = map(splits, ~ assessment(.x)$Survived)) %>%
    unnest(truth, estimate) %>%
    group_by(id) %>%
    accuracy(truth = truth, estimate = estimate) %>%
    summarize(mean(.estimate))
})
```

---

# Ames housing model

```{r ames-bag, dependson = "ames"}
# make bootstrapping reproducible
set.seed(123)

# train bagged model
bagged_m1 <- bagging(
  formula = Sale_Price ~ .,
  data    = ames_train,
  coob    = TRUE
)

bagged_m1
```

---

# Optimal number of trees

```{r ames-bag-ntrees, dependson = "ames"}
# assess 10-50 bagged trees
ntree <- 10:50

# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(123)
  
  # perform bagged model
  model <- bagging(
    formula = Sale_Price ~ .,
    data    = ames_train,
    coob    = TRUE,
    nbagg   = ntree[i]
  )
  # get OOB error
  rmse[i] <- model$err
}

tibble(
  ntree = ntree,
  rmse = rmse
) %>%
  ggplot(aes(ntree, rmse)) +
  geom_line() +
  geom_vline(xintercept = 25, linetype = 2) +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Number of trees",
       y = "RMSE")
```

---

# Feature importance

```{r ames-bag-caret, dependson = "ames"}
# Specify 10-fold cross validation
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  trControl = ctrl
  )
```

```{r ames-bag-predictor, dependson = "ames-bag-caret"}
predictor_bag <- Predictor$new(
  model = bagged_cv,
  data = select(ames_train, - Sale_Price),
  y = ames_train$Sale_Price
)
```

```{r ames-bag-feat-imp, dependson = "ames-bag-predictor"}
feat_imp_bag <- FeatureImp$new(predictor_bag, loss = "mse")
```

```{r ames-bag-feat-imp-plot, dependson = "ames-bag-feat-imp", fig.asp = .7}
plot(feat_imp_bag) +
  ggtitle("Bagging") +
  theme_minimal(base_size = 10)
```

---

# Partial dependence plots

```{r ames-bag-pdp, dependson = "ames-bag-predictor"}
pdp_Gr_Liv_Area <- FeatureEffect$new(predictor_bag,
                                     "Gr_Liv_Area",
                                     method = "pdp+ice",
                                     center.at = min(ames_train$Gr_Liv_Area),
                                     grid.size = 50)
pdp_Year_Built <- FeatureEffect$new(predictor_bag,
                                    "Year_Built",
                                    method = "pdp+ice",
                                    center.at = min(ames_train$Year_Built),
                                    grid.size = 50)
pdp_Exter_Qual <- FeatureEffect$new(predictor_bag,
                                    "Exter_Qual",
                                    method = "pdp+ice")

p1 <- plot(pdp_Gr_Liv_Area) +
  theme_minimal(base_size = 12)
p2 <- plot(pdp_Year_Built) +
  theme_minimal(base_size = 12)
p3 <- plot(pdp_Exter_Qual) +
  theme_minimal(base_size = 12)

p1 + p2 + p3
```













