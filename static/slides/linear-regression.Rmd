---
title: "Linear Regression"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2)

library(tidyverse)
library(here)
library(broom)
library(modelr)
library(knitr)
library(patchwork)
library(plotly)

set.seed(1234)
theme_set(theme_minimal(base_size = 20))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Linear regression

* Assumes $\E(Y|X)$ is linear in the inputs $X_1, \ldots, X_p$
* Popular for a multitude of reasons
    * Simple to interpret
    * Easy(ish) to estimate
    * Highly adaptable, even to non-linear relationships

---

# Key concepts

* Simple linear regression model

    $$\E(Y|X) = \beta_0 + \beta_1 X$$
--

* Further assume that $\Var (\epsilon_i | X = x) = \sigma^2$ does not depend on $x$

    $$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

    * $\E (\epsilon_i | X_i) = 0$
    * $\Var (\epsilon_i | X_i) = \sigma^2$
    * $\beta_0\, \beta_1, \sigma^2$

---

# Key concepts

* Let $\hat{\beta}_0$ and $\hat{\beta}_1$ denote estimates of $\beta_0$ and $\beta_1$
* Fitted line

    $$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$$

* Fitted/predicted values

    $$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$

* Residuals

    $$\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X)$$

---

# Residual sum of squares (RSS)

$$
\begin{align}
\text{RSS}(\beta) &= \sum_{i=1}^N \hat{\epsilon}_i^2 \\
&= \sum_{i=1}^N (Y_i - f(X_i))^2 \\
&= \sum_{i=1}^N \left(Y_i - \beta_0 - \sum_{j=1}^p X_{ij} \beta_j\right)^2
\end{align}
$$

---

class: center

# Estimating $\hat{\beta}$

```{r sim-plot}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

---

class: center

# Estimating $\hat{\beta}$

```{r sim-random-fit}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

---

# Estimating $\hat{\beta}$

* Classically desired properties
* Unbiased
    * $E(\hat{\beta}) = \beta$
    * Estimator that "gets it right" vis-a-vis $\beta$
* Efficient
    * $\min(Var(\hat{\beta}))$
    * Not only do we get it right, but for any given sample used to generate the model we never want to be too far off from "right"

---

# Least squares estimator

* Values $\hat{\beta}_0, \hat{\beta}_1$ that minimize the RSS

    $$\min(\text{RSS})$$

---

# Least squares estimator

$$
\begin{aligned}
\text{RSS} &= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
\sum_{i=1}^n (\hat{\epsilon}_i)^2 &= \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i))^2\\
f(\beta_0, \beta_1 | x_i, y_i) & = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i )^2
\end{aligned}
$$

---

# Least squares estimator

$$
\begin{aligned}
\dfrac{\partial{ f(\beta_0, \beta_1 | x_i, y_i)}}{\partial \beta_0} & = -2 (\sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i))\\
& = \sum_{i=1}^n -2Y_i + 2\beta_0 + 2\beta_1 X_i\\
0 & = \sum_{i=1}^n -2Y_{i} + 2\beta_0 + 2\beta_1 X_i\\
0 & = -2 \sum_{i=1}^n Y_{i} +  2\sum_{i=1}^n \beta_0 +  2\beta_1 \sum_{i=1}^n X_i\\
0 & = -2 \sum_{i=1}^n Y_{i} +  (n \times 2\beta_0) +  2\beta_1 \sum_{i=1}^n X_i\\
n \times 2\beta_0 & = 2 \sum_{i=1}^n Y_i - 2\beta_1 \sum_{i=1}^n X_i\\
\hat \beta_0 & = \dfrac{2 \sum_{i=1}^n Y_i}{2n} - \dfrac{2\beta_1 \sum_{i=1}^n X_i}{2n}\\
& =  \dfrac{\sum_{i=1}^n Y_i}{n} - \beta_1\dfrac{ \sum_{i=1}^n X_i}{n}\\
 \hat \beta_0 & = \bar{Y}_n - \beta_1 \bar{X}_n
\end{aligned}
$$

---

# Least squares estimator

$$
\begin{aligned}
\dfrac{\partial{ f(\beta_0, \beta_1 | x_i, y_i)}}{\partial \beta_1} & = \sum_{i=1}^n -2X_i(Y_i - \beta_0 - \beta_1 X_i) \\
& =  \sum_{i=1}^n -2Y_iX_i + 2\beta_0X_i + 2\beta_1 X_i^2\\
0 & =  \sum_{i=1}^n -2Y_iX_i + 2\beta_0 \sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
& =  \sum_{i=1}^n -2Y_iX_i + 2 (\bar{Y}_n - \beta_1 \bar{X}_n) \sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
& = \sum_{i=1}^n -2Y_iX_i + 2\bar{Y}_n \sum_{i=1}^nX_i - 2\beta_1 \bar{X}_n\sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
2\beta_1  \sum_{i=1}^n X_i^2 - 2\beta_1 \bar{X}_n\sum_{i=1}^nX_i  & = \sum_{i=1}^n 2Y_iX_i  - 2\bar{Y}_n \sum_{i=1}^nX_i\\
\beta_1 ( \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i ) & = \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i\\
\hat \beta_1 & = \dfrac{ \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i}{ \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i}\\
 \hat \beta_0 & = \bar{Y}_n - \hat{\beta}_1 \bar{X}_n
\end{aligned}
$$

$$\hat{\sigma}^2 = \left( \frac{1}{n - 2} \right) \sum_{i=1}^n \hat{\epsilon}_i^2$$

---

class: center

# Least squares estimator

```{r sim-lm}
sim1_mod <- lm(y ~ x, data = sim1)

dist2 <- sim1 %>%
  add_predictions(sim1_mod) %>%
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge
  )

ggplot(dist2, aes(x1, y)) + 
  geom_smooth(method = "lm", color = "grey40", se = FALSE) +
  geom_point(color = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), color = "#3366FF")
```

---

# Gauss-Markov theorem

* Among all linear unbiased estimates, the least squares estimate of $\beta$ will have the smallest variance
* Best Linear Unbiased Estimator (BLUE)

---

# Gauss-Markov theorem

$$
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2} \\
&= \frac{\sum (X_i - \bar{X})Y_i}{\sum (X_i - \bar{X})^2} - \frac{\bar{Y}(X_i - \bar{X})}{\sum (X_i - \bar{X})^2} \\
&= \frac{\sum (X_i - \bar{X})Y_i}{\sum (X_i - \bar{X})^2} \\
&= \sum k_i Y_i
\end{aligned}
$$

---

# Gauss-Markov theorem

* Consider now a new weighting function $w_i$:

    $$\tilde{\beta}_1 = \sum w_i Y_i$$

* What happens to $\hat{\beta}_1$?

$$
\begin{align}
\E(\tilde{\beta}_1) &= \sum w_i \E(Y_i) \\
&= \sum w_i (\beta_0 + \beta_1 X_i) \\
&= \beta_0 \sum w_i + \beta_1 \sum w_i X_i
\end{align}
$$

---

# Do we want this?

$$
\begin{align}
\text{MSE} (\tilde{\theta}) &= \E (\tilde{\theta} - \theta)^2 \\
&= \Var (\tilde{\theta}) + [\E (\tilde{\theta}) - \theta]^2 + \text{Irreducible error}
\end{align}
$$

---

# Extensions

* Additive assumption
* Linearity assumption

---

class: center

# Removing the additive assumption

$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \epsilon_i$$

```{r sim-linear}
n_sim <- 100

additive <- tibble(x = runif(n_sim),
                       z = runif(n_sim)) %>%
  mutate(z = ifelse(z > .5, 1, 0),
         y = x + z + rnorm(n_sim))

additive %>%
  add_predictions(lm(y ~ x + z, data = .)) %>%
  ggplot(aes(x, y, color = factor(z))) +
  geom_point() +
  geom_line(aes(y = pred)) +
  theme(legend.position = "none")
```

---

# Additive model

$$\E[Y] = \beta_0 + \beta_1 X + \beta_2 Z$$

$$\frac{\delta \E[Y]}{\delta X} = \beta_1$$

$$\frac{\delta \E[Y]}{\delta Z} = \beta_2$$

---

# Multiplicative interaction model

$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 XZ + \epsilon_i$$

* Direct effects
* Constitutive terms
* Interaction term

---

# Multiplicative interaction model

$$
\begin{align}
\E[Y] & = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 XZ \\
     & = \beta_0 + \beta_2 Z + (\beta_1 + \beta_3 Z) X
\end{align}
$$

$$\frac{\delta \E[Y]}{\delta X} = \beta_1 + \beta_3 Z$$

$$\E[Y] = \beta_0 + \beta_2 Z + \psi_1 X$$

$$
\begin{align}
\E[Y] & = \beta_0 + \beta_1 X + (\beta_2 + \beta_3 X) Z \\
     & = \beta_0 + \beta_1 X + \psi_2 Z
\end{align}
$$

---

# Multiplicative interaction model

* Conditional impact
* If $Z = 0$, then:
    
$$
\begin{align}
\E[Y] & = \beta_0 + \beta_1 X + \beta_2 (0) + \beta_3 X (0) \\
     & = \beta_0 + \beta_1 X
\end{align}
$$
        
* If $X = 0$, then:
    
$$
\begin{align}
\E[Y] & = \beta_0 + \beta_1 (0) + \beta_2 Z + \beta_3 (0) Z \\
     & = \beta_0 + \beta_2 Z
\end{align}
$$
* $\psi_1 = \beta_1$ and $\psi_2 = \beta_2$
* $+\beta_3$ and $-\beta_3$
* $\psi_1$ and $\psi_2$

---

# Conducting inference

* Obtaining estimates of parameters

    $$\hat{\psi}_1 = \hat{\beta}_1 + \hat{\beta}_3 Z$$
    $$\hat{\psi}_2 = \hat{\beta}_2 + \hat{\beta}_3 X$$
    
* Obtaining estimates of standard errors

---

# Conducting inference

1. $\text{Var}(aX) = a^2 \text{Var}(X)$
1. $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X,Y)$
1. $\text{Cov}(X, aY) = a \text{Cov}(X,Y)$

--

$$\widehat{\text{Var}(\hat{\psi}_1}) = \widehat{\text{Var} (\hat{\beta}_1)} +Z^2  \widehat{\text{Var} (\hat{\beta}_3)} + 2 Z \widehat{\text{Cov} (\hat{\beta}_1, \hat{\beta}_3)}$$

$$\widehat{\text{Var}(\hat{\psi}_2}) = \widehat{\text{Var} (\hat{\beta}_2)} + X^2  \widehat{\text{Var} (\hat{\beta}_3)} + 2 X \widehat{\text{Cov} (\hat{\beta}_2, \hat{\beta}_3)}$$

* Depend on $\beta_1$, $\beta_2$, and/or $\beta_3$
* Both also depend on the level/value of the interacted variable

---

# Two dichtomous covariates

$$Y = \beta_0 + \beta_1 D_1 + \beta_2 D_2 + \beta_3 D_1 D_2 + \epsilon_i$$

$$
\begin{align}
\E[Y | D_1 = 0, D_2 = 0] & = \beta_0 \\
\E[Y | D_1 = 1, D_2 = 0] & = \beta_0 + \beta_1 \\
\E[Y | D_1 = 0, D_2 = 1] & = \beta_0 + \beta_2 \\
\E[Y | D_1 = 1, D_2 = 1] & = \beta_0 + \beta_1 + \beta_2 + \beta_3 \\
\end{align}
$$

---

class: center

# Two dichtomous covariates

```{r sim-two-dich}
two_dich <- tibble(x = runif(n_sim),
                       z = runif(n_sim)) %>%
  mutate_at(vars(x, z), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 10 + 20 * x - 20 * z + 40 * (x * z) + rnorm(n_sim, 0, 5))
```

```{r sim-two-dich-hist}
ggplot(two_dich, aes(y, color = interaction(x, z))) +
  geom_density() +
  scale_color_discrete(labels = c("D1 = D2 = 0",
                                  "D1 = 1, D2 = 0",
                                  "D1 = 0, D2 = 1",
                                  "D1 = D2 = 1"),
                       guide = guide_legend(nrow = 2)) +
  labs(title = expression(paste("Values of ", Y, " for various combinations of values of ", D[1], ",", D[2])),
       x = "Value of Y",
       color = NULL) +
  theme(legend.position = "bottom")
```

---

class: center

# Two dichtomous covariates

```{r sim-two-dich-box}
ggplot(two_dich, aes(interaction(z, x), y)) +
  geom_boxplot() +
  scale_x_discrete(labels = c(expression("D1 = D2 = 0"),
                                  expression("D1 = 0, D2 = 1"),
                                  expression("D1 = 1, D2 = 0"),
                                  expression("D1 = D2 = 1"))) +
  labs(title = expression(paste("Values of ", Y, " for various combinations of values of ", D[1], ",", D[2])),
       x = NULL,
       y = "Values of Y")
```

---

class: center

# One dichotomous and one continuous covariate

$$Y = \beta_0 + \beta_1 X + \beta_2 D + \beta_3 XD + \epsilon_i$$

$$
\begin{align}
\E[Y | X, D = 0] & = \beta_0 + \beta_1 X \\
\E[Y | X, D = 1] & = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X
\end{align}
$$

---

class: center

# One dichotomous and one continuous covariate

```{r sim-cont-dich-00}
tibble(x = runif(n_sim, -2, 2),
           z = runif(n_sim, -2, 2)) %>%
  mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 0 + 20 * x + 0 * z + 0 * (x * z) + rnorm(n_sim, 0, 5)) %>%
  ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
  geom_vline(xintercept = 0, alpha = .3) +
  geom_hline(yintercept = 0, alpha = .3) +
  geom_point() +
  geom_abline(intercept = 0, slope = 20) +
  labs(title = expression({beta[2] == beta[3]} == 0),
       color = "Z",
       shape = "Z") +
  theme(legend.position = "bottom")
```
    
---

class: center

# One dichotomous and one continuous covariate

```{r sim-cont-dich-01}
tibble(x = runif(n_sim, -2, 2),
           z = runif(n_sim, -2, 2)) %>%
  mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 0 + 20 * x + 10 * z + 0 * (x * z) + rnorm(n_sim, 0, 5)) %>%
  ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
  geom_vline(xintercept = 0, alpha = .3) +
  geom_hline(yintercept = 0, alpha = .3) +
  geom_point() +
  geom_abline(intercept = 0, slope = 20) +
  geom_abline(intercept = 10, slope = 20, linetype = 2) +
  labs(title = expression(paste({beta[2] != 0}, ",", beta[3] == 0)),
       color = "Z",
       shape = "Z") +
  theme(legend.position = "bottom")
```

---

class: center

# One dichotomous and one continuous covariate

```{r sim-cont-dich-10}
tibble(x = runif(n_sim, -2, 2),
           z = runif(n_sim, -2, 2)) %>%
  mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 0 + 20 * x + 0 * z + -40 * (x * z) + rnorm(n_sim, 0, 5)) %>%
  ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
  geom_vline(xintercept = 0, alpha = .3) +
  geom_hline(yintercept = 0, alpha = .3) +
  geom_point() +
  geom_abline(intercept = 0, slope = 20) +
  geom_abline(intercept = 0, slope = -20, linetype = 2) +
  labs(title = expression(paste({beta[2] == 0}, ",", beta[3] != 0)),
       color = "Z",
       shape = "Z") +
  theme(legend.position = "bottom")
```
    
---

class: center

# One dichotomous and one continuous covariate

```{r sim-cont-dich-11}
tibble(x = runif(n_sim, -2, 2),
           z = runif(n_sim, -2, 2)) %>%
  mutate_at(vars(z), funs(ifelse(. > .5, 1, 0))) %>%
  mutate(y = 0 + 20 * x + 20 * z + -40 * (x * z) + rnorm(n_sim, 0, 5)) %>%
  ggplot(aes(x, y, color = factor(z), shape = factor(z))) +
  geom_vline(xintercept = 0, alpha = .3) +
  geom_hline(yintercept = 0, alpha = .3) +
  geom_point() +
  geom_abline(intercept = 0, slope = 20) +
  geom_abline(intercept = 20, slope = -20, linetype = 2) +
      labs(title = expression(paste(beta[2] != 0, ",", beta[3] != 0)),
       color = "Z",
       shape = "Z") +
  theme(legend.position = "bottom")
```

---

class: center

# Two continuous covariates

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon_i$$

```{r two-cont}
# no interactive effects
tibble(x = runif(n_sim, 0, 10),
           z = runif(n_sim, 0, 10),
           y = 10 + 10 * x - 10 * z + 0 * x * z) %>%
  plot_ly(x = ~x, y = ~ z, z = ~ y, type = "mesh3d") %>%
  layout(title = "No interactive effects")
```

---

class: center

# Two continuous covariates

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon_i$$

```{r two-cont-x}
# inteactive effects
tibble(x = runif(n_sim, 0, 10),
           z = runif(n_sim, 0, 10),
           y = 10 + 10 * x - 10 * z + 10 * x * z) %>%
  plot_ly(x = ~x, y = ~ z, z = ~ y, type = "mesh3d") %>%
  layout(title = "Interactive effects")
```

---

# Removing the linear assumption

* Linear model

    $$Y_i = \beta_0 + \beta_{1}X_{i} + \epsilon_{i}$$

* Polynomial model

    $$Y_i = \beta_0 + \beta_{1}X_{i} + \beta_{2}X_i^2 + \beta_{3}X_i^3 + \dots + \beta_{d}X_i^d + \epsilon_i$$

---

class: center

# Joe Biden feeling thermometer

```{r biden}
# get data
biden <- read_csv(here("static", "data", "biden.csv"))

# histogram of biden variable
ggplot(biden, aes(biden)) +
  geom_histogram() +
  labs(title = "Biden feeling thermometer",
       x = "Biden thermometer rating",
       y = "Number of observations")
```

---

class: center

# Joe Biden feeling thermometer

$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age}$$

```{r biden-age, fig.height = 7}
# estimate linear model
ggplot(biden, aes(age, biden)) +
  geom_point(alpha = .25) +
  geom_smooth(method = lm) +
  labs(title = "Linear regression of Biden feeling thermometer",
       subtitle = "With 95% confidence interval",
       x = "Age",
       y = "Biden thermometer rating")
```

---

class: center

# Joe Biden feeling thermometer

$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$

```{r biden-poly}
# estimate polynomial model
biden_age <- glm(biden ~ poly(x = age, degree = 4, raw = TRUE), data = biden)

# plot the model results
ggplot(biden, aes(age, biden)) +
  geom_point(alpha = .25) +
  geom_smooth(method = lm, formula = y ~ poly(x = x, degree = 4, raw = TRUE)) +
  labs(title = "Polynomial regression of Biden feeling thermometer",
       subtitle = "With 95% confidence interval",
       x = "Age",
       y = "Biden thermometer rating")
```

---

# Removing the linear assumption

## Pointwise standard error

```{r biden-matrix}
vcov(biden_age) %>%
  kable(caption = "Variance-covariance matrix of Biden polynomial regression",
        digits = 5,
        format='html')
```

---

# Assumptions of linear regression models

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

> The key assumptions of linear regression concern the behavior of the errors

---

# Assumptions of linear regression models

* Linearity
* Constant variance
* Normality
* Independence
* Fixed $X$
* $X$ is not invariant

---

# Linearity

$$\E(\epsilon_i) \equiv E(\epsilon_i | X_i) = 0$$

$$
\begin{aligned}
\mu_i \equiv E(Y_i) \equiv E(Y | X_i) &= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
\mu_i &= \beta_0 + \beta_1 X_i + E(\epsilon_i) \\
\mu_i &= \beta_0 + \beta_1 X_i + 0 \\
\mu_i &= \beta_0 + \beta_1 X_i
\end{aligned}
$$

---

# Constant variance

* The variance of the errors is the same regardless of the values of $X$

    $$\Var(\epsilon_i | X_i) = \sigma^2$$

---

# Normality

* The errors are assumed to be normally distributed

    $$\epsilon_i \mid X_i \sim N(0, \sigma^2)$$

---

# Independence

* Observations are sampled independently from one another
* Any pair of errors $\epsilon_i$ and $\epsilon_j$ are independent for $i \neq j$.

---

# Fixed $X$

* $X$ is assumed to be fixed or measured without error and independent of the error
* Experimental design
* Observational study

    $$\epsilon_i \sim N(0, \sigma^2), \text{for } i = 1, \dots, n$$

---

# $X$ is not invariant

```{r invariant}
tibble(x = 1,
           y = rnorm(10)) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "You cannot regress this",
       subtitle = "Slope is undefined")
```

---

# Handling violations of assumptions

* Ooopsie
* Inference can be
    * Tricky
    * Biased
    * Inefficient
    * Error prone
* Move to more robust inferential method
* Diagnose assumption violations and solve them in the OLS framework
