---
title: "Random Forests/Boosting"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(tidymodels)
library(randomForest)
library(patchwork)
library(rcfss)
library(rpart)
library(rpart.plot)
library(ranger)
library(iml)
library(xgboost)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Limitations of bagging

```{r mpg-bag-trees, dependson = "ames"}
# generate bootstrapped samples
bag_trees <- bootstraps(ISLR::Auto, times = 6) %>%
  mutate(tree = map(splits, ~ rpart(mpg ~ .,
                                    data = analysis(.x) %>%
                                      select(-name),
                                    method = "anova",
                                    model = TRUE)))

par(mfrow = c(2, 3))
for(i in 1:nrow(bag_trees)) {
  rpart.plot(bag_trees$tree[[i]],
             main = str_c("Decision Tree", i, sep = " "))
}
```

---

# Limitations of bagging

* Grow deep trees
* Randomization component
* Lack of independence

---

# Limitations of bagging

* Independently and identically distributed (i.i.d.) random variables

    $$\Var(\overline{\hat{Y}_b}) = \frac{1}{B} \sigma^2$$

* Identically distributed (i.d.) random variables

    $$\Var(\overline{\hat{Y}_b}) = \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2$$

* Limit on variance reduction

---

# Random forests

* Minimize correlation between the trees
* Random forests

1. Bootstrap sampling
1. Split-variable randomization
    * $m = p$
    * $m \leq p$
    * Regression: $\frac{p}{3}$
    * Classification: $\sqrt{p}$
    * Tuning parameter

---

# Random forest

1. For $b=1$ to $B$:
    1. Draw a bootstrap sample $\mathbf{Z}^*$ of size $N$ from the training data
    1. Grow a decision tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree until the minimum node size $n_\text{min}$ is reached:
        1. Select $m$ variables at random from the $p$ predictors
        1. Pick the best variable/split-point among the $m$
        1. Split the node into two child nodes
1. Output the ensemble of trees $\{ T_b \}_1^B$

* Making predictions
* Out-of-bag estimates
* Model interpretation

---

# Default parameters

```{r ames}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

```{r ames-rf, dependson = "ames"}
# for reproduciblity
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)

m1
```

---

# Number of trees

```{r ames-rf-plot, dependson = "ames-rf"}
tibble(
  mse = m1$mse
) %>%
  mutate(
    ntrees = row_number()
  ) %>%
  ggplot(aes(ntrees, mse)) +
  geom_line() +
  geom_vline(xintercept = which.min(m1$mse), linetype = 2) +
  labs(x = "Number of trees grown",
       y = "OOB MSE")
```

---

# OOB vs. validation set

```{r ames-rf-oob-val, dependson = "ames-rf"}
# create training and validation data 
set.seed(123)
valid_split <- initial_split(ames_train, .8)

# training data
ames_train_v2 <- analysis(valid_split)

# validation data
ames_valid <- assessment(valid_split)
x_test <- select(ames_valid, -Sale_Price)
y_test <- ames_valid$Sale_Price

rf_oob_comp <- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train_v2,
  xtest = x_test,
  ytest = y_test
)
```

```{r ames-rf-oob-val-plot, dependson = "ames-rf-oob-val"}
# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation
) %>%
  mutate(ntrees = row_number()) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual") +
  xlab("Number of trees")
```

---

# Tuning parameter options

* Number of trees
* Number of variables to randomly sample at each split
* Sample size for each sample
* Minimum number of observations within the terminal nodes
* Maximum number of terminal nodes

---

# Tuning parameter options

```{r ames-hypergrid}
# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)
```

```{r ames-rf-tune, dependson = "ames"}
for(i in 1:nrow(hyper_grid)) {
  # train model
  model <- ranger(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    num.trees = 500,
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
```

```{r ames-rf-tune-results, dependson = "ames-rf-tune"}
hyper_grid <- hyper_grid %>% 
  arrange(OOB_RMSE)
hyper_grid %>%
  head(10)
```

---

# Final model performance

```{r ames-rf-tune-final, dependson = "ames-rf-tune-results"}
OOB_RMSE <- vector(mode = "numeric", length = 100)

set.seed(123)
for(i in seq_along(OOB_RMSE)) {
  optimal_ranger <- ranger(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    num.trees = 500,
    mtry = hyper_grid$mtry[[1]],
    min.node.size = hyper_grid$node_size[[1]],
    sample.fraction = hyper_grid$sampe_size[[1]]
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}
```

```{r ames-rf-tune-final-plot, dependson = "ames-rf-tune-final"}
tibble(RMSE = OOB_RMSE) %>%
  ggplot(aes(RMSE)) +
  geom_histogram() +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Distribution of OOB RMSE",
       subtitle = "Repeats = 100",
       x = "OOB RMSE",
       y = "Frequency")
```

---

# Feature importance

```{r ames-rf-predictor, dependson = "ames-rf-tune-final"}
pred_ranger <- function(model, newdata){
  results <- predict(model, newdata)
  return(results$prediction)
}

predictor_rf <- Predictor$new(
  model = optimal_ranger,
  data = select(ames_train, - Sale_Price),
  y = ames_train$Sale_Price,
  predict.fun = pred_ranger
)
```

```{r iml-parallel-begin, cache = FALSE}
library(doParallel)

cl <- makePSOCKcluster(3)
registerDoParallel(cl)
```

```{r ames-rf-feat-imp, dependson = "ames-rf-predictor"}
feat_imp_rf <- FeatureImp$new(predictor_rf, loss = "mse", parallel = TRUE)
```

```{r ames-rf-feat-imp-plot, dependson = "ames-rf-feat-imp", fig.asp = .7}
plot(feat_imp_rf) +
  ggtitle("Random forest") +
  theme_minimal(base_size = 8)
```

---

# Partial dependence

```{r ames-rf-pdp, dependson = "ames-rf-predictor"}
pdp_Overall_Qual <- FeatureEffect$new(predictor_rf,
                                      "Overall_Qual",
                                      method = "pdp+ice")
pdp_Gr_Liv_Area <- FeatureEffect$new(predictor_rf,
                                     "Gr_Liv_Area",
                                     method = "pdp+ice",
                                     center.at = min(ames_train$Gr_Liv_Area),
                                     grid.size = 50)
pdp_Year_Built <- FeatureEffect$new(predictor_rf,
                                    "Year_Built",
                                    method = "pdp+ice",
                                    center.at = min(ames_train$Year_Built),
                                    grid.size = 50)

p1 <- plot(pdp_Overall_Qual)
p2 <- plot(pdp_Gr_Liv_Area)
p3 <- plot(pdp_Year_Built)

p1 + p2 + p3
```

---

# Comparison to bagging

```{r bag-compare, dependson = "ames"}
bag_oob <- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train,
  num.trees = 500,
  mtry = ncol(ames_train) - 1,
  nodesize = hyper_grid$node_size[[1]],
  replace = FALSE,
  sampsize = ceiling(hyper_grid$sampe_size[[1]] * nrow(ames_train))
)
```

```{r rf-compare, dependson = "ames-rf-tune-results"}
rf_oob <- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train,
  num.trees = 500,
  mtry = hyper_grid$mtry[[1]],
  nodesize = hyper_grid$node_size[[1]],
  replace = FALSE,
  sampsize = ceiling(hyper_grid$sampe_size[[1]] * nrow(ames_train))
)
```

```{r rf-bag-compare, dependson = c("bag-compare", "rf-compare")}
tibble(
  Bagging = sqrt(bag_oob$mse),
  `Random Forest` = sqrt(rf_oob$mse)
) %>%
  mutate(ntrees = row_number()) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual") +
  xlab("Number of trees")
```

---

# Types of predictive models

* Strong vs. weak models
* Single predictive model
* Ensemble model
    * Parallel construction
    * Sequential construction

---

# Sequential ensemble construction

![](https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png)

---

# Why train weak models?

* Speed
* Accuracy improvement
* Slow learning
* Avoids overfitting

---

# Sequential training using errors

1. Fit a decision tree to the data: $F_1(x) = y$
1. Fit the next decision tree to the residuals of the previous: $h_1(x) = y - \lambda F_1(x)$
1. Add this new tree to the algorithm: $F_2(x) = F_1(x) + \lambda h_1(x)$
1. Fit the next decision tree to the residuals of $F_2$: $h_2(x) = y - \lambda F_2(x)$
1. Add this new tree to the algorithm: $F_3(x) = F_2(x) + \lambda h_1(x)$
1. Continue this process until some mechanism (i.e. cross validation) says to stop

* Stagewise additive model

    $$f(x) =  \sum^B_{b=1} \lambda f^b(x)$$
    
* Learning parameter $\lambda$

---

# Sequential training using errors

```{r seq-learn}
# source: https://github.com/bgreenwell/MLDay18/blob/master/code/rpartBoost.R
rpartBoost <- function(X, y, data, num_trees = 100, learn_rate = 0.1, 
                       tree_depth = 6, verbose = FALSE) {
  require(rpart)
  G_b_hat <- matrix(0, nrow = length(y), ncol = num_trees + 1)
  r <- y
  for (tree in seq_len(num_trees)) {
    if (verbose) {
      message("iter ", tree, " of ", num_trees)
    }
    g_b_tilde <- rpart(r ~ X, control = list(cp = 0, maxdepth = tree_depth))
    g_b_hat <- learn_rate * predict(g_b_tilde)
    G_b_hat[, tree + 1] <- G_b_hat[, tree] + matrix(g_b_hat)
    r <- r - g_b_hat
  }
  colnames(G_b_hat) <- paste0("tree_", c(0, seq_len(num_trees)))
  G_b_hat
}

# Simulate some sine wave data
set.seed(101)
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
sim_data <- tibble(
  x = x,
  y = y
)

# gradient boosted decision trees
bst <- rpartBoost(X = x, y = y, num_trees = 200, learn_rate = 0.1, 
                  tree_depth = 3)

# Plot iterations
bst_gather <- bst %>%
  as_tibble %>%
  bind_cols(sim_data) %>%
  mutate(id = row_number()) %>%
  gather(tree, pred, -id, -x, -y) %>%
  mutate(tree = parse_number(tree))

library(gganimate)

sin_gif <- ggplot(sim_data, aes(x, y)) +
  geom_point(alpha = 0.2) +
  stat_function(aes(color = "True function"), fun = sin, size = 1) +
  geom_line(data = bst_gather,
            aes(x,
                pred,
                color = "Boosted prediction",
                group = tree),
            size = 1) +
  scale_color_brewer(type = "qual") +
  labs(title = "Number of trees = {current_frame}",
       x = expression(X),
       y = expression(Y),
       color = NULL) +
  theme(legend.position = "bottom") +
  transition_manual(tree)

animate(sin_gif, fps = 3, end_pause = 20)
```

---

# Boosting

* Bagging
    * Combine low bias, high variance models
* Random forest
    * Combine low-ish bias, less high variance models
* Boosting
    * Combine high bias, low variance models

---

# Gradient descent

* Loss functions
    * MSE
    * MAE
    * Deviance
    * All differentiable
* How to minimize the loss function
* Gradient boosting

---

# Gradient descent

.center[

![:scale 80%](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0402.png)

]

* Iteratively tweak estimated parameters
* Steepest slope
* Gradient

---

# Learning rate comparisons

.pull-left[

##### Too small

![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0403.png)

]

--

.pull-right[

##### Too large

![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0404.png)

]

---

# Global/local minima

![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0405.png)

* MSE - globally convex

---

# Tuning gradient boosting

* Number of trees
* Depth of trees
* Learning rate
* `xgboost`

---

# `xgboost` model

```{r ames-prep, dependson = "ames"}
# create recipe for one-hot-encoding
ames_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal())

# train the recipe
trained_rec <- prep(ames_rec, training = ames_train)

# generate train/test sets
train_data <- bake(trained_rec, new_data = ames_train)
test_data <- bake(trained_rec, new_data = ames_test)

# separate into features and response
features_train <- select(train_data, -Sale_Price) %>%
  as.matrix
response_train <- train_data$Sale_Price

features_test <- select(test_data, -Sale_Price) %>%
  as.matrix
response_test <- test_data$Sale_Price
```

* Learning rate: 0.3
* Tree depth: 6
* Minimum node size: 1

```{r ames-xgb, dependson = "ames-prep"}
# reproducibility
set.seed(123)

xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0               # silent,
)
```

---

# `xgboost` model

```{r ames-xgb-err, dependson = "ames-xgb"}
# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = "Training set")) +
  geom_line(aes(iter, test_rmse_mean, color = "Validation set")) +
  geom_vline(aes(xintercept = which.min(xgb.fit1$evaluation_log$train_rmse_mean),
                 color = "Training set"), linetype = 2, show.legend = FALSE) +
  geom_vline(aes(xintercept = which.min(xgb.fit1$evaluation_log$test_rmse_mean),
                 color = "Validation set"), linetype = 2, show.legend = FALSE) +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual") +
  labs(title = "Training vs. validation error",
       x = "Iteration",
       y = "5-fold CV RMSE",
       color = NULL) +
  theme(legend.position = "bottom")
```

---

# Early stopping

```{r ames-xgb-stop, dependson = "ames-prep"}
# reproducibility
set.seed(123)

xgb.fit2 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

# plot error vs number trees
ggplot(xgb.fit2$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean, color = "Training set")) +
  geom_line(aes(iter, test_rmse_mean, color = "Validation set")) +
  geom_vline(aes(xintercept = which.min(xgb.fit2$evaluation_log$train_rmse_mean),
                 color = "Training set"), linetype = 2, show.legend = FALSE) +
  geom_vline(aes(xintercept = which.min(xgb.fit2$evaluation_log$test_rmse_mean),
                 color = "Validation set"), linetype = 2, show.legend = FALSE) +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual") +
  labs(title = "Training vs. validation error",
       x = "Iteration",
       y = "5-fold CV RMSE",
       color = NULL) +
  theme(legend.position = "bottom")
```

---

# Stochastic gradient descent

.pull-left[

### Batch gradient descent

* Calculate gradient using all observations simultaneously
* Terribly slow for large data sets

]

--

.pull-right[

### Stochastic gradient descent

* Random subsampling
* Use only one/small number of observations at a time
    
![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0408.png)

]

---

# Stochastic `xgboost`

```{r ames-xgb-stochastic, dependson = "ames"}
# reproducibility
set.seed(123)

# train model
xgb_sto_8 <- xgb.cv(
  params = list(
    subsample = .8
  ),
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

xgb_sto_6 <- xgb.cv(
  params = list(
    subsample = .6
  ),
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

xgb_sto_4 <- xgb.cv(
  params = list(
    subsample = .4
  ),
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

xgb_sto_2 <- xgb.cv(
  params = list(
    subsample = .2
  ),
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

bind_rows(
  Batch = xgb.fit2$evaluation_log,
  `Stochastic (.8)` = xgb_sto_8$evaluation_log,
  `Stochastic (.6)` = xgb_sto_6$evaluation_log,
  `Stochastic (.4)` = xgb_sto_4$evaluation_log,
  `Stochastic (.2)` = xgb_sto_2$evaluation_log,
  .id = "id"
) %>%
  mutate(id = factor(id)) %>%
  ggplot(aes(iter, test_rmse_mean, color = id)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  coord_cartesian(ylim = c(27000, 40000)) +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  labs(title = "Comparison of gradient descent methods",
       x = "Iteration",
       y = "5-fold CV RMSE",
       color = NULL)
```

---

# Feature importance

```{r ames-xgb-predictor, dependson = "ames-xgb-stop"}
library(caret)

xgb_final <- train(
  x = features_train,
  y = response_train,
  trControl = trainControl(
    method = "none",
    verboseIter = FALSE, # no training log
    allowParallel = TRUE # FALSE for reproducible results 
  ),
  tuneGrid = expand.grid(
    nrounds = xgb.fit2$best_iteration,
    max_depth = 6,
    eta = 0.3,
    gamma = 0,
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = 1
  ),
  method = "xgbTree"
)

predictor_xgb <- Predictor$new(
  model = xgb_final,
  data = train_data,
  y = "Sale_Price"
)
```

```{r ames-xgb-feat-imp, dependson = "ames-xgb-predictor"}
feat_imp_xgb <- FeatureImp$new(predictor_xgb, loss = "mse", parallel = TRUE)
```

```{r ames-xgb-feat-imp-plot, dependson = "ames-xgb-feat-imp", fig.asp = 3}
plot(feat_imp_xgb) +
  ggtitle("GBM") +
  theme_minimal(base_size = 8)
```

```{r iml-parallel-end, cache = FALSE}
stopCluster(cl)
```

---

# Comparison of untuned models

```{r bag-final, dependson = "ames-rf-tune-results"}
bag_cv <- vfold_cv(ames_train, v = 5) %>%
  mutate(model = map(splits, ~ randomForest(
    formula = Sale_Price ~ .,
    data = analysis(.x),
    xtest = select(assessment(.x), -Sale_Price),
    ytest = assessment(.x)$Sale_Price,
    num.trees = 500,
    mtry = ncol(assessment(.x)) - 1
  )),
  mse = map(model, ~ .x$test$mse)
  ) %>%
  unnest(mse) %>%
  group_by(id) %>%
  mutate(rmse = sqrt(mse),
         iter = row_number()) %>%
  group_by(iter) %>%
  summarize(rmse = mean(rmse))
```

```{r rf-final, dependson = "ames-rf-tune-results"}
rf_cv <- vfold_cv(ames_train, v = 5) %>%
  mutate(model = map(splits, ~ randomForest(
    formula = Sale_Price ~ .,
    data = analysis(.x),
    xtest = select(assessment(.x), -Sale_Price),
    ytest = assessment(.x)$Sale_Price,
    num.trees = 500
  )),
  mse = map(model, ~ .x$test$mse)
  ) %>%
  unnest(mse) %>%
  group_by(id) %>%
  mutate(rmse = sqrt(mse),
         iter = row_number()) %>%
  group_by(iter) %>%
  summarize(rmse = mean(rmse))
```

```{r all-final, dependson = c("bag-final", "rf-final", "ames-xgb")}
bind_rows(
  Bagging = bag_cv,
  `Random Forest` = rf_cv,
  .id = "model"
) %>%
  rename(test_rmse_mean = rmse) %>%
  bind_rows(
    xgb.fit2$evaluation_log %>%
      mutate(model = "Boosting")) %>%
  mutate(model = factor(model)) %>%
  ggplot(aes(iter, test_rmse_mean, color = model)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  coord_cartesian(ylim = c(25000, 40000)) +
  labs(title = "Comparison of tree-aggregation methods",
       x = "Iteration",
       y = "5-fold CV RMSE",
       color = NULL)
```

---

# Hyperparameter tuning

* Grid search
    * Exhaustive
    * Random
    * Parallelize your functions
* Targeted search
    * Bayesian optimization
    * Sequential model-based optimization
* `scikit-learn`
* `caret`/`mlr`
