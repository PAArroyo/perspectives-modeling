---
title: "Interpreting Machine Learning Algorithms"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2)

library(tidyverse)
library(tidymodels)
library(margins)
library(here)
library(rcfss)
library(patchwork)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Statistical background

* Interpreting regression coefficients

    $$Y = \beta_0 + \beta_1 X_1 + \ldots \beta_p X_p + \epsilon$$

* Generalizations of regression models
* Substantive interpretations

---

# Marginal effects

$$\frac{\partial Y}{\partial X}$$

--

$$
\begin{align}
Y &= \beta_0 + \beta_1 X_1 \\
\frac{\partial Y}{\partial X_1} &= \beta_1
\end{align}
$$

--

$$
\begin{align}
Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 \\
\frac{\partial Y}{\partial X_1} &= \beta_1 \\
\frac{\partial Y}{\partial X_2} &= \beta_2 \\
\end{align}
$$

---

# Marginal effects

$$
\begin{aligned}
Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 \\
\frac{\partial Y}{\partial X_1} &= \beta_1 + \beta_3 X_2 \\
\frac{\partial Y}{\partial X_2} &= \beta_2 + \beta_3 X_1
\end{aligned}
$$

--

$$
\begin{aligned}
Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 \\
\frac{\partial Y}{\partial X_1} &= \beta_1 + 2\beta_2 X_1 \\
\frac{\partial Y}{\partial X_2} &= \beta_3
\end{aligned}
$$

---

# Types of marginal effects

1. Marginal effects at representative values (MERs)
1. Marginal effects at means (MEMs)
1. Average marginal effects (AMEs)

---

# Logistic regression marginal effects

```{r titanic-data, message = FALSE}
library(titanic)
titanic <- titanic_train %>%
  as_tibble() %>%
  # remove missing values
  na.omit()
```

```{r age-woman, dependson = "titanic-data"}
age_woman <- glm(Survived ~ Age + Sex, data = titanic,
                 family = binomial)
summary(age_woman)
```

---

# Average marginal effects

## Link function

```{r age-woman-ame, dependson = "age-woman"}
margins(age_woman, type = "link")
```

--

## Response variable

```{r response-link, dependson = "age-woman"}
margins(age_woman, type = "response")
```

---

# Marginal effects at representative cases (MERs)

```{r age-woman-mer, dependson = "age-woman"}
margins(age_woman, at = list(Sex = c("male", "female")))
```

--

* AMEs calculated separately for men and women

```{r age-woman-subset-ame, dependson = "age-woman"}
margins(age_woman) %>%
  split(.$Sex)
```

---

# Marginal effects plots

```{r age-margin-effect, dependson = "age-woman", include = FALSE, fig.width = 12}
pred_age <- cplot(age_woman, "Age", what = "prediction", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Predicted probability of survival",
       subtitle = "Given age",
       x = "Age",
       y = "Predicted probability of survival")

ame_age <- cplot(age_woman, "Age", what = "effect", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Average marginal effect of age",
       x = "Age",
       y = "Marginal effect of age")
```

```{r age-margin-effect2, dependson = "age-woman", fig.width = 12}
pred_age +
  ame_age
```

---

# Interaction models

```{r age-fare, dependson = "titanic-data"}
age_fare_x <- glm(Survived ~ Age * Fare, data = titanic,
                  family = binomial)
summary(age_fare_x)
```

---

# Interaction models

.pull-left[

```{r age-fare-pred, dependson = "age-fare", results = "hide"}
cplot(age_fare_x, "Age", what = "prediction")
```

]

.pull-right[

```{r age-fare-me, dependson = "age-fare", results = "hide"}
cplot(age_fare_x, "Age", what = "effect")
```

]

---

# Interaction models

.pull-left[

```{r age-fare-fare, dependson = "age-fare", results = "hide"}
cplot(age_fare_x, "Fare", what = "prediction")
```

]

.pull-right[


```{r age-fare-fare2, dependson = "age-fare"}
cplot(age_fare_x, "Fare", what = "effect")
```

]

---

# Three-dimensional models

.pull-left[

```{r age-fare-joint, dependson = "age-fare"}
persp(age_fare_x, theta = c(45, 135, 225, 315), what = "prediction")
```
]

.pull-right[

```{r age-fare-joint2, dependson = "age-fare"}
persp(age_fare_x, theta = c(45, 135, 225, 315), what = "effect")
```

]

---

# Three-dimensional models

.center[

```{r age-fare-joint-plotly, dependson = "age-fare"}
library(plotly)

# predicted prob
surface <- margins:::calculate_surface(x = age_fare_x,
                                       xvar = "Age",
                                       yvar = "Fare",
                                       dx = "Age",
                                       nx = 25L,
                                       ny = 25L,
                                       type = "response",
                                       vcov = stats::vcov(age_fare_x),
                                       what = "prediction")

outcome <- surface[["outcome"]]
xvals <- surface[["xvals"]]
yvals <- surface[["yvals"]]

plot_ly(x = ~xvals, y = ~yvals, z = ~outcome) %>%
  add_surface() %>%
  layout(
    title = "Predicted probability of survival",
    scene = list(
      xaxis = list(title = "Age"),
      yaxis = list(title = "Fare"),
      zaxis = list(title = "Predicted probability of survival")
    ))
```

]

---

# Three-dimensional models

.center[

```{r age-fare-joint-plotly2, dependson = "age-fare"}
# marginal effect
surface <- margins:::calculate_surface(x = age_fare_x,
                                       xvar = "Age",
                                       yvar = "Fare",
                                       dx = "Age",
                                       nx = 25L,
                                       ny = 25L,
                                       type = "response",
                                       vcov = stats::vcov(age_fare_x),
                                       what = "effect")

outcome <- surface[["outcome"]]
xvals <- surface[["xvals"]]
yvals <- surface[["yvals"]]

plot_ly(x = ~xvals, y = ~yvals, z = ~outcome) %>%
  add_surface() %>%
  layout(
    title = "Marginal effect on survival",
    scene = list(
      xaxis = list(title = "Age"),
      yaxis = list(title = "Fare"),
      zaxis = list(title = "AME")
    ))
```

]

---

# Interpreting models

* Model-specific methods
* Model-agnostic methods

--

## Desirable traits

* Model flexibility
* Explanation flexibility
* Representation flexibility

--

## Type of interpretation

* Global
* Local

---

# `attrition`

```{r prep-iml, include = FALSE, cache = FALSE}
# load required packages
library(rsample)   # data splitting
library(ggplot2)   # allows extension of visualizations
library(dplyr)     # basic data transformation
library(h2o)       # machine learning modeling
library(iml)       # ML interprtation

# initialize h2o session
h2o.no_progress()
h2o.init()
```

```{r prep-data, include = FALSE}
# classification data
df <- rsample::attrition %>% 
  mutate_if(is.ordered, factor, ordered = FALSE) %>%
  mutate(Attrition = recode(Attrition, "Yes" = "1", "No" = "0") %>% factor(levels = c("1", "0")))

# convert to h2o object
df.h2o <- as.h2o(df)

# create train, validation, and test splits
set.seed(123)
splits <- h2o.splitFrame(df.h2o, ratios = c(.7, .15), destination_frames = c("train","valid","test"))
names(splits) <- c("train","valid","test")

# variable names for resonse & features
y <- "Attrition"
x <- setdiff(names(df), y) 
```

```{r models, include = FALSE, dependson = "prep-data"}
# elastic net model 
glm <- h2o.glm(
  x = x, 
  y = y, 
  training_frame = splits$train,
  validation_frame = splits$valid,
  family = "binomial",
  seed = 123
  )

# naive Bayes model
nb <- h2o.naiveBayes(
  x = x, 
  y = y,
  training_frame = splits$train,
  validation_frame = splits$valid,
  seed = 123
  )

# gradient boosting machine model
gbm <-  h2o.gbm(
  x = x, 
  y = y,
  training_frame = splits$train,
  validation_frame = splits$valid,
  ntrees = 1000,
  stopping_metric = "AUC",    
  stopping_rounds = 10,         
  stopping_tolerance = 0.005,
  seed = 123
  )

# model performance
h2o.auc(glm, valid = TRUE)
h2o.auc(nb, valid = TRUE)
h2o.auc(gbm, valid = TRUE)
```

```{r adapt-iml, include = FALSE, dependson = "models"}
# 1. create a data frame with just the features
features <- as.data.frame(splits$valid) %>%
  select(-Attrition)

# 2. Create a vector with the actual responses
response <- as.numeric(as.vector(splits$valid$Attrition))

# 3. Create custom predict function that returns the predicted values as a
#    vector (probability of purchasing in our example)
pred <- function(model, newdata)  {
  results <- as.data.frame(h2o.predict(model, as.h2o(newdata)))
  return(results[[3L]])
}

# example of prediction output
pred(nb, features) %>%
  head()
```

```{r create-predictor-obj, include = FALSE, dependson = "adapt-iml"}
# create predictor object to pass to explainer functions
predictor.glm <- Predictor$new(
  model = glm, 
  data = features, 
  y = response, 
  predict.fun = pred,
  class = "classification"
  )

predictor.nb <- Predictor$new(
  model = nb, 
  data = features, 
  y = response, 
  predict.fun = pred,
  class = "classification"
  )

predictor.gbm <- Predictor$new(
  model = gbm, 
  data = features, 
  y = response, 
  predict.fun = pred,
  class = "classification"
  )
```

```{r attrition, dependson = "prep-data"}
glimpse(df)
```

---

# `attrition`

## Model types

* Logistic regression (`glm`)
* Naive Bayes (`nb`)
* Gradient boosting machine (`gbm`)

---

# Partial dependence plot

$$\hat{f}_{x_S}(x_S)=\E_{x_C}\left[\hat{f}(x_S,x_C)\right]=\int\hat{f}(x_S,x_C)d\Pr(x_C)$$

* Marginal effect a feature has on the predictor
* $x_s$ - features we want to know
* $x_C$ - all the other features
* Average over all remaining features
* Equivalent to AME in regression models

---

# Individual conditional expectation

* Plot one line per observation that shows how the observation's prediction changes when a feature changes
* PDP is average of all ICEs
* Captures heterogeneous relationships created by interactions
* Shows the full distribution rather than a single summary line
* Centered ICE plot

---

# PDP/ICE

```
For a selected predictor (x)
1. Determine grid space of j evenly spaced values across distribution of x
2: for value i in {1,...,j} of grid space do
     | set x to i for all observations
     | apply given ML model
     | estimate predicted value
     | if PDP: average predicted values across all observations
   end
```

---

# Categorical variables

```{r pdp, dependson = "create-predictor-obj", fig.width = 12}
glm.ot <- Partial$new(predictor.glm, "OverTime") %>% plot() + ggtitle("GLM")
nb.ot <- Partial$new(predictor.nb, "OverTime") %>% plot() + ggtitle("NB") 
gbm.ot <- Partial$new(predictor.gbm, "OverTime") %>% plot() + ggtitle("GBM")

glm.ot +
  nb.ot +
  gbm.ot
```

---

# Continuous variables

```{r pdp-cont, dependson = "create-predictor-obj", fig.width = 12}
# GLM model
glm.age <- Partial$new(predictor.glm, "Age", ice = TRUE, grid.size = 50)
glm.age$center(min(features$Age))
p1 <- plot(glm.age) + ggtitle("GLM")

# nb model
nb.age <- Partial$new(predictor.nb, "Age", ice = TRUE, grid.size = 50)
nb.age$center(min(features$Age))
p2 <- plot(nb.age) + ggtitle("NB")

# GBM model
gbm.age <- Partial$new(predictor.gbm, "Age", ice = TRUE, grid.size = 50)
gbm.age$center(min(features$Age))
p3 <- plot(gbm.age) + ggtitle("GBM")

p1 + p2 + p3
```

---

# Two-way interactions

```{r pdp-two-way, dependson = "create-predictor-obj", fig.width = 12}
p1 <- Partial$new(predictor.glm, c("MonthlyIncome", "OverTime")) %>% plot() + ggtitle("GLM") + ylim(c(0, .4)) + theme(legend.position = "bottom")
p2 <- Partial$new(predictor.nb, c("MonthlyIncome", "OverTime")) %>% plot() + ggtitle("NB") + ylim(c(0, .4)) + theme(legend.position = "bottom")
p3 <- Partial$new(predictor.gbm, c("MonthlyIncome", "OverTime")) %>% plot() + ggtitle("GBM") + ylim(c(0, .4)) + theme(legend.position = "bottom")

p1 + p2 + p3
```

---

# PDP/ICE

## Strengths

* Intuitive
* Interpretation is clear (if other features are uncorrelated)
* Easy to implement
* Uncover heterogeneous relationships (ICE only)

## Weaknesses

* Maximum number of features
* No feature distribution (resolved with ICE)
* Assumption of independence

---

# Measuring interactions

* Measuring interactions without explicit interaction terms
* H-statistic

    ```
    1: for variable i in {1,...,p} do
         | f(x) = estimate predicted values with original model
         | pd(x) = partial dependence of variable i
         | pd(!x) = partial dependence of all features excluding i
         | upper = sum(f(x) - pd(x) - pd(!x))
         | lower = variance(f(x))
         | rho = upper / lower
       end
    2. Sort variables by descending rho (interaction strength)  
    ```

--

* Share of variance explained by the interaction
* $\rho \in [0,1]$
    * $0$ - no interaction
    * $1$ - all variation depends on a given interaction

---

# Measuring interactions

```{r interactions, dependson = "create-predictor-obj", fig.width = 12}
# identify variables with largest interactions in each model
interact.glm <- Interaction$new(predictor.glm) %>% plot() + ggtitle("GLM") + theme_minimal(base_size = 12)
interact.nb  <- Interaction$new(predictor.nb) %>% plot() + ggtitle("NB") + theme_minimal(base_size = 12)
interact.gbm <- Interaction$new(predictor.gbm) %>% plot() + ggtitle("GBM") + theme_minimal(base_size = 12)

# plot
interact.glm + interact.nb + interact.gbm
```

---

# Measuring interactions

* With which other variable(s) is the interaction strongest?

```
1: i = a selected variable of interest
2: for remaining variables j in {1,...,p} do
     | pd(ij) = interaction partial dependence of variables i and j
     | pd(i) = partial dependence of variable i
     | pd(j) = partial dependence of variable j
     | upper = sum(pd(ij) - pd(i) - pd(j))
     | lower = variance(pd(ij))
     | rho = upper / lower
   end
3. Sort interaction relationship by descending rho (interaction strength)  
```

---

# Measuring interactions

```{r interactions-overtime, dependson = "create-predictor-obj", fig.width = 12}
# identify variables with largest interactions w/OverTime
interact.glm <- Interaction$new(predictor.glm, feature = "OverTime") %>% plot() + theme_minimal(base_size = 12)
interact.nb  <- Interaction$new(predictor.nb, feature = "OverTime") %>% plot() + theme_minimal(base_size = 12)
interact.gbm <- Interaction$new(predictor.gbm, feature = "OverTime") %>% plot() + theme_minimal(base_size = 12)

# plot
interact.glm + interact.nb + interact.gbm
```

---

# Measuring interactions

## Strengths

* Underlying theory via partial dependence decomposition
* Dimensionless
* Detects all shapes of interactions

## Weaknesses

* Computationally intensive
* Requires sampling from the distribution rather than using the entire distribution
* No test for statistical significance

---

# Feature importance

* Calculate the increase in the model's prediction error after **permuting** the feature
    * Randomly shuffle the feature's values across observations
* Important feature
* Unimportant feature

```
For any given loss function do
1: compute loss function for original model
2: for variable i in {1,...,p} do
     | randomize values
     | apply given ML model
     | estimate loss function
     | compute feature importance (permuted loss / original loss)
   end
3. Sort variables by descending feature importance   
```

---

# Feature importance

```{r feature-imp, dependson = "create-predictor-obj", fig.width = 12}
# compute feature importance with specified loss metric
imp.glm <- FeatureImp$new(predictor.glm, loss = "mse")
imp.nb <- FeatureImp$new(predictor.nb, loss = "mse")
imp.gbm <- FeatureImp$new(predictor.gbm, loss = "mse")

# plot output
p1 <- plot(imp.glm) + ggtitle("GLM") + theme_minimal(base_size = 12)
p2 <- plot(imp.nb) + ggtitle("NB") + theme_minimal(base_size = 12)
p3 <- plot(imp.gbm) + ggtitle("GBM") + theme_minimal(base_size = 12)

p1 + p2 + p3
```

---

# Feature importance

## Strengths

* Nice interpretation
* Global insight into each variable's performance
* Takes into account all interactions
* Does not require retraining the model

## Weaknesses

* Depends on random shuffling - results can vary
* Computationally intensive
* Correlated features
    * Unrealistic observations
    * Adding correlated feature can decrease importance of the original feature

---

# Global surrogate

* Interpretable model trained to approximate the predictions of a black box model
* Generate conclusions about the black box model by interpreting the simpler model

$$g(\hat{f(x)}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

---

# Global surrogate

1. Apply original model and get predictions
1. Choose an interpretable "white box" model (linear model, decision tree)
1. Train the interpretable model on the original dataset and its predictions
1. Measure how well the surrogate model replicates the prediction of the black box model
1. Interpret / visualize the surrogate model

--
* How well does the surrogate model replicate the black box model?

---

# Global surrogate

.pull-left[

```{r global-surrogate, dependson = "create-predictor-obj"}
# fit surrogate decision tree model
tree.glm <- TreeSurrogate$new(predictor.glm, maxdepth = 3)
tree.nb <- TreeSurrogate$new(predictor.nb, maxdepth = 3)
tree.gbm <- TreeSurrogate$new(predictor.gbm, maxdepth = 3)

# how well does this model fit the original results
tribble(
  ~model, ~r.squared,
  "GLM", tree.glm$r.squared,
  "NB", tree.nb$r.squared,
  "GBM", tree.gbm$r.squared
) %>%
  knitr::kable(format = "html")
```

]

.pull-right[

```{r global-surrogate-nb, dependson = "create-predictor-obj"}
plot(tree.nb$tree, main = "NB")
```

]

---

# Global surrogate

## Strengths

* Flexible (uses any interpretable model)
* Intuitive
* Easy measure of how well the surrogate model approximates the black box model

## Weaknesses

* Draw conclusions about the model, not the data
* What makes a good surrogate model?
* Surrogate model systematically over/underpredicts subsets of the data set
* Accept the flaws of the interpretable model

---

# LIME

## Local Interpretable Model-agnostic Explanations

* Global $\rightarrow$ local
* Interpretable models used to explain individual predictions of a black box model
* Assumes every complex model is linear on a local scale
* Fit a simple model around a single observation to mimic how the global model behaves at that locality
* Simple model explains the predictions of the complex model **locally**
    * Local fidelity
    * Does not require global fidelity

---

# LIME

$$\text{explanation}(x) = \arg \min_{g \in G} L(f, g, \pi_x) + \Omega(g)$$

* Explanation model for $x$
* Minimize loss $L$
    * Difference between explanation and original prediction $g(x)$
* $G$ - family of possible explanations
* $\pi_x$
* Keep $\Omega(g)$ low to be interpretable by humans

---

# LIME

1. For each prediction to explain, permute the observation $n$ times
1. Let the complex model predict the outcome of all permuted observations
1. Calculate the distance from all permutations to the original observation
1. Convert the distance to a similarity score
1. Select $m$ features best describing the complex model outcome from the permuted data
1. Fit a simple model to the permuted data, explaining the complex model outcome with the $m$ features from the permuted data weighted by its similarity to the original observation
1. Extract the feature weights from the simple model and use these as explanations for the complex models local behavior

```{r lime-pkgs, include = FALSE}
library(lime)       # ML local interpretation
library(caret)      # ML model building
```

```{r lime-data, include = FALSE}
# create data sets
df <- rsample::attrition %>% 
  dplyr::mutate_if(is.ordered, factor, ordered = FALSE) %>%
  dplyr::mutate(Attrition = factor(Attrition, levels = c("Yes", "No")))

index <- 1:5
train_obs <- df[-index, ]
local_obs <- df[index, ]

# create h2o objects for modeling
y <- "Attrition"
x <- setdiff(names(train_obs), y)
train_obs.h2o <- as.h2o(train_obs)
local_obs.h2o <- as.h2o(local_obs)
```

```{r lime-models, dependson = "lime-data", include = FALSE}
# create h2o models
h2o_nb <- h2o.naiveBayes(x, y, training_frame = train_obs.h2o)
h2o_glm <- h2o.glm(x, y, training_frame = train_obs.h2o, family = "binomial")
h2o_gbm <- h2o.gbm(x, y, training_frame = train_obs.h2o)
```

---

# Features plot

```{r lime-interpret, dependson = "lime-models", fig.asp = 2}
explainer_h2o_nb  <- lime(train_obs, h2o_nb, n_bins = 5)
explainer_h2o_glm <- lime(train_obs, h2o_glm, n_bins = 5)
explainer_h2o_gbm <- lime(train_obs, h2o_gbm, n_bins = 5)

explanation_nb <- explain(local_obs, explainer_h2o_nb, n_features = 5, labels = "Yes", kernel_width = .1, feature_select = "highest_weights")
explanation_glm <- explain(local_obs, explainer_h2o_glm, n_features = 5, labels = "Yes", kernel_width = .1, feature_select = "highest_weights")
explanation_gbm <- explain(local_obs, explainer_h2o_gbm, n_features = 5, labels = "Yes", kernel_width = .1, feature_select = "highest_weights")

p1 <- plot_features(explanation_nb, ncol = 1) + ggtitle("nb") + theme_minimal(base_size = 8) + theme(legend.position = "bottom")
p2 <- plot_features(explanation_glm, ncol = 1) + ggtitle("glm") + theme_minimal(base_size = 8) + theme(legend.position = "bottom")
p3 <- plot_features(explanation_gbm, ncol = 1) + ggtitle("gbm") + theme_minimal(base_size = 8) + theme(legend.position = "bottom")
```

```{r lime-nb, dependson = "lime-interpret", fig.width = 12}
plot_features(explanation_nb) + ggtitle("nb")
```

---

# Explanations plot

```{r lime-glm, dependson = "lime-interpret", fig.width = 12}
plot_explanations(explanation_nb) + ggtitle("nb")
```

---

# Comparing models

```{r lime-interpret-all, dependson = "lime-interpret", fig.width = 12, fig.asp = 0.7}
p1 + p2 + p3
```

---

# Tuning LIME

* Interpretable model
* Distance metric
* Size of local region
* Feature selection

---

# Tuning LIME

.pull-left[

* Gower's distance

    $$d(\mathbf{p}, \mathbf{q}) = \frac{1}{k} \sum_{i=1}^k d_{p_i q_i}^{(f)}$$
    
    * Range for each variable $[0,1]$

* $\text{Kernel width} = 0.75$
* $\text{Number of features} = 5$
* Feature selection - ridge regression

]

.pull-right[

* Manhattan distance

    $$d(\mathbf{p}, \mathbf{q}) = \sum_{i = 1}^k | p_i - q_i|$$

* $\text{Kernel width} = 3$
* $\text{Number of features} = 10$
* Feature selection - lasso regression

]

---

# Tuning LIME

```{r explain-tuning, dependson = "lime-models", fig.height = 8, fig.width = 12}
explanation_nb_tuned <- explain(local_obs,
                                explainer_h2o_nb,
                                dist_fun = "manhattan",
                                kernel_width = 3,
                                n_features = 10, 
                                feature_select = "lasso_path",
                                labels = "Yes"
)

{
  plot_features(explanation_nb, ncol = 2) +
    ggtitle("NB: Untuned") +
    theme_minimal(base_size = 8) +
    theme(legend.position = "bottom")
} +
{
  plot_features(explanation_nb_tuned, ncol = 2) +
    ggtitle("NB: Tuned") +
    theme_minimal(base_size = 8) +
    theme(legend.position = "bottom")
}
```

---

# LIME for images

```{r lime-images-package-example}
explanation <- .load_image_example()
plot_image_explanation(explanation)
```

---

# LIME

## Strengths

* Same explanatory model for different machine learning algorithms
* Human-friendly explanations
* Works for tabular, text, and image data

## Weaknesses

* What is the correct neighborhood?
* Complexity of explanation model is defined a priori
* Explanations for similar data points can vary greatly
