<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Random Forests/Boosting</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 30100   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Random Forests/Boosting
### <a href="https://model.uchicago.edu">MACS 30100</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Limitations of bagging

&lt;img src="rf-boosting_files/figure-html/mpg-bag-trees-1.png" width="864" /&gt;

---

# Limitations of bagging

* Grow deep trees
* Randomization component
* Lack of independence

---

# Limitations of bagging

* Independently and identically distributed (i.i.d.) random variables

    `$$\Var(\overline{\hat{Y}_b}) = \frac{1}{B} \sigma^2$$`

* Identically distributed (i.d.) random variables

    `$$\Var(\overline{\hat{Y}_b}) = \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2$$`

* Limit on variance reduction

---

# Random forests

* Minimize correlation between the trees
* Random forests

1. Bootstrap sampling
1. Split-variable randomization
    * `\(m = p\)`
    * `\(m \leq p\)`
    * Regression: `\(\frac{p}{3}\)`
    * Classification: `\(\sqrt{p}\)`
    * Tuning parameter

---

# Random forest

1. For `\(b=1\)` to `\(B\)`:
    1. Draw a bootstrap sample `\(\mathbf{Z}^*\)` of size `\(N\)` from the training data
    1. Grow a decision tree `\(T_b\)` to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree until the minimum node size `\(n_\text{min}\)` is reached:
        1. Select `\(m\)` variables at random from the `\(p\)` predictors
        1. Pick the best variable/split-point among the `\(m\)`
        1. Split the node into two child nodes
1. Output the ensemble of trees `\(\{ T_b \}_1^B\)`

* Making predictions
* Out-of-bag estimates
* Model interpretation

---

# Default parameters




```
## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 661089658
##                     % Var explained: 89.8
```

---

# Number of trees

&lt;img src="rf-boosting_files/figure-html/ames-rf-plot-1.png" width="864" /&gt;

---

# OOB vs. validation set



&lt;img src="rf-boosting_files/figure-html/ames-rf-oob-val-plot-1.png" width="864" /&gt;

---

# Tuning parameter options

* Number of trees
* Number of variables to randomly sample at each split
* Sample size for each sample
* Minimum number of observations within the terminal nodes
* Maximum number of terminal nodes

---

# Tuning parameter options


```
## [1] 96
```




```
##    mtry node_size sampe_size OOB_RMSE
## 1    20         5        0.8 25918.20
## 2    20         3        0.8 25963.96
## 3    28         3        0.8 25997.78
## 4    22         5        0.8 26041.05
## 5    22         3        0.8 26050.63
## 6    20         7        0.8 26061.72
## 7    26         3        0.8 26069.40
## 8    28         5        0.8 26069.83
## 9    26         7        0.8 26075.71
## 10   20         9        0.8 26091.08
```

---

# Final model performance



&lt;img src="rf-boosting_files/figure-html/ames-rf-tune-final-plot-1.png" width="864" /&gt;

---

# Feature importance







&lt;img src="rf-boosting_files/figure-html/ames-rf-feat-imp-plot-1.png" width="864" /&gt;

---

# Partial dependence

&lt;img src="rf-boosting_files/figure-html/ames-rf-pdp-1.png" width="864" /&gt;

---

# Comparison to bagging





&lt;img src="rf-boosting_files/figure-html/rf-bag-compare-1.png" width="864" /&gt;

---

# Types of predictive models

* Strong vs. weak models
* Single predictive model
* Ensemble model
    * Parallel construction
    * Sequential construction

---

# Sequential ensemble construction

![](https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png)

---

# Why train weak models?

* Speed
* Accuracy improvement
* Slow learning
* Avoids overfitting

---

# Sequential training using errors

1. Fit a decision tree to the data: `\(F_1(x) = y\)`
1. Fit the next decision tree to the residuals of the previous: `\(h_1(x) = y - \lambda F_1(x)\)`
1. Add this new tree to the algorithm: `\(F_2(x) = F_1(x) + \lambda h_1(x)\)`
1. Fit the next decision tree to the residuals of `\(F_2\)`: `\(h_2(x) = y - \lambda F_2(x)\)`
1. Add this new tree to the algorithm: `\(F_3(x) = F_2(x) + \lambda h_1(x)\)`
1. Continue this process until some mechanism (i.e. cross validation) says to stop

* Stagewise additive model

    `$$f(x) =  \sum^B_{b=1} \lambda f^b(x)$$`
    
* Learning parameter `\(\lambda\)`

---

# Sequential training using errors

![](rf-boosting_files/figure-html/seq-learn-1.gif)&lt;!-- --&gt;

---

# Boosting

* Bagging
    * Combine low bias, high variance models
* Random forest
    * Combine low-ish bias, less high variance models
* Boosting
    * Combine high bias, low variance models

---

# Gradient descent

* Loss functions
    * MSE
    * MAE
    * Deviance
    * All differentiable
* How to minimize the loss function
* Gradient boosting

---

# Gradient descent

.center[

![:scale 80%](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0402.png)

]

* Iteratively tweak estimated parameters
* Steepest slope
* Gradient

---

# Learning rate comparisons

.pull-left[

##### Too small

![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0403.png)

]

--

.pull-right[

##### Too large

![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0404.png)

]

---

# Global/local minima

![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0405.png)

* MSE - globally convex

---

# Tuning gradient boosting

* Number of trees
* Depth of trees
* Learning rate
* `xgboost`

---

# `xgboost` model



* Learning rate: 0.3
* Tree depth: 6
* Minimum node size: 1



---

# `xgboost` model

&lt;img src="rf-boosting_files/figure-html/ames-xgb-err-1.png" width="864" /&gt;

---

# Early stopping

&lt;img src="rf-boosting_files/figure-html/ames-xgb-stop-1.png" width="864" /&gt;

---

# Stochastic gradient descent

.pull-left[

### Batch gradient descent

* Calculate gradient using all observations simultaneously
* Terribly slow for large data sets

]

--

.pull-right[

### Stochastic gradient descent

* Random subsampling
* Use only one/small number of observations at a time
    
![[Geron, 2017](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0408.png)

]

---

# Stochastic `xgboost`

&lt;img src="rf-boosting_files/figure-html/ames-xgb-stochastic-1.png" width="864" /&gt;

---

# Feature importance





&lt;img src="rf-boosting_files/figure-html/ames-xgb-feat-imp-plot-1.png" width="864" /&gt;



---

# Comparison of untuned models





&lt;img src="rf-boosting_files/figure-html/all-final-1.png" width="864" /&gt;

---

# Hyperparameter tuning

* Grid search
    * Exhaustive
    * Random
    * Parallelize your functions
* Targeted search
    * Bayesian optimization
    * Sequential model-based optimization
* `scikit-learn`
* `caret`/`mlr`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
