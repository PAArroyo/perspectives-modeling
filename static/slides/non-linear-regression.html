<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Moving Beyond Linearity</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 30100   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
    <script src="libs/kePrint/kePrint.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Moving Beyond Linearity
### <a href="https://model.uchicago.edu">MACS 30100</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Linearity in linear models

* Assumes linear relationship
* Non-linear relationships
* Alternative strategies
    * Decision trees
    * Support vector machines
    * Neural networks
* Conducting inference

---

# Linear relationship

`$$Y = 2 + 3X + \epsilon$$`

&lt;img src="non-linear-regression_files/figure-html/sim-linear-1.png" width="864" /&gt;

---

# Linear relationship

&lt;img src="non-linear-regression_files/figure-html/sim-linear-resid-1.png" width="864" /&gt;

---

# Non-linear relationship

`$$Y = 2 + 3X + 2X^2 + \epsilon$$`

&lt;img src="non-linear-regression_files/figure-html/sim-nonlinear-1.png" width="864" /&gt;

---

# Non-linear relationship

&lt;img src="non-linear-regression_files/figure-html/sim-nonlinear-resid-1.png" width="864" /&gt;

---

# Variance in the error term

* Assume `\(\epsilon_i\)` has a constant variance `\(\text{Var}(\epsilon_i) = \sigma^2\)`
* Homoscedasticity

    `$$\widehat{s.e.}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^{2} (X^{T}X)^{-1}_{jj}}$$`

* Heteroscedasticity

---

# Variance in the error term

`$$Y = 2 + 3X + \epsilon, \quad \epsilon \sim N(0,1)$$`

&lt;img src="non-linear-regression_files/figure-html/sim-homo-1.png" width="864" /&gt;

---

# Variance in the error term

`$$Y = 2 + 3X + \epsilon, \quad \epsilon \sim N(0, \frac{X}{2})$$`

&lt;img src="non-linear-regression_files/figure-html/sim-hetero-1.png" width="864" /&gt;

---

# Monotonic transformations

* Function for transforming a set of numbers into a different set of numbers so that the rank order of the original set of numbers is preserved

--

## Ladder of powers

Transformation | Power | `\(f(X)\)`
---------------|-------|--------
Cube | 3 | `\(X^3\)`
Square | 2 | `\(X^2\)`
Identity | 1 | `\(X\)`
Square root | `\(\frac{1}{2}\)` | `\(\sqrt{X}\)`
Cube root | `\(\frac{1}{3}\)` | `\(\sqrt[3]{X}\)`
Log | 0 (sort of) | `\(\log(X)\)`

---

# Ladder of powers

&lt;img src="non-linear-regression_files/figure-html/power-ladder-1.png" width="864" /&gt;

---

# Which transformation should I use?

.center[
&lt;img src="non-linear-regression_files/figure-html/bulge-rule-1.png" width="576" /&gt;
]

---

# Interpreting log transformations

`$$\log(Y_i) = \beta_0 + \beta_{1}X_i + \epsilon_i$$`

`$$\E(Y) = e^{\beta_0 + \beta_{1}X_i}$$`

`$$\frac{\partial \E(Y)}{\partial X} = e^{\beta_1}$$`

`$$Y_i = \beta_0 + \beta_{1} \log(X_i) + \epsilon_i$$`

---

# Log-log regressions

`$$\log(Y_i) = \beta_0 + \beta_{1} \log(X_i) + \dots + \epsilon_i$$`

* `\(\beta_1\)` - elasticity of `\(Y\)` with respect to `\(X\)`

    `$$\text{Elasticity}_{YX} = \frac{\% \Delta Y}{\% \Delta X}$$`

---

# Polynomial regressions

`$$y_i = \beta_0 + \beta_{1}x_{i} + \epsilon_{i}$$`

`$$y_i = \beta_0 + \beta_{1}x_{i} + \beta_{2}x_i^2 + \beta_{3}x_i^3 + \dots + \beta_{d}x_i^d + \epsilon_i$$`

---

# Biden and age

`$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$`

&lt;img src="non-linear-regression_files/figure-html/biden-age-1.png" width="864" /&gt;

---

# Biden and age

&lt;table&gt;
&lt;caption&gt;Variance-covariance matrix of Biden polynomial regression&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; (Intercept) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; age &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; I(age^2) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; I(age^3) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; I(age^4) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 620.00316 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.20765 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; I(age^2) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00533 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; I(age^3) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; I(age^4) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Biden and age

&lt;img src="non-linear-regression_files/figure-html/biden-margins-1.png" width="864" /&gt;

---

# Voter turnout and mental health

`$$\Pr(\text{Voter turnout} = \text{Yes} | \text{mhealth}) = \frac{\exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}{1 + \exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}$$`

&lt;img src="non-linear-regression_files/figure-html/mhealth-1.png" width="864" /&gt;

---

# Voter turnout and mental health

&lt;img src="non-linear-regression_files/figure-html/mhealth-prob-1.png" width="864" /&gt;

---

# Step functions

* Global transformations
* Local transformations
* Step functions
    * Split `\(X\)` into bins
    * Fit a different constant to each bin

---

# Step functions

* Create `\(c_1, c_2, \dots, c_K\)` cutpoints in the range of `\(X\)`
* Construct `\(K + 1\)` new indicator variables `\(C_1(X), C_2(X), \dots, C_K(X)\)`
* Fit the linear regression model to the new indicator variables as predictors:

`$$y_i = \beta_0 + \beta_1 C_1 (x_i) + \beta_2 C_2 (x_i) + \dots + \beta_K C_K (x_i) + \epsilon_i$$`

---

# Age and voting

&lt;img src="non-linear-regression_files/figure-html/vote96-step-1.png" width="864" /&gt;

---

# Basis functions

* Family of functions or transformations applied to a variable `\(X\)`

    `$$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \ldots + \beta_K b_K(x_i) + \epsilon_i$$`
    
* Fix the functional form
* Uses least squares estimation

---

# Regression splines

* Fit separate polynomial functions over different regions of `\(X\)`

---

# Piecewise polynomial

* Fit separate low-degree polynomials for different regions of `\(X\)`
* Cubic piecewise polynomial regression model

    `$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$`

    * `\(K\)` knots

--
* Piecewise cubic polynomial with 0 knots

    `$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$`

* Piecewise constant polynomial = piecewise constant regression

---

# Piecewise cubic polynomial

`$$y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp; \text{if } x_i &lt; c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp; \text{if } x_i \geq c
   \end{cases}$$`

* `\(K = 1\)`
* Increasing `\(K\)`

---

# Piecewise cubic regression

&lt;img src="non-linear-regression_files/figure-html/sim-piecewise-1.png" width="864" /&gt;

---

# Constraints and splines

&lt;img src="non-linear-regression_files/figure-html/sim-spline-1.png" width="864" /&gt;

---

# Constraints and splines

&lt;img src="non-linear-regression_files/figure-html/sim-spline-smooth-1.png" width="864" /&gt;

---

# Increasing `\(K\)`

&lt;img src="non-linear-regression_files/figure-html/sim-spline-smooth-5-1.png" width="864" /&gt;

---

# Spline basis representation

* Regression splines as basis functions
* Cubic spline with `\(K\)` knots

    `$$y_i = \beta_0 + \beta_1 b_1 (x_i) + \beta_2 b_2 (x_i) + \cdots + \beta_{K + 3} b_{K + 3} (x_i) + \epsilon_i$$`
* Imposing constraints with truncated power basis

    $$
    h(x, \zeta) = (x - \zeta)_+^3 = 
    \begin{cases} 
      (x - \zeta)^3 &amp; \text{if } x &gt; \zeta \\
      0 &amp; \text{otherwise}
    \end{cases}
    $$

* Cubic spline with `\(K\)` knots requires
    * Intercept
    * `\(3 + K\)` predictors

    `$$X, X^2, X^3, h(X, \zeta_1), h(X, \zeta_2), \ldots, h(X, \zeta_K)$$`

* Degrees of freedom

---

# Spline basis representation

## `\(K = 4\)`


```
## # A tibble: 100 x 8
##        x        `1`     `2`       `3`    `4`      `5`   `6`    `7`
##    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1  3.56 0.0257     0.472   0.489     0.0138 0        0     0     
##  2  1.98 0.410      0.481   0.0865    0      0        0     0     
##  3  1.50 0.532      0.331   0.0363    0      0        0     0     
##  4  4.49 0.00000279 0.199   0.669     0.132  0        0     0     
##  5  8.73 0          0       0         0.0570 0.452    0.469 0.0220
##  6  8.21 0          0       0.0000143 0.176  0.622    0.202 0     
##  7  4.72 0          0.144   0.663     0.193  0.000150 0     0     
##  8  6.37 0          0.00122 0.220     0.642  0.136    0     0     
##  9  5.60 0          0.0275  0.473     0.473  0.0267   0     0     
## 10  8.96 0          0       0         0.0291 0.333    0.561 0.0765
## # â€¦ with 90 more rows
```

---

# Basis splines vs. natural splines

* High variance at the outer range of the predictors

&lt;img src="non-linear-regression_files/figure-html/vote-age-basis-spline-1.png" width="864" /&gt;

---

# Boundary constraints

* Linear function at the boundaries
* Natural spline

&lt;img src="non-linear-regression_files/figure-html/vote-age-natural-spline-1.png" width="864" /&gt;

---

# Choosing the knots

* Uniform placement of knots
* Partition `\(X\)` into `\(K\)` uniform quantiles

---

# Choosing the knots

&lt;img src="non-linear-regression_files/figure-html/vote-spline-1.png" width="864" /&gt;

---

# Choosing the knots

&lt;img src="non-linear-regression_files/figure-html/vote-cv-1.png" width="864" /&gt;

---

# Choosing the knots

&lt;img src="non-linear-regression_files/figure-html/vote-optimal-mod-1.png" width="864" /&gt;

---

# Comparison to polynomial regression

&lt;img src="non-linear-regression_files/figure-html/vote-spline-poly-1.png" width="864" /&gt;

---

# Goal of regression

`$$\min \left\{ \text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2 \right\}$$`

---

# No constraints

&lt;img src="non-linear-regression_files/figure-html/sim-perfect-fit-1.png" width="864" /&gt;

---

# Smoothing spline

* Minimize

    `$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$`

* "Loss + Penalty"
* `\(g''(t)\)`
* `\(\lambda \rightarrow \infty\)`

---

# Smoothing spline

&lt;img src="non-linear-regression_files/figure-html/sim-spline-compare-1.png" width="864" /&gt;

---

# Choosing the smoothing parameter `\(\lambda\)`

* Degrees of freedom
* Effective degrees of freedom
    * `\(\lambda\)` increases from `\(0\)` to `\(\infty\)`
    * Effective degrees of freedom decreases from `\(n\)` to `\(2\)`

---

# Measuring effective `\(df\)`

`$$\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda \mathbf{y}$$`

* `\(\hat{\mathbf{g}}_\lambda\)`

    `$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$`
* `\(\mathbf{S}_\lambda\)`
* Effective degrees of freedom

    `$$df_\lambda = \sum_{i=1}^n \{\mathbf{S}_\lambda \}_{ii}$$`

---

# Tuning smoothing splines

* Number and location of knots
* Value for `\(\lambda\)`
* LOOCV

    `$$\text{RSS}_{cv}(\lambda) = \sum_{i=1}^n (y_i - \hat{g}_\lambda^{(-i)} (x_i))^2 = \sum_{i=1}^n \left[ \frac{y_i - \hat{g}_\lambda (x_i)}{1 - \{ \mathbf{S}_\lambda \}_{ii}} \right]^2$$`

    * `\(\hat{g}_\lambda^{(-i)} (x_i)\)`
    * `\(\hat{g}_\lambda (x_i)\)`

---

# Tuning smoothing splines

&lt;img src="non-linear-regression_files/figure-html/sim-smooth-spline-compare-1.png" width="864" /&gt;

---

# Kernel functions

* Weighting function used for non-parametric estimation techniques
* Non-negative real-valued integrable function `\(K\)`
* Normalization

    `$$\int_{-\infty}^{+\infty} K(u) du = 1$$`
    
* Symmetry

    `$$K(-u) = K(u) \text{ for all values of } u$$`
    
---

# Gaussian kernel

`$$K(u) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2} u^2}$$`

&lt;img src="non-linear-regression_files/figure-html/gaussian-1.png" width="864" /&gt;

---

# Rectangular (uniform) kernel

`$$K(u) = \frac{1}{2} \mathbf{1}_{\{ |u| \leq 1 \} }$$`

&lt;img src="non-linear-regression_files/figure-html/uniform-1.png" width="864" /&gt;

---

# Triangular kernel

`$$K(u) = (1 - |z|) \mathbf{1}_{\{ |u| \leq 1 \} }$$`

&lt;img src="non-linear-regression_files/figure-html/triangular-1.png" width="864" /&gt;

---

# Tricube kernel

`$$K(u) = \frac{70}{81} (1 - |u|^3)^3 \mathbf{1}_{\{ |u| \leq 1 \} }$$`

&lt;img src="non-linear-regression_files/figure-html/tricube-1.png" width="864" /&gt;

---

# Epanechnikov kernel

`$$K(u) = \frac{3}{4} (1 - u^2) \mathbf{1}_{\{ |u| \leq 1 \} }$$`

&lt;img src="non-linear-regression_files/figure-html/epanechnikov-1.png" width="864" /&gt;

---

# Comparison of kernels

&lt;img src="non-linear-regression_files/figure-html/kernels-1.png" width="864" /&gt;

---

# Kernel smoothing

* Method for estimating the regression function `\(f(X)\)` over the domain `\(\Re^p\)`
* Fit a different but simple model separately at each query point `\(x_0\)`
    * Use only local observations
    * Ensure estimated function `\(f(X)\)` is smooth
* Weighting function `\(K_\lambda (x_o, x_i)\)`
* Defining the neighborhood

---

# Connection to nearest neighbors

`$$\hat{f}(x) = \text{Ave} (y_i | x_i \in N_k(x))$$`



&lt;img src="non-linear-regression_files/figure-html/sim-naive-1.png" width="864" /&gt;

---

# Kernel smoothing

* Nadaraya-Watson kernel-weighted average

`$$\hat{f}(x_0) = \frac{\sum_{i=1}^N K_\lambda(x_0, x_i)y_i}{\sum_{i=1}^N K_\lambda (x_0, x_i)}$$`

`$$K_\lambda(x_0, x_i) = D \left( \frac{| x - x_0 |}{\lambda} \right)$$`


`$$D(t) = \begin{cases}
\frac{3}{4} (1 - t^2) &amp; \text{if } |t| \leq 1 \\
0 &amp; \text{otherwise}
\end{cases}$$`

---

# Kernel smoothing

&lt;img src="non-linear-regression_files/figure-html/sim-epanechnikov-1.png" width="864" /&gt;

---

# Kernel smoothing

* Defining `\(\lambda\)`
    * Constant
    * Adaptive

`$$K_\lambda(x_0, x_i) = D \left( \frac{| x - x_0 |}{h_\lambda (x_0)} \right)$$`

---

# Tuning parameters

1. Choice of kernel function
1. Bandwidth

---

# Infant mortality

&lt;img src="non-linear-regression_files/figure-html/infant-1.png" width="864" /&gt;

---

# Infant mortality

&lt;img src="non-linear-regression_files/figure-html/infant-kernels-1.png" width="864" /&gt;

---

# Infant mortality

![](non-linear-regression_files/figure-html/infant-epan-1.gif)&lt;!-- --&gt;

---

# Infant mortality

&lt;img src="non-linear-regression_files/figure-html/infant-epan-optimal-1.png" width="864" /&gt;

---

# Local regression

* Biases in the boundaries of the domain of `\(X\)`
* Biases in the interior of the domain of `\(X\)`
* Kernel smoothing
* Local linear regression

---

# Local linear regression

* Fit a separate linear function at each target point `\(x_0\)` using only the nearby training observations
* Builds regression model from localized subsets of data

    `$$\min_{\alpha(x_0), \beta(x_0)} \sum_{i=1}^N K_\lambda (x_0, x_i) [y_i - \alpha(x_0) - \beta (x_0)x_i]^2$$`

* `\(\hat{f}(x_0) = \hat{\alpha} + \hat{\beta}(x_0)x_0\)`
* Repeat for varying `\(x_0\)`

---

# Local linear regression

&lt;img src="non-linear-regression_files/figure-html/loess-1.png" width="864" /&gt;

---

# Local linear regression

![](non-linear-regression_files/figure-html/loess_buildup-1.gif)&lt;!-- --&gt;

---

# Local linear regression

![](non-linear-regression_files/figure-html/loess_span-1.gif)&lt;!-- --&gt;

---

# Local polynomial regression

`$$\min_{\alpha(x_0), \beta_j(x_0), j = 1, \ldots, d} \sum_{i=1}^N K_\lambda (x_0, x_i) \left[y_i - \alpha(x_0) - \sum_{j=1}^d \beta_j (x_0)x_i^j \right]^2$$`

* Bias-variance tradeoff

---

# Local polynomial regression

&lt;img src="non-linear-regression_files/figure-html/loess-poly-1.png" width="864" /&gt;

---

# Higher dimensional local regression

* Pairs of independent variables `\(X_1, X_2\)`
    * Two-dimensional neighborhoods
    * Bivariate linear regression models
* `\(p &gt; 2\)`
* `\(n &gt; 1000\)`

---

# Generalized additive models

* Non-linear linear models with multiple predictors
* Extend the linear model to allow non-linear functions of each predictor
* Maintains additive assumption

---

# GAMs for regression problems

`$$y_i = \beta_0 + \beta_{1} X_{i1} + \beta_{2} X_{i2} + \dots + \beta_{p} X_{ip} + \epsilon_i$$`

`$$y_i = \beta_0 + \sum_{j = 1}^p f_j(x_{ij}) + \epsilon_i$$`

`$$y_i = \beta_0 + f_1(x_{i1}) + \beta_{2} f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i$$`

---

# GAMs for regression models

`$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$`

* `\(f_1, f_2\)` - cubic spline with 2 knots
* `\(f_3\)` - traditional dummy variable

---

# GAMs for regression models

`$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$`



&lt;img src="non-linear-regression_files/figure-html/biden-gam-1.png" width="864" /&gt;

---

# GAMs for regression models

`$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$`

* `\(f_1, f_2\)` - local regression functions
* `\(f_3\)` - traditional dummy variable

---

# GAMs for regression models

&lt;img src="non-linear-regression_files/figure-html/biden-gam-local-1.png" width="864" /&gt;

---

# GAMs for classification models

&lt;img src="non-linear-regression_files/figure-html/titanic-gam-1.png" width="864" /&gt;

---

# Benefits and drawbacks to GAMs

* Allow for non-linear `\(f_j\)` to each `\(X_j\)`
* Potentially more accurate than purely linear model
* Still additive
    * Effects of each predictor are independent from one another
    * Does not capture interactive effects

---

# Multivariate adaptive regression splines

* Combines
    * Stepwise regression
    * Polynomial regression
    * Step functions
* Good for high-dimensional data
* Efficient estimation with cross-validation

---

# Piecewise basis functions

.pull-left[

`$$(x - t)_+ = \begin{cases}
x - t, &amp; \text{if } x &gt; t, \\
0, &amp; \text{otherwise}
\end{cases}$$`

and

`$$(t - x)_+ = \begin{cases}
t - x, &amp; \text{if } x &lt; t, \\
0, &amp; \text{otherwise}
\end{cases}$$`

]

.pull-right[

&lt;img src="non-linear-regression_files/figure-html/mars-hinge-1.png" width="432" /&gt;

]

---

# MARS

* Piecewise linear basis functions
* Reflected pairs
* Form pairs for each input `\(X_j\)` with knots at each observed value `\(x_{ij}\)`

`$$\begin{align}
C &amp;= \{ (X_j - t)_+, (t - X_j)_+ \} \\
t &amp;\in \{ x_{1j}, x_{2j}, \ldots, x_{Nj} \} \\
j &amp;= 1, 2, \ldots, p
\end{align}$$`

* `\(2Np\)` basis functions

---

# MARS model construction

* Forward stepwise linear regression
* Use functions from set `\(C\)` and their products

    `$$f(X) = \beta_0 + \sum_{m=1}^M \beta_m h_m (X)$$`
* Start with constant function `\(h_0 (X) = 1\)`
* Test each basis function pair from `\(C\)`
* Add to the model `\(M\)` the term of the form

    `$$\hat{\beta}_{M+1} h_\lagr (X) \times (X_j - t)_+ + \hat{\beta}_{M + 2} h_\lagr (X) \times (t - X_j)_+, h_\lagr \in M$$`

    that produces the largest decrease in training error
* Estimated via least squares
* Continue until model `\(M\)` contains some preset maximum number of terms
* Prune model back

---

# Ames housing data





&lt;img src="non-linear-regression_files/figure-html/ames-poly-step-1.png" width="864" /&gt;

---

# MARS for Ames





`$$\text{Sale_Price} = \begin{cases}
145088.1 -356 (1971 - \text{Year_Built}) &amp; \text{Year_Built} \leq 1971 \\
145088.1 + 3088 (\text{Year_Built} - 1971) &amp; \text{Year_Built} &gt; 1971
\end{cases}$$`

&lt;img src="non-linear-regression_files/figure-html/ames-mars-k1-plot-1.png" width="864" /&gt;

---

# MARS for Ames



`$$\text{Sale_Price} = \begin{cases}
147285.2 -407 (1971 - \text{Year_Built}) &amp; \text{Year_Built} \leq 1971 \\
147285.2 + 2726 (\text{Year_Built} - 1971) &amp; 1971 &lt; \text{Year_Built} \leq 2004 \\
147285.2 + 8566.11 (\text{Year_Built} - 2004) &amp; \text{Year_Built} &gt; 2004
\end{cases}$$`

&lt;img src="non-linear-regression_files/figure-html/ames-mars-k2-plot-1.png" width="864" /&gt;

---

# MARS for Ames

&lt;img src="non-linear-regression_files/figure-html/ames-mars-plots-1.png" width="864" /&gt;

---

# Fitting the full model


```
## Selected 34 of 39 terms, and 25 of 307 predictors
## Termination condition: RSq changed by less than 0.001 at 39 terms
## Importance: First_Flr_SF, Second_Flr_SF, Year_Built, ...
## Number of terms at each degree of interaction: 1 33 (additive model)
## GCV 539058657    RSS 1.036171e+12    GRSq 0.9142879    RSq 0.9197103
```

```
##                              Sale_Price
## (Intercept)                262160.96669
## h(Gr_Liv_Area-2944)           -95.18430
## h(Year_Built-2003)           2521.36354
## h(2003-Year_Built)           -273.26088
## h(Total_Bsmt_SF-2217)         -70.59361
## h(2217-Total_Bsmt_SF)         -33.82657
## h(1795-Bsmt_Unf_SF)            22.88060
## Overall_QualExcellent       87926.04658
## Overall_QualVery_Excellent 110552.06993
## Overall_QualVery_Good       35416.02058
```

---

# Model selection

&lt;img src="non-linear-regression_files/figure-html/ames-mars-select-1.png" width="864" /&gt;

---

# Potential interactions

* Interaction of hinge functions


```
##                                           Sale_Price
## (Intercept)                             3.356495e+05
## h(Gr_Liv_Area-2944)                     1.534509e+02
## h(2944-Gr_Liv_Area)                    -4.637141e+01
## h(Year_Built-2003)                      8.578523e+03
## h(2003-Year_Built)                     -5.708871e+02
## h(Total_Bsmt_SF-2217)                   5.914583e+01
## h(2217-Total_Bsmt_SF)                  -4.856431e+01
## Mas_Vnr_TypeStone*h(Gr_Liv_Area-2944)  -4.584539e+02
## h(Year_Built-2003)*h(2944-Gr_Liv_Area) -3.572707e+00
## h(2003-Year_Built)*h(2944-Gr_Liv_Area)  1.423427e-01
```

---

# Tuning a MARS model

* Degree of interactions
* Number of retained terms
* Grid search

---

# Tuning a MARs model

&lt;img src="non-linear-regression_files/figure-html/ames-mars-grid-search-1.png" width="864" /&gt;

---

# Comparison to other models



&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Min. &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1st Qu. &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Median &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 3rd Qu. &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Max. &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; NA's &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Multiple_regression &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18738.04 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23221.23 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 29517.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 34136.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 47277.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 52421.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PCR &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27364.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 31158.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35674.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35822.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 37850.24 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 47404.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PLS &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20657.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26678.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28085.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32335.96 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35472.19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 59351.24 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Elastic_net &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20838.31 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25029.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27102.92 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 31705.14 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35862.75 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 55692.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MARS &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22097.02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 24011.23 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25912.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28520.63 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 33021.95 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 39698.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Feature importance



&lt;img src="non-linear-regression_files/figure-html/ames-feature-imp-1.png" width="864" /&gt;

---

# Partial dependence

&lt;img src="non-linear-regression_files/figure-html/ames-pdp-1.png" width="864" /&gt;

---

# MARS

## Advantages

* Accurate if local linear relationships are correct
* Quick computation
* Automated feature selection
* Intuitive non-linear relationships

## Distadvantages

* Not accurate if local linear relationships are incorrect
* Not as accurate as more advanced non-linear algorithms
* Difficult to implement more advanced spline features
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
