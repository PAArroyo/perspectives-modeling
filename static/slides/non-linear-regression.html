<!DOCTYPE html>
<html>
  <head>
    <title>Moving Beyond Linearity</title>
    <meta charset="utf-8">
    <meta name="author" content="MACS 30100   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Moving Beyond Linearity
### <a href="https://model.uchicago.edu">MACS 30100</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Linearity in linear models

* Assumes linear relationship
* Non-linear relationships
* Alternative strategies
    * Decision trees
    * Support vector machines
    * Neural networks
* Conducting inference

---

# Linear relationship

`$$Y = 2 + 3X + \epsilon$$`

&lt;img src="non-linear-regression_files/figure-html/sim-linear-1.png" width="864" /&gt;

---

# Linear relationship

&lt;img src="non-linear-regression_files/figure-html/sim-linear-resid-1.png" width="864" /&gt;

---

# Non-linear relationship

`$$Y = 2 + 3X + 2X^2 + \epsilon$$`

&lt;img src="non-linear-regression_files/figure-html/sim-nonlinear-1.png" width="864" /&gt;

---

# Non-linear relationship

&lt;img src="non-linear-regression_files/figure-html/sim-nonlinear-resid-1.png" width="864" /&gt;

---

# Variance in the error term

* Assume `\(\epsilon_i\)` has a constant variance `\(\text{Var}(\epsilon_i) = \sigma^2\)`
* Homoscedasticity

    `$$\widehat{s.e.}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^{2} (X^{T}X)^{-1}_{jj}}$$`

* Heteroscedasticity

---

# Variance in the error term

`$$Y = 2 + 3X + \epsilon, \quad \epsilon \sim N(0,1)$$`

&lt;img src="non-linear-regression_files/figure-html/sim-homo-1.png" width="864" /&gt;

---

# Variance in the error term

`$$Y = 2 + 3X + \epsilon, \quad \epsilon \sim N(0, \frac{X}{2})$$`

&lt;img src="non-linear-regression_files/figure-html/sim-hetero-1.png" width="864" /&gt;

---

# Monotonic transformations

* Function for transforming a set of numbers into a different set of numbers so that the rank order of the original set of numbers is preserved

--

## Ladder of powers

Transformation | Power | `\(f(X)\)`
---------------|-------|--------
Cube | 3 | `\(X^3\)`
Square | 2 | `\(X^2\)`
Identity | 1 | `\(X\)`
Square root | `\(\frac{1}{2}\)` | `\(\sqrt{X}\)`
Cube root | `\(\frac{1}{3}\)` | `\(\sqrt[3]{X}\)`
Log | 0 (sort of) | `\(\log(X)\)`

---

# Ladder of powers

&lt;img src="non-linear-regression_files/figure-html/power-ladder-1.png" width="864" /&gt;

---

# Which transformation should I use?

.center[
&lt;img src="non-linear-regression_files/figure-html/bulge-rule-1.png" width="576" /&gt;
]

---

# Interpreting log transformations

`$$\log(Y_i) = \beta_0 + \beta_{1}X_i + \epsilon_i$$`

`$$\E(Y) = e^{\beta_0 + \beta_{1}X_i}$$`

`$$\frac{\partial \E(Y)}{\partial X} = e^{\beta_1}$$`

`$$Y_i = \beta_0 + \beta_{1} \log(X_i) + \epsilon_i$$`

---

# Log-log regressions

`$$\log(Y_i) = \beta_0 + \beta_{1} \log(X_i) + \dots + \epsilon_i$$`

* `\(\beta_1\)` - elasticity of `\(Y\)` with respect to `\(X\)`

    `$$\text{Elasticity}_{YX} = \frac{\% \Delta Y}{\% \Delta X}$$`

---

# Polynomial regressions

`$$y_i = \beta_0 + \beta_{1}x_{i} + \epsilon_{i}$$`

`$$y_i = \beta_0 + \beta_{1}x_{i} + \beta_{2}x_i^2 + \beta_{3}x_i^3 + \dots + \beta_{d}x_i^d + \epsilon_i$$`

---

# Biden and age

`$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$`

&lt;img src="non-linear-regression_files/figure-html/biden-age-1.png" width="864" /&gt;

---

# Biden and age

&lt;table&gt;
&lt;caption&gt;Variance-covariance matrix of Biden polynomial regression&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; (Intercept) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; age &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; I(age^2) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; I(age^3) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; I(age^4) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 620.00316 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.20765 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; I(age^2) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00533 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; I(age^3) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; I(age^4) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Biden and age

&lt;img src="non-linear-regression_files/figure-html/biden-margins-1.png" width="864" /&gt;

---

# Voter turnout and mental health

`$$\Pr(\text{Voter turnout} = \text{Yes} | \text{mhealth}) = \frac{\exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}{1 + \exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}$$`

&lt;img src="non-linear-regression_files/figure-html/mhealth-1.png" width="864" /&gt;

---

# Voter turnout and mental health

&lt;img src="non-linear-regression_files/figure-html/mhealth-prob-1.png" width="864" /&gt;

---

# Step functions

* Global transformations
* Local transformations
* Step functions
    * Split `\(X\)` into bins
    * Fit a different constant to each bin

---

# Step functions

* Create `\(c_1, c_2, \dots, c_K\)` cutpoints in the range of `\(X\)`
* Construct `\(K + 1\)` new indicator variables `\(C_1(X), C_2(X), \dots, C_K(X)\)`
* Fit the linear regression model to the new indicator variables as predictors:

`$$y_i = \beta_0 + \beta_1 C_1 (x_i) + \beta_2 C_2 (x_i) + \dots + \beta_K C_K (x_i) + \epsilon_i$$`

---

# Age and voting

&lt;img src="non-linear-regression_files/figure-html/vote96-step-1.png" width="864" /&gt;

---

# Basis functions

* Family of functions or transformations applied to a variable `\(X\)`

    `$$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \ldots + \beta_K b_K(x_i) + \epsilon_i$$`
    
* Fix the functional form
* Uses least squares estimation

---

# Regression splines

* Fit separate polynomial functions over different regions of `\(X\)`

---

# Piecewise polynomial

* Fit separate low-degree polynomials for different regions of `\(X\)`
* Cubic piecewise polynomial regression model

    `$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$`

    * `\(K\)` knots

--
* Piecewise cubic polynomial with 0 knots

    `$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$`

* Piecewise constant polynomial = piecewise constant regression

---

# Piecewise cubic polynomial

`$$y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp; \text{if } x_i &lt; c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp; \text{if } x_i \geq c
   \end{cases}$$`

* `\(K = 1\)`
* Increasing `\(K\)`

---

# Piecewise cubic regression

&lt;img src="non-linear-regression_files/figure-html/sim-piecewise-1.png" width="864" /&gt;

---

# Constraints and splines

&lt;img src="non-linear-regression_files/figure-html/sim-spline-1.png" width="864" /&gt;

---

# Constraints and splines

&lt;img src="non-linear-regression_files/figure-html/sim-spline-smooth-1.png" width="864" /&gt;

---

# Increasing `\(K\)`

&lt;img src="non-linear-regression_files/figure-html/sim-spline-smooth-5-1.png" width="864" /&gt;

---

# Spline basis representation

* Regression splines as basis functions
* Cubic spline with `\(K\)` knots

    `$$y_i = \beta_0 + \beta_1 b_1 (x_i) + \beta_2 b_2 (x_i) + \cdots + \beta_{K + 3} b_{K + 3} (x_i) + \epsilon_i$$`
* Imposing constraints with truncated power basis

    $$
    h(x, \zeta) = (x - \zeta)_+^3 = 
    \begin{cases} 
      (x - \zeta)^3 &amp; \text{if } x &gt; \zeta \\
      0 &amp; \text{otherwise}
    \end{cases}
    $$

* Cubic spline with `\(K\)` knots requires
    * Intercept
    * `\(3 + K\)` predictors

    `$$X, X^2, X^3, h(X, \zeta_1), h(X, \zeta_2), \ldots, h(X, \zeta_K)$$`

* Degrees of freedom

---

# Spline basis representation

## `\(K = 4\)`


```
## # A tibble: 100 x 8
##        x        `1`     `2`       `3`    `4`      `5`   `6`    `7`
##    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1  3.56 0.0257     0.472   0.489     0.0138 0        0     0     
##  2  1.98 0.410      0.481   0.0865    0      0        0     0     
##  3  1.50 0.532      0.331   0.0363    0      0        0     0     
##  4  4.49 0.00000279 0.199   0.669     0.132  0        0     0     
##  5  8.73 0          0       0         0.0570 0.452    0.469 0.0220
##  6  8.21 0          0       0.0000143 0.176  0.622    0.202 0     
##  7  4.72 0          0.144   0.663     0.193  0.000150 0     0     
##  8  6.37 0          0.00122 0.220     0.642  0.136    0     0     
##  9  5.60 0          0.0275  0.473     0.473  0.0267   0     0     
## 10  8.96 0          0       0         0.0291 0.333    0.561 0.0765
## # … with 90 more rows
```

---

# Basis splines vs. natural splines

* High variance at the outer range of the predictors

&lt;img src="non-linear-regression_files/figure-html/vote-age-basis-spline-1.png" width="864" /&gt;

---

# Boundary constraints

* Linear function at the boundaries
* Natural spline

&lt;img src="non-linear-regression_files/figure-html/vote-age-natural-spline-1.png" width="864" /&gt;

---

# Choosing the knots

* Uniform placement of knots
* Partition `\(X\)` into `\(K\)` uniform quantiles

---

# Choosing the knots

&lt;img src="non-linear-regression_files/figure-html/vote-spline-1.png" width="864" /&gt;

---

# Choosing the knots

&lt;img src="non-linear-regression_files/figure-html/vote-cv-1.png" width="864" /&gt;

---

# Choosing the knots

&lt;img src="non-linear-regression_files/figure-html/vote-optimal-mod-1.png" width="864" /&gt;

---

# Comparison to polynomial regression

&lt;img src="non-linear-regression_files/figure-html/vote-spline-poly-1.png" width="864" /&gt;

---

# Goal of regression

`$$\min \left\{ \text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2 \right\}$$`

---

# No constraints

&lt;img src="non-linear-regression_files/figure-html/sim-perfect-fit-1.png" width="864" /&gt;

---

# Smoothing spline

* Minimize

    `$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$`

* "Loss + Penalty"
* `\(g''(t)\)`
* `\(\lambda \rightarrow \infty\)`

---

# Smoothing spline

&lt;img src="non-linear-regression_files/figure-html/sim-spline-compare-1.png" width="864" /&gt;

---

# Choosing the smoothing parameter `\(\lambda\)`

* Degrees of freedom
* Effective degrees of freedom
    * `\(\lambda\)` increases from `\(0\)` to `\(\infty\)`
    * Effective degrees of freedom decreases from `\(n\)` to `\(2\)`

---

# Measuring effective `\(df\)`

`$$\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda \mathbf{y}$$`

* `\(\hat{\mathbf{g}}_\lambda\)`

    `$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$`
* `\(\mathbf{S}_\lambda\)`
* Effective degrees of freedom

    `$$df_\lambda = \sum_{i=1}^n \{\mathbf{S}_\lambda \}_{ii}$$`

---

# Tuning smoothing splines

* Number and location of knots
* Value for `\(\lambda\)`
* LOOCV

    `$$\text{RSS}_{cv}(\lambda) = \sum_{i=1}^n (y_i - \hat{g}_\lambda^{(-i)} (x_i))^2 = \sum_{i=1}^n \left[ \frac{y_i - \hat{g}_\lambda (x_i)}{1 - \{ \mathbf{S}_\lambda \}_{ii}} \right]^2$$`

    * `\(\hat{g}_\lambda^{(-i)} (x_i)\)`
    * `\(\hat{g}_\lambda (x_i)\)`

---

# Tuning smoothing splines

&lt;img src="non-linear-regression_files/figure-html/sim-smooth-spline-compare-1.png" width="864" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
