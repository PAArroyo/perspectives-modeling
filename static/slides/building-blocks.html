<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Building blocks of neural networks</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 30200   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Building blocks of neural networks
### <a href="https://github.com/css-research/course/">MACS 30200</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Neural network

* Nonlinear statistical model
* Two-stage regression or classification model
* Graph

---

# Neural network

.center[

![](/img/esl-11-3.png)

]

---

# First look at a neural network

.center[

![MNIST digits](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

]
---

# Import data


```r
library(keras)

mnist &lt;- dataset_mnist()
train_images &lt;- mnist$train$x
train_labels &lt;- mnist$train$y
test_images &lt;- mnist$test$x
test_labels &lt;- mnist$test$y
```


```r
str(train_images)
##  int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
str(train_labels)
##  int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...
```


```r
str(test_images)
##  int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
str(test_labels)
##  int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...
```

---

# Build the network


```r
network &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %&gt;% 
  layer_dense(units = 10, activation = "softmax")
```

---

# Compile the network


```r
network %&gt;% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

---

# Reshape the data


```r
train_images &lt;- array_reshape(train_images, c(60000, 28 * 28))
train_images &lt;- train_images / 255
str(train_images)
```

```
##  num [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
```

```r
test_images &lt;- array_reshape(test_images, c(10000, 28 * 28))
test_images &lt;- test_images / 255
str(test_images)
```

```
##  num [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
```


```r
train_labels &lt;- to_categorical(train_labels)
test_labels &lt;- to_categorical(test_labels)
```

---

# Train the network


```r
set.seed(1234)

network %&gt;% fit(train_images, train_labels, epochs = 5, batch_size = 128)
```

---

# Assess the model


```r
metrics &lt;- network %&gt;% evaluate(test_images, test_labels, verbose = 0)
metrics
```

```
## $loss
## [1] 0.07085208
## 
## $acc
## [1] 0.9793
```

---

# Data representations for neural networks

* Tensor
* Generalization of vectors and matricies to higher-dimensions

---

# Tensor

## Scalars (0D tensors)

* A tensor containing a single number

--

## Vectors (1D tensors)

* A tensor with a one-dimensional array of numbers
* Has exactly one axis


```r
x &lt;- c(12, 3, 6, 14, 10)
str(x)
##  num [1:5] 12 3 6 14 10
dim(as.array(x))
## [1] 5
```

* 5D vector `\(\neq\)` 5D tensor

---

# Tensor

## Matricies (2D tensors)

* A tensor with a two-dimensional array of numbers
* Two axes (frequently rows and columns)
* Visually is a rectangular grid of numbers


```r
x &lt;- matrix(rep(0, 3*5), nrow = 3, ncol = 5)
x
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0    0    0    0    0
## [2,]    0    0    0    0    0
## [3,]    0    0    0    0    0

dim(x)
## [1] 3 5
```

---

# Tensor

## 3D tensors and higher-dimensional tensors

* Array of matricies


```r
x &lt;- array(rep(0, 2*3*2), dim = c(2, 3, 2))
x
## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    0    0    0
## [2,]    0    0    0
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    0    0    0
## [2,]    0    0    0
str(x)
##  num [1:2, 1:3, 1:2] 0 0 0 0 0 0 0 0 0 0 ...
dim(x)
## [1] 2 3 2
```

---

# Key attributes

* Number of axes (rank)
* Shape
    * Matrix example - `\((3,5)\)`
    * 3D tensor example - `\((2,3,2)\)`
* Data type

--

## Data batches

* Samples axis
* Batching and batch axis

---

# Vector data

* 2D tensors of shape (`samples`, `features`)
* Each data point is encoded as a vector

---

# Actuarial dataset of people

## Variables

* Age
* ZIP code
* Income

--

## Data structure

* Person - vector of 3 values
* Dataset - 100,000 individuals
* 2D tensor of shape `(100000, 3)`

---

# Text documents

* Each document has a count of the frequency each word appears in it
* Dictionary of 20,000 words
* Document - vector of 20,000 values
* Documents - 500 documents

--

* 2D tensor of shape `(500, 20000)`

---

# Time series data

![](/img/fig-2-3.png)

---

# Stock prices

## Variables

* Current price of a stock
* Highest price in the past minute
* Lowest price in the past minute
* Recorded every minute for a single stock

--

## Data structure

* Day of trading - 2D tensor of shape `(390, 3)`
* 250 days' worth of data - 3D tensor of shape `(250, 390, 3)`

---

# Image data

![](/img/fig-2-4.png)

---

# Video data

* Requires 5D tensor
* Each frame is a color image (`height`, `width`, `color_depth`)
* Sample of frames for a single video (`frames`, `height`, `width`, `color_depth`)
* Batch of videos (`samples`, `frames`, `height`, `width`, `color_depth`)

---

# Tensor operations

* Generalizations of matrix operations
    * Addition
    * Subtraction
    * Multiplication
* If you can do it with a matrix, you can do it with a tensor

---

# Geometric interpretation

.center[

![](/img/fig-2-6.png)

]

---

# Geometric interpretation

.center[

![](/img/fig-2-8.png)

]

---

# Geometric interpretation of deep learning

.center[

![](/img/fig-2-9.png)

]

---

# Gradient-based optimization

```
output = relu(dot(W, input) + B)
```

* `input`
* `dot()`
* `relu()`
* `W, b`

* Adjust the weights to produce the lowest loss score
* Initialize the weights with random values that are not useful

---

# Training loop

1. Draw a batch of training samples `x` and targets `y`
1. Forward pass - run the network on `x` to obtain predictions `y_pred`
1. Compute the loss of the network on the batch (mismatch between `y_pred` and `y`)
1. Update the weights of the network in a way that slightly reduces the loss on this batch

--

## Implementation

* Naive solution
* Differentiation

---

# Derivative

.center[

![](/img/fig-2-10.png)

]

---

# Derivative of a tensor operation

```
y_pred = dot(W, x)
loss_value = loss(y_pred, y)
```

```
loss_value = f(W)
```

```
loss_value = f(W0)
f'(W0)
```

---

# Stochastic gradient descent

1. Draw a batch of training samples `x` and targets `y`
1. Forward pass - run the network on `x` to obtain predictions `y_pred`
1. Compute the loss of the network on the batch (mismatch between `y_pred` and `y`)
1. Backward pass - compute the gradient of the loss with regard to the network's parameters
1. Move the parameters a little in the opposite direction from the gradient to reduce the loss for the batch

---

# Stochastic gradient descent

.center[

![](/img/fig-2-11.png)

]

---

# Stochastic gradient descent

.center[

![](/img/fig-2-12.png)

]

---

# Momentum

.center[

![](/img/fig-2-13.png)

]

---

# Backpropogation

* Chained tensor operations

    ```
    f(W1, W2, W3) = a(W1, b(W2, c(W3)))
    ```

* Calculus and the chain rule

    ```
    f(g(x)) = f'(g(x)) * g'(x)
    ```

--

* Backpropogation
    * Start with the final loss value
    * Work backwards from the top layers to the bottom layers
    * Apply the chain rule to compute the contribution that each parameter had in the loss value

---

# Import our data

```r
library(keras)

mnist &lt;- dataset_mnist()
train_images &lt;- mnist$train$x
train_labels &lt;- mnist$train$y
test_images &lt;- mnist$test$x
test_labels &lt;- mnist$test$y
```

---

# The network

```r
network &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %&gt;% 
  layer_dense(units = 10, activation = "softmax")
```

---

# Compilation

```r
network %&gt;% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

---

# Training loop

```r
network %&gt;%
  fit(train_images, train_labels, epochs = 5, batch_size = 128)
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
