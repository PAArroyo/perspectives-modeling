---
title: "Moving Beyond Linearity"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(tidymodels)
library(rcfss)
library(titanic)
library(knitr)
library(splines)
library(ISLR)
library(lattice)
library(gam)
library(here)
library(patchwork)
library(margins)
library(earth)
library(gganimate)
library(caret)
library(iml)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Linearity in linear models

* Assumes linear relationship
* Non-linear relationships
* Alternative strategies
    * Decision trees
    * Support vector machines
    * Neural networks
* Conducting inference

---

# Linear relationship

$$Y = 2 + 3X + \epsilon$$

```{r sim-linear, echo = FALSE}
sim_linear <- tibble(
  x = runif(100, 0, 10),
  y = 2 + 3 * x + rnorm(100, 0, 3)
)
sim_linear_mod <- glm(y ~ x, data = sim_linear)

ggplot(sim_linear, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Linear model for a linear relationship")
```

---

# Linear relationship

```{r sim-linear-resid, echo = FALSE, dependson = "sim-linear"}
sim_linear_pred <- augment(sim_linear_mod)

# distribution of residuals
{
  ggplot(sim_linear_pred, aes(.resid)) +
    geom_histogram(aes(y = stat(density))) +
    stat_function(fun = dnorm,
                  args = list(mean = mean(sim_linear_pred$.resid),
                              sd = sd(sim_linear_pred$.resid))) +
    labs(title = "Linear model for a linear relationship",
         x = "Residuals")
} + {
  # predicted vs. residuals
  ggplot(sim_linear_pred, aes(.fitted, .resid)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    labs(title = "Linear model for a linear relationship",
         x = "Predicted values",
         y = "Residuals")
}
```

---

# Non-linear relationship

$$Y = 2 + 3X + 2X^2 + \epsilon$$

```{r sim-nonlinear, echo = FALSE}
sim_nonlinear <- tibble(
  x = runif(100, 0, 10),
  y = 2 + 3 * x + 2 * x^2 + rnorm(100, 0, 3)
)
sim_nonlinear_mod <- glm(y ~ x, data = sim_nonlinear)

ggplot(sim_nonlinear, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Linear model for a non-linear relationship")
```

---

# Non-linear relationship

```{r sim-nonlinear-resid, echo = FALSE, dependson = "sim-nonlinear"}
sim_nonlinear_pred <- augment(sim_nonlinear_mod)

# distribution of residuals
{
  ggplot(sim_nonlinear_pred, aes(.resid)) +
    geom_histogram(aes(y = stat(density))) +
    stat_function(fun = dnorm,
                  args = list(mean = mean(sim_nonlinear_pred$.resid),
                              sd = sd(sim_nonlinear_pred$.resid))) +
    labs(title = "Linear model for a non-linear relationship",
         x = "Residuals")
} + {
  # predicted vs. residuals
  ggplot(sim_nonlinear_pred, aes(.fitted, .resid)) +
    geom_point() +
    geom_smooth(se = FALSE) +
    labs(title = "Linear model for a non-linear relationship",
         x = "Predicted values",
         y = "Residuals")
}
```

---

# Variance in the error term

* Assume $\epsilon_i$ has a constant variance $\text{Var}(\epsilon_i) = \sigma^2$
* Homoscedasticity

    $$\widehat{s.e.}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^{2} (X^{T}X)^{-1}_{jj}}$$

* Heteroscedasticity

---

# Variance in the error term

$$Y = 2 + 3X + \epsilon, \quad \epsilon \sim N(0,1)$$

```{r sim-homo, echo = FALSE}
sim_homo <- tibble(
  x = runif(1000, 0, 10),
  y = 2 + 3 * x + rnorm(1000, 0, 1)
)
sim_homo_mod <- glm(y ~ x, data = sim_homo)

augment(sim_homo_mod) %>%
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Homoscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

---

# Variance in the error term

$$Y = 2 + 3X + \epsilon, \quad \epsilon \sim N(0, \frac{X}{2})$$

```{r sim-hetero, echo = FALSE}
sim_hetero <- tibble(
  x = runif(1000, 0, 10),
  y = 2 + 3 * x + rnorm(1000, 0, (x / 2))
)
sim_hetero_mod <- glm(y ~ x, data = sim_hetero)

augment(sim_hetero_mod) %>%
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Heteroscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

---

# Monotonic transformations

* Function for transforming a set of numbers into a different set of numbers so that the rank order of the original set of numbers is preserved

--

## Ladder of powers

Transformation | Power | $f(X)$
---------------|-------|--------
Cube | 3 | $X^3$
Square | 2 | $X^2$
Identity | 1 | $X$
Square root | $\frac{1}{2}$ | $\sqrt{X}$
Cube root | $\frac{1}{3}$ | $\sqrt[3]{X}$
Log | 0 (sort of) | $\log(X)$

---

# Ladder of powers

```{r power-ladder, echo = FALSE}
tibble(x = runif(1000, 0, 10),
           cube = x^3,
           square = x^2,
           identity = x,
           sqrt = sqrt(x),
           cubert = x ^ (1/3),
           log = log(x)) %>%
  gather(transform, value, -x) %>%
  mutate(transform = factor(transform,
                            levels = c("cube", "square", "identity", "sqrt", "cubert", "log"),
                            labels = c("X^3", "X^2", "X", "sqrt(X)", "sqrt(X, 3)", "log(X)"))) %>%
  ggplot(aes(x, value)) +
  geom_line() +
  facet_wrap( ~ transform, scales = "free_y", labeller = label_parsed) +
  labs(title = "Ladder of powers transformations",
       x = "X",
       y = "Transformed X")
```

---

# Which transformation should I use?

.center[
```{r bulge-rule, echo = FALSE, fig.asp = 1, fig.width = 8}
# from http://freakonometrics.hypotheses.org/14967
fakedataMT <- function(p = 1, q = 1, n = 500, s = .1) {
  X <- seq(1 / (n + 1), 1 - 1 / (n + 1), length = n)
  Y <- (5 + 2 * X ^ p + rnorm(n, sd = s)) ^ (1 / q)
  return(tibble(x = X, y = Y))
}

bind_rows(`1` = fakedataMT(p = .5, q = 2),
          `2` = fakedataMT(p = 3, q = -5),
          `3` = fakedataMT(p = .5, q = -1),
          `4` = fakedataMT(p = 3, q = 5),
          .id = "id") %>%
  mutate(id = factor(id, levels = 1:4,
                     labels = c("Log X or Square Y", "Square X or Y",
                                "Log X or Y", "Square X or Log Y"))) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  facet_wrap(~ id, scales = "free_y") +
  labs(title = 'Tukey and Mosteller\'s "Bulging Rule" for monotone transformations to linearity',
       x = "X",
       y = "Y") +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_blank())
```
]

---

# Interpreting log transformations

$$\log(Y_i) = \beta_0 + \beta_{1}X_i + \epsilon_i$$

$$\E(Y) = e^{\beta_0 + \beta_{1}X_i}$$

$$\frac{\partial \E(Y)}{\partial X} = e^{\beta_1}$$

$$Y_i = \beta_0 + \beta_{1} \log(X_i) + \epsilon_i$$

---

# Log-log regressions

$$\log(Y_i) = \beta_0 + \beta_{1} \log(X_i) + \dots + \epsilon_i$$

* $\beta_1$ - elasticity of $Y$ with respect to $X$

    $$\text{Elasticity}_{YX} = \frac{\% \Delta Y}{\% \Delta X}$$

---

# Polynomial regressions

$$y_i = \beta_0 + \beta_{1}x_{i} + \epsilon_{i}$$

$$y_i = \beta_0 + \beta_{1}x_{i} + \beta_{2}x_i^2 + \beta_{3}x_i^3 + \dots + \beta_{d}x_i^d + \epsilon_i$$

---

# Biden and age

$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$

```{r biden-age}
# get data
biden <- read_csv(here("static", "data", "biden.csv"))

# estimate model
biden_age <- glm(biden ~ age + I(age^2) + I(age^3) + I(age^4), data = biden)

# estimate the predicted values and confidence interval
biden_pred <- augment(biden_age, newdata = biden) %>%
  mutate(pred_low = .fitted - 1.96 * .se.fit,
         pred_high = .fitted + 1.96 * .se.fit)

# plot the curve
ggplot(biden_pred, aes(age)) +
  geom_point(data = biden, aes(age, biden), alpha = .05) +
  geom_line(aes(y = .fitted)) +
  geom_line(aes(y = pred_low), linetype = 2) +
  geom_line(aes(y = pred_high), linetype = 2) +
  labs(title = "Polynomial regression of Biden feeling thermometer",
       subtitle = "With 95% confidence interval",
       x = "Age",
       y = "Predicted Biden thermometer rating")
```

---

# Biden and age

```{r biden-matrix}
vcov(biden_age) %>%
  kable(caption = "Variance-covariance matrix of Biden polynomial regression",
        digits = 5,
        format = "html")
```

---

# Biden and age

```{r biden-margins, dependson = "biden-age"}
# plot marginal effect
cplot(biden_age, "age", dx = "age", what = "effect", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = biden, aes(x = age)) +
  labs(title = "Average marginal effect of age",
       x = "Age",
       y = "Marginal effect of age")
```

---

# Voter turnout and mental health

$$\Pr(\text{Voter turnout} = \text{Yes} | \text{mhealth}) = \frac{\exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}{1 + \exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}$$

```{r mhealth, results = "hide", fig.show = "asis"}
# load data
mh <- read_csv(here("static", "data", "mental_health.csv")) %>%
  na.omit

# estimate model
mh_mod <- glm(vote96 ~ mhealth_sum + I(mhealth_sum^2) +
                I(mhealth_sum^3) + I(mhealth_sum^4), data = mh,
              family = binomial)
tidy(mh_mod)

# log-odds
## predicted probability
pred_mh <- cplot(mh_mod, "mhealth_sum", what = "prediction",
                 type = "link", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Log-odds of voting",
       x = "Mental health",
       y = "Predicted log-odds of voting")

## marginal effect
ame_mh <- cplot(mh_mod, "mhealth_sum", what = "effect",
                 type = "link", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Average marginal effect",
       x = "Mental health",
       y = "Marginal effect of mental health")

pred_mh +
  ame_mh
```

---

# Voter turnout and mental health

```{r mhealth-prob, dependson = "mhealth", results = "hide", fig.show = "asis"}
# probability
## predicted probability
pred_mh <- cplot(mh_mod, "mhealth_sum", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Predicted probability of voting",
       x = "Mental health",
       y = "Predicted probability of voting")

## marginal effect
ame_mh <- cplot(mh_mod, "mhealth_sum", what = "effect", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Average marginal effect",
       x = "Mental health",
       y = "Marginal effect of mental health")

pred_mh +
  ame_mh
```

---

# Step functions

* Global transformations
* Local transformations
* Step functions
    * Split $X$ into bins
    * Fit a different constant to each bin

---

# Step functions

* Create $c_1, c_2, \dots, c_K$ cutpoints in the range of $X$
* Construct $K + 1$ new indicator variables $C_1(X), C_2(X), \dots, C_K(X)$
* Fit the linear regression model to the new indicator variables as predictors:

$$y_i = \beta_0 + \beta_1 C_1 (x_i) + \beta_2 C_2 (x_i) + \dots + \beta_K C_K (x_i) + \epsilon_i$$

---

# Age and voting

```{r vote96-step, results = "hide", fig.show = "asis"}
{
  # stepwise model with 5 intervals
  glm(vote96 ~ cut_interval(age, 5), data = mh, family = binomial) %>%
    prediction %>%
    ggplot(aes(x = age)) + 
    geom_line(aes(y = fitted)) +
    geom_line(aes(y = fitted + 1.96 * se.fitted), linetype = 2) +
    geom_line(aes(y = fitted - 1.96 * se.fitted), linetype = 2) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
    geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
    labs(title = "5 intervals",
         x = "Age",
         y = "Predicted probability of voting")
} + {
  # stepwise model with 10 intervals
  glm(vote96 ~ cut_interval(age, 10), data = mh, family = binomial) %>%
    prediction %>%
    ggplot(aes(x = age)) + 
    geom_line(aes(y = fitted)) +
    geom_line(aes(y = fitted + 1.96 * se.fitted), linetype = 2) +
    geom_line(aes(y = fitted - 1.96 * se.fitted), linetype = 2) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
    geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
    labs(title = "10 intervals",
         x = "Age",
         y = "Predicted probability of voting")
} + {
  # stepwise model with 25 intervals
  glm(vote96 ~ cut_interval(age, 25), data = mh, family = binomial) %>%
    prediction %>%
    ggplot(aes(x = age)) + 
    geom_line(aes(y = fitted)) +
    geom_line(aes(y = fitted + 1.96 * se.fitted), linetype = 2) +
    geom_line(aes(y = fitted - 1.96 * se.fitted), linetype = 2) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
    geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
    labs(title = "25 intervals",
         x = "Age",
         y = "Predicted probability of voting")
} + {
  # plain model
  glm(vote96 ~ age, data = mh, family = binomial) %>%
    cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
    ggplot(aes(x = xvals)) + 
    geom_line(aes(y = yvals)) +
    geom_line(aes(y = upper), linetype = 2) +
    geom_line(aes(y = lower), linetype = 2) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
    geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
    labs(title = "No step function",
         x = "Age",
         y = "Predicted probability of voting")
} + {
  # polynomial model
  glm(vote96 ~ poly(age, degree = 5, raw = TRUE), data = mh, family = binomial) %>%
    cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
    ggplot(aes(x = xvals)) + 
    geom_line(aes(y = yvals)) +
    geom_line(aes(y = upper), linetype = 2) +
    geom_line(aes(y = lower), linetype = 2) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
    geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
    labs(title = "Fifth-order polynomial",
         x = "Age",
         y = "Predicted probability of voting")
}
```

---

# Basis functions

* Family of functions or transformations applied to a variable $X$

    $$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \ldots + \beta_K b_K(x_i) + \epsilon_i$$
    
* Fix the functional form
* Uses least squares estimation

---

# Regression splines

* Fit separate polynomial functions over different regions of $X$

---

# Piecewise polynomial

* Fit separate low-degree polynomials for different regions of $X$
* Cubic piecewise polynomial regression model

    $$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$

    * $K$ knots

--
* Piecewise cubic polynomial with 0 knots

    $$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$

* Piecewise constant polynomial = piecewise constant regression

---

# Piecewise cubic polynomial

$$y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i & \text{if } x_i < c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i & \text{if } x_i \geq c
   \end{cases}$$

* $K = 1$
* Increasing $K$

---

# Piecewise cubic regression

```{r sim-piecewise, echo = FALSE}
# simulate data
sim_piece <- tibble(
  x = runif(100, 0, 10),
  y = ifelse(x < 5,
             .05 * x + .05 * x^2 + .05 * x^3 + rnorm(100, 0, 3),
             .1 * x + .1 * x^2 - .05 * x^3 + rnorm(100, 0, 3))
)

# estimate models
sim_piece_mod1 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece, subset = x < 5)
sim_piece_mod2 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece, subset = x >= 5)

# draw the plot
sim_piece_grid <- tibble(
  x = seq(0, 10, by = .001)
)

bind_rows(
  sim_piece_mod1 = augment(sim_piece_mod1, newdata = sim_piece_grid),
  sim_piece_mod2 = augment(sim_piece_mod2, newdata = sim_piece_grid),
  .id = "model"
) %>%
  filter((x < 5 & model == "sim_piece_mod1") |
           (x >=5 & model == "sim_piece_mod2")) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_piece) +
  geom_line(aes(y = .fitted, color = model), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  labs(title = "Piecewise cubic regression",
       x = "X",
       y = "Y")
```

---

# Constraints and splines

```{r sim-spline, echo = FALSE}
###### very hackish implementation
# simulate data
sim_piece_cont <- sim_piece %>%
  mutate(y = ifelse(x < 5,
                    15 + .05 * x - .5 * x^2 - .05 * x^3,
                    .05 * x + .1 * x^2 - .05 * x^3))

# estimate models
sim_piece_cont_mod1 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece_cont, subset = x < 5)
sim_piece_cont_mod2 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece_cont, subset = x >= 5)

# draw the plot
bind_rows(
  sim_piece_cont_mod1 = augment(sim_piece_cont_mod1, newdata = sim_piece_grid),
  sim_piece_cont_mod2 = augment(sim_piece_cont_mod2, newdata = sim_piece_grid),
  .id = "model"
) %>%
  filter((x < 5 & model == "sim_piece_cont_mod1") |
           (x >=5 & model == "sim_piece_cont_mod2")) %>%
  ggplot() +
  geom_point(data = sim_piece %>%
               mutate(y = ifelse(x < 5,
                                 15 + .05 * x - .5 * x^2 - .05 * x^3 + rnorm(100, 0, 3),
                                 .05 * x + .1 * x^2 - .05 * x^3) + rnorm(100, 0, 3)), aes(x, y)) +
  geom_line(aes(x, .fitted, color = model), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  labs(title = "Continuous piecewise cubic regression",
       x = "X",
       y = "Y")
```

---

# Constraints and splines

```{r sim-spline-smooth, echo = FALSE}
# estimate models
sim_piece_smooth <- glm(y ~ bs(x, knots = c(5)), data = sim_piece)

# draw the plot
augment(sim_piece_smooth, newdata = sim_piece) %>%
  ggplot(aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = .fitted), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  labs(title = "Cubic spline",
       x = "X",
       y = "Y")
```

---

# Increasing $K$

```{r sim-spline-smooth-5, echo = FALSE}
tibble(
  terms = c(1, 5, 10),
  models = map(terms, ~ glm(y ~ bs(x, df = . + 3), data = sim_piece)),
  pred = map(models, augment, newdata = sim_piece)
) %>%
  unnest(pred) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_piece, alpha = .2) +
  geom_line(aes(y = .fitted, color = factor(terms))) +
  scale_color_brewer(type = "qual") +
  labs(title = "Cubic spline",
       x = "X",
       y = "Y",
       color = "Knots")
```

---

# Spline basis representation

* Regression splines as basis functions
* Cubic spline with $K$ knots

    $$y_i = \beta_0 + \beta_1 b_1 (x_i) + \beta_2 b_2 (x_i) + \cdots + \beta_{K + 3} b_{K + 3} (x_i) + \epsilon_i$$
* Imposing constraints with truncated power basis

    $$
    h(x, \zeta) = (x - \zeta)_+^3 = 
    \begin{cases} 
      (x - \zeta)^3 & \text{if } x > \zeta \\
      0 & \text{otherwise}
    \end{cases}
    $$

* Cubic spline with $K$ knots requires
    * Intercept
    * $3 + K$ predictors

    $$X, X^2, X^3, h(X, \zeta_1), h(X, \zeta_2), \ldots, h(X, \zeta_K)$$

* Degrees of freedom

---

# Spline basis representation

## $K = 4$

```{r basis-spline-output}
sim_piece %>%
  select(x) %>%
  bind_cols(splines = bs(sim_piece$x, df = 7) %>%
              as_tibble)
```

---

# Basis splines vs. natural splines

* High variance at the outer range of the predictors

```{r vote-age-basis-spline, dependson = "mhealth", results = "hide", fig.show = "asis"}
# estimate model
glm(vote96 ~ bs(age, df = 6), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  labs(title = "Predicted probability of voting",
       subtitle = "Basis spline with 3 knots",
       x = "Age",
       y = "Predicted probability of voting")
```

---

# Boundary constraints

* Linear function at the boundaries
* Natural spline

```{r vote-age-natural-spline, dependson = "mhealth", results = "hide", fig.show = "asis"}
# estimate basis spline model
vote_age_basis <- glm(vote96 ~ bs(age, df = 6), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)
vote_age_natural <- glm(vote96 ~ ns(age, df = 6), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)

bind_rows(
  Basis = vote_age_basis,
  Natural = vote_age_natural,
  .id = "model"
) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals, color = model)) +
  geom_line(aes(y = upper, color = model), linetype = 2) +
  geom_line(aes(y = lower, color = model), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  scale_color_brewer(type = "qual") +
  labs(title = "Predicted probability of voting",
       x = "Age",
       y = "Predicted probability of voting",
       color = "Spline") +
  theme(legend.position = "bottom")
```

---

# Choosing the knots

* Uniform placement of knots
* Partition $X$ into $K$ uniform quantiles

---

# Choosing the knots

```{r vote-spline, results = "hide", fig.show = "asis"}
# estimate model
glm(vote96 ~ ns(age, df = 8), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_vline(xintercept = attr(bs(mh$age, df = 8), "knots"),
             linetype = 2, color = "blue") +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  labs(title = "Predicted probability of voting",
       subtitle = "Natural spline with 5 knots",
       x = "Age",
       y = "Predicted probability of voting")
```

---

# Choosing the knots

```{r vote-cv}
# function to simplify things
vote_spline <- function(splits, df = NULL){
  # estimate the model on each fold
  model <- glm(vote96 ~ ns(age, df = df),
                data = analysis(splits))
  
  model_acc <- augment(model, newdata = assessment(splits)) %>%
    accuracy(truth = factor(vote96), estimate = factor(round(.fitted)))
  
  mean(model_acc$.estimate)
}

tune_over_knots <- function(splits, knots){
  vote_spline(splits, df = knots + 3)
}

# estimate CV error for knots in 0:25
results <- vfold_cv(mh, v = 10)

expand(results, id, knots = 1:25) %>%
  left_join(results) %>%
  mutate(acc = map2_dbl(splits, knots, tune_over_knots)) %>%
  group_by(knots) %>%
  summarize(acc = mean(acc)) %>%
  mutate(err = 1 - acc) %>%
  ggplot(aes(knots, err)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Optimal number of knots for natural cubic spline regression",
       x = "Knots",
       y = "10-fold CV error")
```

---

# Choosing the knots

```{r vote-optimal-mod, results = "hide", fig.show = "asis"}
glm(vote96 ~ ns(age, df = 15), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_vline(xintercept = attr(bs(mh$age, df = 15), "knots"),
             linetype = 2, color = "blue") +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  labs(title = "Predicted probability of voting",
       subtitle = "Natural spline with 12 knots",
       x = "Age",
       y = "Predicted probability of voting")
```

---

# Comparison to polynomial regression

```{r vote-spline-poly, dependson = "mhealth", results = "hide", fig.show = "asis"}
# estimate natural spline model with df = 15
vote_age_spline <- glm(vote96 ~ ns(age, df = 15), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)
vote_age_poly <- glm(vote96 ~ poly(age, degree = 15), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)

bind_rows(
  `Natural cubic spline` = vote_age_spline,
  `Polynomial` = vote_age_poly,
  .id = "model"
) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals, color = model)) +
  # geom_line(aes(y = upper, color = model), linetype = 2) +
  # geom_line(aes(y = lower, color = model), linetype = 2) +
  # geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  scale_color_brewer(type = "qual") +
  labs(title = "Predicted probability of voting",
       x = "Age",
       y = "Predicted probability of voting",
       color = NULL) +
  theme(legend.position = "bottom")
```

---

# Goal of regression

$$\min \left\{ \text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2 \right\}$$

---

# No constraints

```{r sim-perfect-fit}
sim_spline_data <- tibble(
  x = runif(100, min = -1, max = 1),
  y = poly(x, degree = 15, raw = TRUE) %>%
    rowSums() + rnorm(100)
)

ggplot(sim_spline_data, aes(x, y)) +
  geom_point() +
  geom_line() +
  labs(x = expression(X),
       y = expression(Y))
```

---

# Smoothing spline

* Minimize

    $$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$

* "Loss + Penalty"
* $g''(t)$
* $\lambda \rightarrow \infty$

---

# Smoothing spline

```{r sim-spline-compare, dependson = "sim-perfect-fit"}
# smooth spline data points
sim_smooth <- smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 6) %>%
  predict %>%
  as_tibble

ggplot(sim_spline_data, aes(x, y)) +
  geom_point() +
  geom_smooth(aes(color = "Cubic spline"),
              method = "lm", formula = y ~ bs(x, df = 6), se = FALSE) +
  geom_smooth(aes(color = "Natural spline"),
              method = "lm", formula = y ~ ns(x, df = 6), se = FALSE) +
  geom_line(data = sim_smooth, aes(color = "Smoothing spline"), size = 1) +
  scale_color_brewer(type = "qual") +
  labs(x = expression(X),
       y = expression(Y),
       color = NULL) +
  theme(legend.position = "bottom")
```

---

# Choosing the smoothing parameter $\lambda$

* Degrees of freedom
* Effective degrees of freedom
    * $\lambda$ increases from $0$ to $\infty$
    * Effective degrees of freedom decreases from $n$ to $2$

---

# Measuring effective $df$

$$\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda \mathbf{y}$$

* $\hat{\mathbf{g}}_\lambda$

    $$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$
* $\mathbf{S}_\lambda$
* Effective degrees of freedom

    $$df_\lambda = \sum_{i=1}^n \{\mathbf{S}_\lambda \}_{ii}$$

---

# Tuning smoothing splines

* Number and location of knots
* Value for $\lambda$
* LOOCV

    $$\text{RSS}_{cv}(\lambda) = \sum_{i=1}^n (y_i - \hat{g}_\lambda^{(-i)} (x_i))^2 = \sum_{i=1}^n \left[ \frac{y_i - \hat{g}_\lambda (x_i)}{1 - \{ \mathbf{S}_\lambda \}_{ii}} \right]^2$$

    * $\hat{g}_\lambda^{(-i)} (x_i)$
    * $\hat{g}_\lambda (x_i)$

---

# Tuning smoothing splines

```{r sim-smooth-spline-compare, dependson = "sim-perfect-fit"}
list(
  `50` = smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 50),
  `20` = smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 20),
  `2` = smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 2),
  `11` = smooth.spline(sim_spline_data$x, sim_spline_data$y)
) %>%
  map_df(predict, .id = "df") %>%
  mutate(df = factor(df, levels = c(2, 11, 20, 50),
                     labels = c("2", "11 (CV)", "20", "50"))) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_spline_data, alpha = .2) +
  geom_line(aes(color = df)) +
  scale_color_brewer(type = "qual", palette = "Dark2", guide = FALSE) +
  facet_wrap(~ df) +
  labs(title = "Effective degrees of freedom",
       x = expression(X),
       y = expression(Y))
```

---

# Kernel functions

* Weighting function used for non-parametric estimation techniques
* Non-negative real-valued integrable function $K$
* Normalization

    $$\int_{-\infty}^{+\infty} K(u) du = 1$$
    
* Symmetry

    $$K(-u) = K(u) \text{ for all values of } u$$
    
---

# Gaussian kernel

$$K(u) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2} u^2}$$

```{r gaussian}
x <- rnorm(1000)

qplot(x, geom = "blank") +
  stat_function(fun = dnorm) +
  labs(title = "Gaussian (normal) kernel",
       x = NULL,
       y = NULL)
```

---

# Rectangular (uniform) kernel

$$K(u) = \frac{1}{2} \mathbf{1}_{\{ |u| \leq 1 \} }$$

```{r uniform}
x <- runif(1000, -1.5, 1.5)
x_lines <- tribble(
  ~x, ~y, ~xend, ~yend,
  -1, 0, -1, .5,
  1, 0, 1, .5
)

qplot(x, geom = "blank") +
  stat_function(fun = dunif, args = list(min = -1), geom = "step") +
  # geom_segment(data = x_lines, aes(x = x, y = y, xend = xend, yend = yend)) +
  labs(title = "Rectangular kernel",
       x = NULL,
       y = NULL)
```

---

# Triangular kernel

$$K(u) = (1 - |z|) \mathbf{1}_{\{ |u| \leq 1 \} }$$

```{r triangular}
triangular <- function(x) {
  (1 - abs(x)) * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(fun = triangular) +
  labs(title = "Triangular kernel",
       x = NULL,
       y = NULL)
```

---

# Tricube kernel

$$K(u) = \frac{70}{81} (1 - |u|^3)^3 \mathbf{1}_{\{ |u| \leq 1 \} }$$

```{r tricube}
tricube <- function(x) {
  (70 / 81) * (1 - abs(x)^3)^3 * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(fun = tricube) +
  labs(title = "Tricube kernel",
       x = NULL,
       y = NULL)
```

---

# Epanechnikov kernel

$$K(u) = \frac{3}{4} (1 - u^2) \mathbf{1}_{\{ |u| \leq 1 \} }$$

```{r epanechnikov}
epanechnikov <- function(x) {
  (3 / 4) * (1 - x^2) * ifelse(abs(x) <= 1, 1, 0)
}

qplot(x, geom = "blank") +
  stat_function(fun = epanechnikov) +
  labs(title = "Epanechnikov kernel",
       x = NULL,
       y = NULL)
```

---

# Comparison of kernels

```{r kernels}
qplot(x, geom = "blank") +
  stat_function(aes(color = "Gaussian"), fun = dnorm) +
  stat_function(aes(color = "Epanechnikov"), fun = epanechnikov) +
  stat_function(aes(color = "Rectangular"), fun = dunif,
                args = list(min = -1), geom = "step") +
  stat_function(aes(color = "Triangular"), fun = triangular) +
  stat_function(aes(color = "Tricube"), fun = tricube) +
  scale_color_brewer(type = "qual") +
  labs(x = NULL,
       y = NULL,
       color = NULL) +
  theme(legend.position = c(0.04, 1),
        legend.justification = c(0, 1),
        legend.background = element_rect(fill = "white"))
```

---

# Kernel smoothing

* Method for estimating the regression function $f(X)$ over the domain $\Re^p$
* Fit a different but simple model separately at each query point $x_0$
    * Use only local observations
    * Ensure estimated function $f(X)$ is smooth
* Weighting function $K_\lambda (x_o, x_i)$
* Defining the neighborhood

---

# Connection to nearest neighbors

$$\hat{f}(x) = \text{Ave} (y_i | x_i \in N_k(x))$$

```{r kernel-sim}
kernel_sim <- tibble(
  x = runif(100),
  y = sin(4 * x) + rnorm(100, 0, 1/3)
)

kernel_test <- tibble(
  x = seq(from = 0, to = 1, by = .001)
)
```

```{r sim-naive, dependson = "kernel-sim"}
kernel_test <- kernel_test %>%
  mutate(y = FNN::knn.reg(train = select(kernel_sim, x),
                          test = kernel_test,
                          y = kernel_sim$y,
                          k = 30)$pred)

ggplot(kernel_sim, aes(x, y)) +
  geom_point(alpha = .2) +
  stat_function(fun = function(x) sin(4 * x)) +
  geom_line(data = kernel_test, color = "blue") +
  labs(title = "30-nearest neighbor kernel",
       x = expression(x[0]),
       y = NULL)
```

---

# Kernel smoothing

* Nadaraya-Watson kernel-weighted average

$$\hat{f}(x_0) = \frac{\sum_{i=1}^N K_\lambda(x_0, x_i)y_i}{\sum_{i=1}^N K_\lambda (x_0, x_i)}$$

$$K_\lambda(x_0, x_i) = D \left( \frac{| x - x_0 |}{\lambda} \right)$$


$$D(t) = \begin{cases}
\frac{3}{4} (1 - t^2) & \text{if } |t| \leq 1 \\
0 & \text{otherwise}
\end{cases}$$

---

# Kernel smoothing

```{r sim-epanechnikov, dependson = "kernel-sim"}
ggplot(kernel_sim, aes(x, y)) +
  geom_point(alpha = .2) +
  stat_function(fun = function(x) sin(4 * x)) +
  geom_line(data = as_tibble(ksmooth(kernel_sim$x,
                                     kernel_sim$y,
                                     kernel = "normal",
                                     bandwidth = 0.2,
                                     n.points = 500)),
            color = "blue") +
  labs(title = "Epanechnikov kernel",
       x = expression(x[0]),
       y = NULL)
```

---

# Kernel smoothing

* Defining $\lambda$
    * Constant
    * Adaptive

$$K_\lambda(x_0, x_i) = D \left( \frac{| x - x_0 |}{h_\lambda (x_0)} \right)$$

---

# Tuning parameters

1. Choice of kernel function
1. Bandwidth

---

# Infant mortality

```{r infant}
infant <- read_csv(here("static", "data", "infant.csv")) %>%
  # remove non-countries
  filter(is.na(`Value Footnotes`) | `Value Footnotes` != 1) %>%
  select(`Country or Area`, Year, Value) %>%
  rename(country = `Country or Area`,
         year = Year,
         mortal = Value)

ggplot(infant, aes(mortal)) +
  geom_histogram() +
  labs(title = "Infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Frequency")
```

---

# Infant mortality

```{r infant-kernels, dependson = "infant"}
ggplot(infant, aes(mortal)) +
  geom_density(aes(color = "Gaussian"), kernel = "gaussian") +
  geom_density(aes(color = "Epanechnikov"), kernel = "epanechnikov") +
  geom_density(aes(color = "Rectangular"), kernel = "rectangular") +
  geom_density(aes(color = "Triangular"), kernel = "triangular") +
  geom_density(aes(color = "Tricube"), kernel = "tricube") +
  scale_color_brewer(type = "qual", palette = "Paired") +
  labs(title = "Density estimators of infant mortality rate for 195 nations",
       x = "Infant mortality rate (per 1,000)",
       y = "Density",
       color = "Kernel") +
  theme(legend.position = c(0.96, 1),
        legend.justification = c(1, 1),
        legend.background = element_rect(fill = "white"))
```

---

# Infant mortality

```{r infant-epan, dependson = "infant"}
kernel_bandwidth <- tibble(
  bandwidth = seq(from = 1, to = 15, by = .1)
) %>%
  mutate(model = map(bandwidth, ~ density(x = infant$mortal, bw = .x, kernel = "epanechnikov")),
         pred = map(model, ~ tibble(x = .x$x,
                                    y = .x$y))) %>%
  unnest(pred) %>%
  ggplot(aes(x = x, y = y, group = bandwidth)) +
  geom_line() +
  xlim(0, NA) +
  labs(title = "Epanechnikov kernel",
       subtitle = "Bandwidth = {closest_state}",
       x = "Infant mortality rate (per 1,000)",
       y = "Density") +
  transition_states(bandwidth,
                    transition_length = .05,
                    state_length = .05) + 
  ease_aes("cubic-in-out")

animate(kernel_bandwidth, nframes = length(seq(from = 1, to = 15, by = .1)) * 2)
```

---

# Infant mortality

```{r infant-epan-optimal, dependson = "infant"}
ggplot(infant, aes(mortal)) +
  geom_density(kernel = "epanechnikov") +
  labs(title = "Epanechnikov kernel function",
       subtitle = str_c("Optimal bandwidth = ",
                        round(density(x = infant$mortal, kernel = "epanechnikov")$bw,
                              digits = 2)),
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

---

# Local regression

* Biases in the boundaries of the domain of $X$
* Biases in the interior of the domain of $X$
* Kernel smoothing
* Local linear regression

---

# Local linear regression

* Fit a separate linear function at each target point $x_0$ using only the nearby training observations
* Builds regression model from localized subsets of data

    $$\min_{\alpha(x_0), \beta(x_0)} \sum_{i=1}^N K_\lambda (x_0, x_i) [y_i - \alpha(x_0) - \beta (x_0)x_i]^2$$

* $\hat{f}(x_0) = \hat{\alpha} + \hat{\beta}(x_0)x_0$
* Repeat for varying $x_0$

---

# Local linear regression

```{r loess, echo = FALSE, warning = FALSE, message = FALSE}
library(lattice)

mod <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
fit <- augment(mod)

ggplot(fit, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = .fitted), color = "red") +
  labs(title = "Local linear regression",
       subtitle = "Tricube kernel",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")
```

---

# Local linear regression

```{r loess_buildup, dependson="loess", echo = FALSE, warning = FALSE, message = FALSE}
dat <- ethanol %>%
  crossing(center = unique(ethanol$E)) %>%
  as_tibble %>%
  group_by(center) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= .75) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

dat_build_up <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight)) +
  geom_smooth(aes(group = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center), lty = 2) +
  geom_line(aes(y = .fitted), data = fit, color = "red") +
  labs(title = "Centered over {closest_state}",
       subtitle = "Tricube kernel",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J",
       alpha = "Weight") +
  theme(legend.position = "bottom") +
  transition_states(center,
                    transition_length = 2,
                    state_length = 1) + 
  ease_aes("cubic-in-out")

animate(dat_build_up, nframes = length(unique(ethanol$E)) * 2)
```

---

# Local linear regression

```{r loess_span, dependson="loess", echo = FALSE, warning = FALSE, message = FALSE}
spans <- c(.25, .5, .75, 1)

# create loess fits, one for each span
fits <- data_frame(span = spans) %>%
  group_by(span) %>%
  do(augment(loess(NOx ~ E, ethanol, degree = 1, span = .$span)))

# calculate weights to reproduce this with local weighted fits
dat <- ethanol %>%
  crossing(span = spans, center = unique(ethanol$E)) %>%
  as_tibble %>%
  group_by(span, center) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= span) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

# create faceted plot with changing points, local linear fits, and vertical lines,
# and constant hollow points and loess fit
dat_spans <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight)) +
  geom_smooth(aes(group = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center), lty = 2) +
  geom_point(shape = 1, data = ethanol, alpha = .25) +
  geom_line(aes(y = .fitted), data = fits, color = "red") +
  facet_wrap(~span) +
  ylim(0, 5) +
  ggtitle("x0 = ") +
  labs(title = "Centered over {closest_state}",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J") +
  transition_states(center,
                    transition_length = 2,
                    state_length = 1) + 
  ease_aes("cubic-in-out")

animate(dat_spans, nframes = length(unique(ethanol$E)) * 2)
```

---

# Local polynomial regression

$$\min_{\alpha(x_0), \beta_j(x_0), j = 1, \ldots, d} \sum_{i=1}^N K_\lambda (x_0, x_i) \left[y_i - \alpha(x_0) - \sum_{j=1}^d \beta_j (x_0)x_i^j \right]^2$$

* Bias-variance tradeoff

---

# Local polynomial regression

```{r loess-poly, dependson = "loess"}
ggplot(ethanol, aes(E, NOx)) +
  geom_point(alpha = .2) +
  geom_smooth(method = "loess", se = FALSE, method.args = list(degree = 0),
              aes(color = "Zero-order")) +
  geom_smooth(method = "loess", se = FALSE, method.args = list(degree = 1),
              aes(color = "First-order")) +
  geom_smooth(method = "loess", se = FALSE, method.args = list(degree = 2),
              aes(color = "Second-order")) +
  scale_color_brewer(type = "qual", breaks = c("Zero-order", "First-order", "Second-order")) +
  labs(title = "Local linear regression",
       subtitle = "Tricube kernel",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J",
       color = "Degree") +
  theme(legend.position = "bottom")
```

---

# Higher dimensional local regression

* Pairs of independent variables $X_1, X_2$
    * Two-dimensional neighborhoods
    * Bivariate linear regression models
* $p > 2$
* $n > 1000$

---

# Generalized additive models

* Non-linear linear models with multiple predictors
* Extend the linear model to allow non-linear functions of each predictor
* Maintains additive assumption

---

# GAMs for regression problems

$$y_i = \beta_0 + \beta_{1} X_{i1} + \beta_{2} X_{i2} + \dots + \beta_{p} X_{ip} + \epsilon_i$$

$$y_i = \beta_0 + \sum_{j = 1}^p f_j(x_{ij}) + \epsilon_i$$

$$y_i = \beta_0 + f_1(x_{i1}) + \beta_{2} f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i$$

---

# GAMs for regression models

$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$

* $f_1, f_2$ - cubic spline with 2 knots
* $f_3$ - traditional dummy variable

---

# GAMs for regression models

$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$

```{r biden}
# get data
biden <- read_csv(here("static", "data", "biden.csv"))
```

```{r biden-gam}
# estimate model for splines on age and education plus dichotomous female
biden_gam <- gam(biden ~ bs(age, df = 5) + bs(educ, df = 5) + female, data = biden)

# get graphs of each term
biden_gam_terms <- preplot(biden_gam, se = TRUE, rug = FALSE)

{
  ## age
  tibble(x = biden_gam_terms$`bs(age, df = 5)`$x,
         y = biden_gam_terms$`bs(age, df = 5)`$y,
         se.fit = biden_gam_terms$`bs(age, df = 5)`$se.y) %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit) %>%
    ggplot(aes(x, y)) +
    geom_line() +
    geom_line(aes(y = y_low), linetype = 2) +
    geom_line(aes(y = y_high), linetype = 2) +
    labs(x = "Age",
         y = expression(f[1](age)))
} +
{
  ## education
  tibble(x = biden_gam_terms$`bs(educ, df = 5)`$x,
         y = biden_gam_terms$`bs(educ, df = 5)`$y,
         se.fit = biden_gam_terms$`bs(educ, df = 5)`$se.y) %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit) %>%
    ggplot(aes(x, y)) +
    geom_line() +
    geom_line(aes(y = y_low), linetype = 2) +
    geom_line(aes(y = y_high), linetype = 2) +
    labs(x = "Education",
         y = expression(f[2](education)))
} +
{
  ## gender
  tibble(x = biden_gam_terms$female$x,
         y = biden_gam_terms$female$y,
         se.fit = biden_gam_terms$female$se.y) %>%
    unique %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit,
           x = factor(x, levels = 0:1, labels = c("Male", "Female"))) %>%
    ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
    geom_errorbar() +
    geom_point() +
    labs(x = NULL,
         y = expression(f[3](gender)))
}
```

---

# GAMs for regression models

$$\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon$$

* $f_1, f_2$ - local regression functions
* $f_3$ - traditional dummy variable

---

# GAMs for regression models

```{r biden-gam-local}
# estimate model for splines on age and education plus dichotomous female
biden_gam_local <- gam(biden ~ lo(age) + lo(educ) + female, data = biden)

# get graphs of each term
biden_gam_local_terms <- preplot(biden_gam_local, se = TRUE, rug = FALSE)

{
  ## age
  tibble(x = biden_gam_local_terms$`lo(age)`$x,
         y = biden_gam_local_terms$`lo(age)`$y,
         se.fit = biden_gam_local_terms$`lo(age)`$se.y) %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit) %>%
    ggplot(aes(x, y)) +
    geom_line() +
    geom_line(aes(y = y_low), linetype = 2) +
    geom_line(aes(y = y_high), linetype = 2) +
    labs(x = "Age",
         y = expression(f[1](age)))
} +
{
  ## education
  tibble(x = biden_gam_local_terms$`lo(educ)`$x,
         y = biden_gam_local_terms$`lo(educ)`$y,
         se.fit = biden_gam_local_terms$`lo(educ)`$se.y) %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit) %>%
    ggplot(aes(x, y)) +
    geom_line() +
    geom_line(aes(y = y_low), linetype = 2) +
    geom_line(aes(y = y_high), linetype = 2) +
    labs(x = "Education",
         y = expression(f[2](education)))
} + {
  ## gender
  tibble(x = biden_gam_local_terms$female$x,
         y = biden_gam_local_terms$female$y,
         se.fit = biden_gam_local_terms$female$se.y) %>%
    unique %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit,
           x = factor(x, levels = 0:1, labels = c("Male", "Female"))) %>%
    ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
    geom_errorbar() +
    geom_point() +
    labs(x = NULL,
         y = expression(f[3](gender)))
}
```

---

# GAMs for classification models

```{r titanic-gam}
library(titanic)

# estimate model for splines on age and education plus dichotomous female
titanic_gam <- gam(Survived ~ bs(Age, df = 5) + bs(Fare, df = 5) + Sex, data = titanic_train,
                   family = binomial)

# get graphs of each term
titanic_gam_terms <- preplot(titanic_gam, se = TRUE, rug = FALSE)

{
  ## age
  tibble(x = titanic_gam_terms$`bs(Age, df = 5)`$x,
         y = titanic_gam_terms$`bs(Age, df = 5)`$y,
         se.fit = titanic_gam_terms$`bs(Age, df = 5)`$se.y) %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit) %>%
    ggplot(aes(x, y)) +
    geom_line() +
    geom_line(aes(y = y_low), linetype = 2) +
    geom_line(aes(y = y_high), linetype = 2) +
    labs(title = "GAM of Titanic survival",
         subtitle = "Cubic spline",
         x = "Age",
         y = expression(f[1](age)))
} + {
  ## fare
  tibble(x = titanic_gam_terms$`bs(Fare, df = 5)`$x,
         y = titanic_gam_terms$`bs(Fare, df = 5)`$y,
         se.fit = titanic_gam_terms$`bs(Fare, df = 5)`$se.y) %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit) %>%
    ggplot(aes(x, y)) +
    geom_line() +
    geom_line(aes(y = y_low), linetype = 2) +
    geom_line(aes(y = y_high), linetype = 2) +
    labs(title = "GAM of Titanic survival",
         subtitle = "Cubic spline",
         x = "Fare",
         y = expression(f[2](fare)))
} + {
  ## gender
  tibble(x = titanic_gam_terms$Sex$x,
         y = titanic_gam_terms$Sex$y,
         se.fit = titanic_gam_terms$Sex$se.y) %>%
    unique %>%
    mutate(y_low = y - 1.96 * se.fit,
           y_high = y + 1.96 * se.fit,
           x = factor(x, levels = c("male", "female"), labels = c("Male", "Female"))) %>%
    ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
    geom_errorbar() +
    geom_point() +
    labs(title = "GAM of Titanic survival",
         x = NULL,
         y = expression(f[3](gender)))
}
```

---

# Benefits and drawbacks to GAMs

* Allow for non-linear $f_j$ to each $X_j$
* Potentially more accurate than purely linear model
* Still additive
    * Effects of each predictor are independent from one another
    * Does not capture interactive effects

---

# Multivariate adaptive regression splines

* Combines
    * Stepwise regression
    * Polynomial regression
    * Step functions
* Good for high-dimensional data
* Efficient estimation with cross-validation

---

# Piecewise basis functions

.pull-left[

$$(x - t)_+ = \begin{cases}
x - t, & \text{if } x > t, \\
0, & \text{otherwise}
\end{cases}$$

and

$$(t - x)_+ = \begin{cases}
t - x, & \text{if } x < t, \\
0, & \text{otherwise}
\end{cases}$$

]

.pull-right[

```{r mars-hinge, fig.width = 6}
tibble(
  x = c(0, 1)
) %>%
  ggplot(aes(x)) +
  stat_function(aes(color = "xt"), fun = function(x) ifelse(x > .5, x - .5, 0)) +
  stat_function(aes(color = "tx"), fun = function(x) ifelse(x < .5, .5 - x, 0),
                linetype = 2) +
  scale_color_brewer(type = "qual",
                     labels = c(expression((t - x)["+"]), expression((x - t)["+"]))) +
  labs(title = "MARS basis functions",
       subtitle = expression(t == 0.5),
       x = expression(x),
       y = "Basis function",
       color = NULL) +
  theme(legend.position = "bottom")
```

]

---

# MARS

* Piecewise linear basis functions
* Reflected pairs
* Form pairs for each input $X_j$ with knots at each observed value $x_{ij}$

$$\begin{align}
C &= \{ (X_j - t)_+, (t - X_j)_+ \} \\
t &\in \{ x_{1j}, x_{2j}, \ldots, x_{Nj} \} \\
j &= 1, 2, \ldots, p
\end{align}$$

* $2Np$ basis functions

---

# MARS model construction

* Forward stepwise linear regression
* Use functions from set $C$ and their products

    $$f(X) = \beta_0 + \sum_{m=1}^M \beta_m h_m (X)$$
* Start with constant function $h_0 (X) = 1$
* Test each basis function pair from $C$
* Add to the model $M$ the term of the form

    $$\hat{\beta}_{M+1} h_\lagr (X) \times (X_j - t)_+ + \hat{\beta}_{M + 2} h_\lagr (X) \times (t - X_j)_+, h_\lagr \in M$$

    that produces the largest decrease in training error
* Estimated via least squares
* Continue until model $M$ contains some preset maximum number of terms
* Prune model back

---

# Ames housing data

```{r ames}
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

```{r ames-base, dependson = "ames"}
ames_base <- ggplot(ames_train, aes(Year_Built, Sale_Price)) +
  geom_point(alpha = .05) +
  scale_y_continuous(labels = scales::dollar)
```

```{r ames-poly-step, dependson = c("ames", "ames-base")}
{
  ames_base +
    geom_smooth(method = "lm", se = FALSE) +
    ggtitle("(A) Linear regression")
  } + {
    ames_base +
      geom_smooth(method = "lm", formula = y ~ poly(x, degree = 2), se = FALSE) +
      ggtitle("(B) Degree-2 polynomial regression")
  } + {
    ames_base +
      geom_smooth(method = "lm", formula = y ~ poly(x, degree = 3), se = FALSE) +
      ggtitle("(C) Degree-3 polynomial regression")
  } + {
    ames_base +
      geom_smooth(method = "lm", formula = y ~ cut_interval(x, 3), se = FALSE) +
      ggtitle("(D) Step function regression")
  }
```

---

# MARS for Ames

```{r ames-mars-knots, include = FALSE, dependson = c("ames", "ames-base")}
# estimate the models
set.seed(1234)
year_built_k1 <- earth(
  Sale_Price ~ Year_Built,
  nk = 3,
  pmethod = "none",
  data = ames_train   
)

year_built_k2 <- earth(
  Sale_Price ~ Year_Built,  
  nk = 5,
  pmethod = "none",
  data = ames_train   
)

year_built_k3 <- earth(
  Sale_Price ~ Year_Built,  
  nk = 7,
  pmethod = "none",
  data = ames_train   
)

year_built_k4 <- earth(
  Sale_Price ~ Year_Built,  
  nk = 9,
  pmethod = "none",
  data = ames_train   
)
```

```{r ames-mars-k1, include = FALSE, dependson = "ames-mars-knots"}
k1_cuts <- year_built_k1$cuts
k1_coefs <- year_built_k1$coefficients
```

$$\text{Sale_Price} = \begin{cases}
`r format(k1_coefs[[1]], scientific = FALSE)` `r round(k1_coefs[[3]])` (`r k1_cuts[[3]]` - \text{Year_Built}) & \text{Year_Built} \leq `r k1_cuts[[3]]` \\
`r format(k1_coefs[[1]], scientific = FALSE)` + `r round(k1_coefs[[2]])` (\text{Year_Built} - `r k1_cuts[[2]]`) & \text{Year_Built} > `r k1_cuts[[2]]`
\end{cases}$$

```{r ames-mars-k1-plot, echo = FALSE, dependson = "ames-mars-knots"}
ames_base +
  geom_line(data = prediction(year_built_k1), aes(y = fitted), size = 1, color = "blue") +
  ggtitle("MARS: One knot")
```

---

# MARS for Ames

```{r ames-mars-k2, include = FALSE, dependson = "ames-mars-knots"}
k2_cuts <- year_built_k2$cuts
k2_coefs <- year_built_k2$coefficients
```

$$\text{Sale_Price} = \begin{cases}
`r format(k2_coefs[[1]], scientific = FALSE)` `r round(k2_coefs[[3]])` (`r k2_cuts[[3]]` - \text{Year_Built}) & \text{Year_Built} \leq `r k2_cuts[[3]]` \\
`r format(k2_coefs[[1]], scientific = FALSE)` + `r round(k2_coefs[[2]])` (\text{Year_Built} - `r k2_cuts[[2]]`) & `r k2_cuts[[2]]` < \text{Year_Built} \leq `r k2_cuts[[4]]` \\
`r format(k2_coefs[[1]], scientific = FALSE)` + `r format(k2_coefs[[4]], scientific = FALSE)` (\text{Year_Built} - `r k2_cuts[[4]]`) & \text{Year_Built} > `r k2_cuts[[4]]`
\end{cases}$$

```{r ames-mars-k2-plot, echo = FALSE, dependson = "ames-mars-knots"}
ames_base +
  geom_line(data = prediction(year_built_k2), aes(y = fitted), size = 1, color = "blue") +
  ggtitle("MARS: Two knots")
```

---

# MARS for Ames

```{r ames-mars-plots, echo = FALSE, dependson = "ames-mars-knots"}
{
  ames_base +
    geom_line(data = prediction(year_built_k1), aes(y = fitted), size = 1, color = "blue") +
    ggtitle("(A) One knot")
} + {
  ames_base +
    geom_line(data = prediction(year_built_k2), aes(y = fitted), size = 1, color = "blue") +
    ggtitle("(B) Two knots")
} + {
  ames_base +
    geom_line(data = prediction(year_built_k3), aes(y = fitted), size = 1, color = "blue") +
    ggtitle("(C) Three knots")
} + {
  ames_base +
    geom_line(data = prediction(year_built_k4), aes(y = fitted), size = 1, color = "blue") +
    ggtitle("(4) Four knots")
}
```

---

# Fitting the full model

```{r ames-mars-full, dependson = "ames"}
# Fit a basic MARS model
mars1 <- earth(
  Sale_Price ~ .,  
  data = ames_train   
)

print(mars1)
summary(mars1) %>%
  .$coefficients %>%
  head(10)
```

---

# Model selection

```{r ames-mars-select, dependson = "ames-mars-full"}
plot(mars1, which = 1)
```

---

# Potential interactions

* Interaction of hinge functions

```{r ames-mars-degree2, dependson = "ames"}
# Fit a basic MARS model
mars2 <- earth(
  Sale_Price ~ .,  
  data = ames_train,
  degree = 2
)

# check out the first 10 coefficient terms
summary(mars2) %>%
  .$coefficients %>%
  head(10)
```

---

# Tuning a MARS model

* Degree of interactions
* Number of retained terms
* Grid search

---

# Tuning a MARs model

```{r ames-mars-grid-search, dependson = "ames"}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
  )

# for reproducibiity
set.seed(123)

# cross validated model
tuned_mars <- train(
  x = subset(ames_train, select = -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# plot results
ggplot(tuned_mars)
```

---

# Comparison to other models

```{r ames-mars-compare, dependson = "ames"}
# multiple regression
set.seed(123)
cv_model1 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale")
  )

# principal component regression
set.seed(123)
cv_model2 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# partial least squares regression
set.seed(123)
cv_model3 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# regularized regression
set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 10
)
```

```{r ames-mars-compare-print, dependson = "ames-mars-compare"}
# extract out of sample performance measures
summary(resamples(list(
  Multiple_regression = cv_model1, 
  PCR = cv_model2, 
  PLS = cv_model3,
  Elastic_net = cv_model4,
  MARS = tuned_mars
  )))$statistics$RMSE %>%
  knitr::kable(format = "html") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

---

# Feature importance

```{r ames-iml-prep, dependson = "ames-mars-grid-search"}
predictor_mars <- Predictor$new(
  model = tuned_mars,
  data = select(ames_train, -Sale_Price),
  y = ames_train$Sale_Price
)
```

```{r ames-feature-imp, dependson = "ames-iml-prep"}
imp_mars <- FeatureImp$new(predictor_mars, loss = "mse")
plot(imp_mars) +
  ggtitle("MARS") +
  theme_minimal(base_size = 12)
```

---

# Partial dependence

```{r ames-pdp, dependson = "ames-iml-prep"}
mars_Gr_Liv_Area <- FeatureEffect$new(predictor_mars, "Gr_Liv_Area",
                                      method = "pdp+ice",
                                      center.at = min(ames_train$Gr_Liv_Area),
                                      grid.size = 50)
p1 <- plot(mars_Gr_Liv_Area)

mars_Sale_Type <- FeatureEffect$new(predictor_mars, "Sale_Type",
                                       method = "pdp+ice")
p2 <- plot(mars_Sale_Type)

mars_Overall_Qual <- FeatureEffect$new(predictor_mars, "Overall_Qual",
                                       method = "pdp+ice")
p3 <- plot(mars_Overall_Qual)

mars_Year_Built <- FeatureEffect$new(predictor_mars, "Year_Built",
                                      method = "pdp+ice",
                                      center.at = min(ames_train$Year_Built),
                                      grid.size = 50)
p4 <- plot(mars_Year_Built)

p1 + p2 + p3 + p4
```

---

# MARS

## Advantages

* Accurate if local linear relationships are correct
* Quick computation
* Automated feature selection
* Intuitive non-linear relationships

## Distadvantages

* Not accurate if local linear relationships are incorrect
* Not as accurate as more advanced non-linear algorithms
* Difficult to implement more advanced spline features
