<!DOCTYPE html>
<html>
  <head>
    <title>Dimension Reduction</title>
    <meta charset="utf-8">
    <meta name="author" content="MACS 30100   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Dimension Reduction
### <a href="https://model.uchicago.edu">MACS 30100</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Supervised learning

* Predict `\(Y = Y_1, \ldots, Y_m\)` given `\(X^T = (X_1, \ldots, X_p)\)`
* Use training sample of known outcomes and predictors
* Minimize loss function `\(L(Y, \hat{Y})\)`
* `\(\Pr(X,Y)\)`
* Estimate properties of `\(\Pr(Y | X)\)`

    `$$\mu(x) = \underset{\theta}{\text{argmin}} \E_{Y | X} L(Y, \theta)$$`
    
    `$$\Pr(X,Y) = \Pr(Y | X) \cdot \Pr(X)$$`
    
    * Ignore `\(\Pr(X)\)`

---

# Unsupervised learning

* `\(X^T = (X_1, \ldots, X_p)\)`
* Estimate properties of `\(\Pr(X)\)`
* No objective function
* Much higher dimensionality
* Not worried about how `\(\Pr(X)\)` changes conditional on changes in `\(X\)`

---

# Unsupervised learning with low `\(p\)`



&lt;img src="dimension-reduction_files/figure-html/infant-epan-optimal-1.png" width="864" /&gt;

---

# Unsupervised learning

* Dimension reduction
* Cluster analysis

--

* Metric for success
    

---

# Principal components analysis

* Reduce the dimensionality of the data set through linear combinations of features `\(X_1, X_2, \ldots, X_p\)`
* Common purposes
    * Feature reduction
    * Reduce multicollinearity
    * Data visualization

---

# `USArrests`


```
## # A tibble: 50 x 5
##    State       Murder Assault UrbanPop  Rape
##    &lt;chr&gt;        &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;
##  1 Alabama       13.2     236       58  21.2
##  2 Alaska        10       263       48  44.5
##  3 Arizona        8.1     294       80  31  
##  4 Arkansas       8.8     190       50  19.5
##  5 California     9       276       91  40.6
##  6 Colorado       7.9     204       78  38.7
##  7 Connecticut    3.3     110       77  11.1
##  8 Delaware       5.9     238       72  15.8
##  9 Florida       15.4     335       80  31.9
## 10 Georgia       17.4     211       60  25.8
## # … with 40 more rows
```

---

# Data preparation

* Mean center
* Standardize variance

##### Unscaled


```
## # A tibble: 1 x 4
##   Murder Assault UrbanPop  Rape
##    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1   19.0   6945.     210.  87.7
```

##### Scaled


```
## # A tibble: 1 x 4
##   Murder Assault UrbanPop  Rape
##    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1      1      1.        1 1.000
```

--

* When not to scale

---

# Principal components

* Explain most of the variability in the data with a smaller number of variables
* Low-dimensional representation of high-dimensional data, with minimal information loss
* Small number of dimensions
    * Amount that the observations vary along each dimension
* Linear combination of `\(p\)` features

---

# Principal components

## First principal component

* Data set `\(X_1, X_2, \ldots, X_p\)`

    `$$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + \dots + \phi_{p1}X_p$$`
    
    `$$\max \Var(Z_1)$$`
    
* `\(\phi_1\)` - first principal component loading vector
    * `\(p\)` individual loadings `\(\phi_{11}, \dots, \phi_{p1}\)`
* Normalized `\(\phi\)`
    
    `$$\sum_{j=1}^p \phi_{j1}^2 = 1$$`

---

# Principal components

## Second principal component

`$$Z_2 = \phi_{12}X_1 + \phi_{22}X_2 + \dots + \phi_{p2}X_p$$`

`$$\max \Var(Z_2)$$`

* `\(\sum_{j=1}^p \phi_{j2}^2 = 1\)`
* `\(Z_1, Z_2\)` uncorrelated
    * Orthogonality

---

# Principal components

## `\(n\)`th principal component

* `\(p\)` unique principal components
* As `\(j\)` increases, variance of `\(Z_j\)` constantly decreasing

---

# Estimating principal components

1. Compute the covariance matrix `\(\mathbf{S}\)`

    `$$\mathbf{S} = \dfrac{1}{N} \mathbf{X}' \mathbf{X}$$`
    
1. Compute the `\(K\)` largest **eigenvectors** of `\(\mathbf{S}\)`
    * Principal components of the dataset
    * Vector elements - PC scores
    * Eigenvalue - variance of each PC

---

# Estimating principal components

## Covariance matrix


```
##              Murder   Assault   UrbanPop      Rape
## Murder   1.00000000 0.8018733 0.06957262 0.5635788
## Assault  0.80187331 1.0000000 0.25887170 0.6652412
## UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412
## Rape     0.56357883 0.6652412 0.41134124 1.0000000
```

## Eigen


```
## eigen() decomposition
## $values
## [1] 2.4802416 0.9897652 0.3565632 0.1734301
## 
## $vectors
##            [,1]       [,2]       [,3]        [,4]
## [1,] -0.5358995  0.4181809 -0.3412327  0.64922780
## [2,] -0.5831836  0.1879856 -0.2681484 -0.74340748
## [3,] -0.2781909 -0.8728062 -0.3780158  0.13387773
## [4,] -0.5434321 -0.1673186  0.8177779  0.08902432
```

---

# Recover `\(\phi\)`


```
##                PC1        PC2
## Murder   0.5358995 -0.4181809
## Assault  0.5831836 -0.1879856
## UrbanPop 0.2781909  0.8728062
## Rape     0.5434321  0.1673186
```

* Principal component vector
* Interpreting PC vectors

---

# Calculating PC scores

* Project `\(x_1, \ldots, x_n\)` onto eigenvector
* Calculate principal component scores

--


```
## # A tibble: 50 x 3
##    State           PC1     PC2
##    &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
##  1 Alabama      0.976  -1.12  
##  2 Alaska       1.93   -1.06  
##  3 Arizona      1.75    0.738 
##  4 Arkansas    -0.140  -1.11  
##  5 California   2.50    1.53  
##  6 Colorado     1.50    0.978 
##  7 Connecticut -1.34    1.08  
##  8 Delaware     0.0472  0.322 
##  9 Florida      2.98   -0.0388
## 10 Georgia      1.62   -1.27  
## # … with 40 more rows
```

---

# Visualizing PC scores

&lt;img src="dimension-reduction_files/figure-html/usarrests-pc-scores-plot-1.png" width="864" /&gt;

---

# Biplot

&lt;img src="dimension-reduction_files/figure-html/usarrests-pc-scores-biplot-1.png" width="864" /&gt;

---

# Selecting the number of PCs

* Total variance in a dataset

    `$$\sum_{j=1}^p \Var(X_j) = \sum_{j=1}^p \frac{1}{N} \sum_{i=1}^N x_{ij}^2$$`

* Variance explained by the `\(m\)`th principal component

    `$$\frac{1}{N} \sum_{i=1}^N z_{im}^2 = \frac{1}{N} \sum_{i=1}^N \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2$$`
    
* Proportion of variance explained by the `\(m\)`th principal component

    `$$\text{PVE} = \frac{\sum_{i=1}^N \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2}{\sum_{j=1}^p \sum_{i=1}^N x_{ij}^2}$$`
    
--

    `$$\text{PVE} = \frac{\lambda_m}{\sum_{j=1}^p \lambda_j}$$`

---

# Variance explained

&lt;img src="dimension-reduction_files/figure-html/pca-usarrests-pve-1.png" width="864" /&gt;

---

# Deciding on `\(m\)` principal components

.center[

&lt;iframe src="https://giphy.com/embed/y65VoOlimZaus" width="600" height="549" frameBorder="0" class="giphy-embed" allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;&lt;a href="https://giphy.com/gifs/idk-shrug-power-rangers-y65VoOlimZaus"&gt;via GIPHY&lt;/a&gt;&lt;/p&gt;

]

---

# MNIST data set

.center[

![MNIST digits](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

]

---

# Import data




```
## # A tibble: 60,000 x 785
##       X1    X2    X3    X4    X5    X6    X7    X8    X9   X10   X11   X12
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     5     0     0     0     0     0     0     0     0     0     0     0
##  2     0     0     0     0     0     0     0     0     0     0     0     0
##  3     4     0     0     0     0     0     0     0     0     0     0     0
##  4     1     0     0     0     0     0     0     0     0     0     0     0
##  5     9     0     0     0     0     0     0     0     0     0     0     0
##  6     2     0     0     0     0     0     0     0     0     0     0     0
##  7     1     0     0     0     0     0     0     0     0     0     0     0
##  8     3     0     0     0     0     0     0     0     0     0     0     0
##  9     1     0     0     0     0     0     0     0     0     0     0     0
## 10     4     0     0     0     0     0     0     0     0     0     0     0
## # … with 59,990 more rows, and 773 more variables: X13 &lt;dbl&gt;, X14 &lt;dbl&gt;,
## #   X15 &lt;dbl&gt;, X16 &lt;dbl&gt;, X17 &lt;dbl&gt;, X18 &lt;dbl&gt;, X19 &lt;dbl&gt;, X20 &lt;dbl&gt;,
## #   X21 &lt;dbl&gt;, X22 &lt;dbl&gt;, X23 &lt;dbl&gt;, X24 &lt;dbl&gt;, X25 &lt;dbl&gt;, X26 &lt;dbl&gt;,
## #   X27 &lt;dbl&gt;, X28 &lt;dbl&gt;, X29 &lt;dbl&gt;, X30 &lt;dbl&gt;, X31 &lt;dbl&gt;, X32 &lt;dbl&gt;,
## #   X33 &lt;dbl&gt;, X34 &lt;dbl&gt;, X35 &lt;dbl&gt;, X36 &lt;dbl&gt;, X37 &lt;dbl&gt;, X38 &lt;dbl&gt;,
## #   X39 &lt;dbl&gt;, X40 &lt;dbl&gt;, X41 &lt;dbl&gt;, X42 &lt;dbl&gt;, X43 &lt;dbl&gt;, X44 &lt;dbl&gt;,
## #   X45 &lt;dbl&gt;, X46 &lt;dbl&gt;, X47 &lt;dbl&gt;, X48 &lt;dbl&gt;, X49 &lt;dbl&gt;, X50 &lt;dbl&gt;,
## #   X51 &lt;dbl&gt;, X52 &lt;dbl&gt;, X53 &lt;dbl&gt;, X54 &lt;dbl&gt;, X55 &lt;dbl&gt;, X56 &lt;dbl&gt;,
## #   X57 &lt;dbl&gt;, X58 &lt;dbl&gt;, X59 &lt;dbl&gt;, X60 &lt;dbl&gt;, X61 &lt;dbl&gt;, X62 &lt;dbl&gt;,
## #   X63 &lt;dbl&gt;, X64 &lt;dbl&gt;, X65 &lt;dbl&gt;, X66 &lt;dbl&gt;, X67 &lt;dbl&gt;, X68 &lt;dbl&gt;,
## #   X69 &lt;dbl&gt;, X70 &lt;dbl&gt;, X71 &lt;dbl&gt;, X72 &lt;dbl&gt;, X73 &lt;dbl&gt;, X74 &lt;dbl&gt;,
## #   X75 &lt;dbl&gt;, X76 &lt;dbl&gt;, X77 &lt;dbl&gt;, X78 &lt;dbl&gt;, X79 &lt;dbl&gt;, X80 &lt;dbl&gt;,
## #   X81 &lt;dbl&gt;, X82 &lt;dbl&gt;, X83 &lt;dbl&gt;, X84 &lt;dbl&gt;, X85 &lt;dbl&gt;, X86 &lt;dbl&gt;,
## #   X87 &lt;dbl&gt;, X88 &lt;dbl&gt;, X89 &lt;dbl&gt;, X90 &lt;dbl&gt;, X91 &lt;dbl&gt;, X92 &lt;dbl&gt;,
## #   X93 &lt;dbl&gt;, X94 &lt;dbl&gt;, X95 &lt;dbl&gt;, X96 &lt;dbl&gt;, X97 &lt;dbl&gt;, X98 &lt;dbl&gt;,
## #   X99 &lt;dbl&gt;, X100 &lt;dbl&gt;, X101 &lt;dbl&gt;, X102 &lt;dbl&gt;, X103 &lt;dbl&gt;, X104 &lt;dbl&gt;,
## #   X105 &lt;dbl&gt;, X106 &lt;dbl&gt;, X107 &lt;dbl&gt;, X108 &lt;dbl&gt;, X109 &lt;dbl&gt;,
## #   X110 &lt;dbl&gt;, X111 &lt;dbl&gt;, X112 &lt;dbl&gt;, …
```

---

# Import data



&lt;img src="dimension-reduction_files/figure-html/instances_12_graph-1.png" width="864" /&gt;

---

# Interpret PCA



&lt;img src="dimension-reduction_files/figure-html/pixels-pca-pc12-1.png" width="864" /&gt;

---

# Interpret PCA

&lt;img src="dimension-reduction_files/figure-html/pixels-pca-pc12-facet-1.png" width="864" /&gt;

---

# Interpret PCA

&lt;img src="dimension-reduction_files/figure-html/pixels-pca-pc12-biplot-1.png" width="864" /&gt;

---

# Scree plots

&lt;img src="dimension-reduction_files/figure-html/pixels-pca-pve-1.png" width="864" /&gt;

---

# Non-linear dimension reduction

* Linear combination of features
* Non-linear combination of features
* Global vs. local
    * Global - preserve geometry at all scales
    * Local - map nearby points in high dimensions to nearby points in low dimensions
    
---

# Stochastic Neighbor Embedding (SNE)

* Convert high-demensional Euclidean distances between data points into conditional probabilities representing similarities
* Similarity of `\(x_j\)` to `\(x_i\)`
* `\(p_{j | i}\)`
    * Conditional probability `\(x_i\)` would choose `\(x_j\)` as its neighbor proportionally to their probability density under a Gaussian centered at `\(x_i\)`
* `\(p_{j | i}\)` for close data points
* `\(p_{j | i}\)` for distant data points

`$$p_{j|i} = \frac{\exp(- \| x_i - x_j \|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2 / 2\sigma_i^2)}$$`

---

# Stochastic Neighbor Embedding (SNE)

* Convert low-demensional counterparts' Euclidean distances between data points into conditional probabilities representing similarities
* Similarity of `\(y_j\)` to `\(y_i\)`
* `\(q_{j | i}\)`
    * Conditional probability `\(y_i\)` would choose `\(y_j\)` as its neighbor proportionally to their probability density under a Gaussian centered at `\(y_i\)`
* `\(q_{j | i}\)` for close data points
* `\(q_{j | i}\)` for distant data points

`$$q_{j|i} = \frac{\exp(- \| x_i - x_j \|^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2)}$$`

---

# Stochastic Neighbor Embedding (SNE)

* Minimize the difference between `\(p_{j | i}\)` and `\(q_{j | i}\)`
* Complex cost function
* Difficult to compute
* `\(t\)`-SNE
    * Use `\(t\)`-distribution for `\(q_{j|i}\)`
    * Resolves many optimization problems
    
---

# `\(t\)`-SNE

* Non-linear dimension reduction algorithm
* Finds patterns in the data by identifying observed clusters based on similarity across feature space
* Maps the multi-dimensional data to a lower dimensional space
* Data exploration/visualization
* Construct features from output of `\(t\)`-SNE
* Major tuning parameters
    * Number of dimensions for reduction
    * Effective size of local neighborhood
    * Initial PCA step

---

# MNIST data set



&lt;img src="dimension-reduction_files/figure-html/pixels-tsne-dim-1.png" width="864" /&gt;

---

# MNIST data set

&lt;img src="dimension-reduction_files/figure-html/pixels-tsne-dim-facet-1.png" width="864" /&gt;

---

# Comparison of PCA and `\(t\)`-SNE

&lt;img src="dimension-reduction_files/figure-html/pca-tsne-compare-1.png" width="864" /&gt;

---

# Latent semantic analysis

* Bag-of-words approach

```
 a abandoned abc ability able about above abroad absorbed absorbing abstract
43         0   0       0    0    10     0      0        0         0        1
```

* Properties of bag-of-words representation
    * Sparsity
    * Stop words
    * Correlation between words
* Reduce dimensions of corpus using PCA
    * Identifies words closely related to one another in lower dimensional space
    * Enables keyword searches

---

# Interpretation: `NYTimes`




```
##  [1] "penchant"  "brought"   "structure" "willing"   "yielding" 
##  [6] "bare"      "school"    "halls"     "challenge" "step"     
## [11] "largest"   "lovers"    "intense"   "borders"   "mall"     
## [16] "classic"   "conducted" "mirrors"   "hole"      "location" 
## [21] "desperate" "published" "head"      "paints"    "another"  
## [26] "starts"    "familiar"  "window"    "thats"     "broker"
```

---

# Interpretation: `NYTimes`



&lt;img src="dimension-reduction_files/figure-html/nytimes-PC1-1.png" width="864" /&gt;

---

# Interpretation: `NYTimes`

&lt;img src="dimension-reduction_files/figure-html/nytimes-PC2-1.png" width="864" /&gt;

---

# Interpretation: `NYTimes`

&lt;img src="dimension-reduction_files/figure-html/nytimes-biplot-1.png" width="864" /&gt;

---

# Interpretation: `NYTimes`

&lt;img src="dimension-reduction_files/figure-html/nytimes-plot-dim-1.png" width="864" /&gt;

---

# Topic modeling

* Themes
* Probabilistic topic models
* Latent Dirichlet allocation

---

# Food and animals

1. I ate a banana and spinach smoothie for breakfast.
1. I like to eat broccoli and bananas.
1. Chinchillas and kittens are cute.
1. My sister adopted a kitten yesterday.
1. Look at this cute hamster munching on a piece of broccoli.

---

# LDA document structure

* Decide on the number of words N the document will have
    * [Dirichlet probability distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)
    * Fixed set of `\(k\)` topics
* Generate each word in the document:
    * Pick a topic
    * Generate the word
* LDA backtracks from this assumption

---

# How does LDA learn?

* Randomly assign each word in the document to one of `\(K\)` topics
* For each document `\(d\)`:
    * Go through each word `\(w\)` in `\(d\)`
        * And for each topic `\(t\)`, compute two things:
            1. `\(p(t | d)\)`
            1. `\(p(w | t)\)`
        * Reassign `\(w\)` a new topic `\(t\)` with probability `\(p(t|d) \times p(w|t)\)`
* Rinse and repeat

* Estimate from LDA
    1. The topic mixtures of each document
    1. The words associated to each topic
---

# `USCongress`


```
## # A tibble: 4,449 x 7
##       ID  cong billnum h_or_sen major text                    label        
##    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;        
##  1     1   107    4499 HR          18 To suspend temporarily… Foreign trade
##  2     2   107    4500 HR          18 To suspend temporarily… Foreign trade
##  3     3   107    4501 HR          18 To suspend temporarily… Foreign trade
##  4     4   107    4502 HR          18 To reduce temporarily … Foreign trade
##  5     5   107    4503 HR           5 To amend the Immigrati… Labor and em…
##  6     6   107    4504 HR          21 To amend title 38, Uni… Public lands…
##  7     7   107    4505 HR          15 To repeal subtitle B o… Banking, fin…
##  8     8   107    4506 HR          18 To suspend temporarily… Foreign trade
##  9     9   107    4507 HR          18 To suspend temporarily… Foreign trade
## 10    10   107    4508 HR          18 To suspend temporarily… Foreign trade
## # … with 4,439 more rows
```

---

# `USCongress`







&lt;img src="dimension-reduction_files/figure-html/plot-tf-idf-1.png" width="864" /&gt;

---

# 20 topic LDA model




```
## # A tibble: 102 x 3
##    topic term     beta
##    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;
##  1     1 health 0.418 
##  2     1 care   0.182 
##  3     1 food   0.0896
##  4     1 human  0.0518
##  5     1 cosmet 0.0397
##  6     2 indian 0.0846
##  7     2 presid 0.0672
##  8     2 tribe  0.0545
##  9     2 war    0.0510
## 10     2 nativ  0.0487
## # … with 92 more rows
```

&lt;img src="dimension-reduction_files/figure-html/congress-20-topn-1.png" width="864" /&gt;

---

# Document classification



&lt;img src="dimension-reduction_files/figure-html/congress-model-compare-1.png" width="864" /&gt;

---

# `r/jokes`

&lt;blockquote class="reddit-card" data-card-created="1551898234"&gt;&lt;a href="https://www.reddit.com/r/Jokes/comments/a593r0/twenty_years_from_now_kids_are_gonna_think_baby/"&gt;Twenty years from now, kids are gonna think "Baby it's cold outside" is really weird, and we're gonna have to explain that it has to be understood as a product of its time.&lt;/a&gt; from &lt;a href="http://www.reddit.com/r/Jokes"&gt;r/Jokes&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="//embed.redditmedia.com/widgets/platform.js" charset="UTF-8"&gt;&lt;/script&gt;

---

# `r/jokes` dataset


```
## Observations: 194,553
## Variables: 4
## $ id    &lt;chr&gt; "5tz52q", "5tz4dd", "5tz319", "5tz2wj", "5tz1pc", "5tz1o1"…
## $ title &lt;chr&gt; "I hate how you cant even say black paint anymore", "What'…
## $ body  &lt;chr&gt; "Now I have to say \"Leroy can you please paint the fence?…
## $ score &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 15, 0, 0, 3, 1, 0, 3, 2, 2, …
```


```
## &lt;&lt;DocumentTermMatrix (documents: 4952, terms: 2927)&gt;&gt;
## Non-/sparse entries: 48020/14446484
## Sparsity           : 100%
## Maximal term length: NA
## Weighting          : term frequency (tf)
```

---

# `\(k=4\)`



&lt;img src="dimension-reduction_files/figure-html/jokes_4_topn-1.png" width="864" /&gt;

---

# `\(k=12\)`



&lt;img src="dimension-reduction_files/figure-html/jokes_12_topn-1.png" width="864" /&gt;

---

# Perplexity

* A statistical measure of how well a probability model predicts a sample
* Given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents
* Perplexity for LDA model with 12 topics
    * 1102.4348005

---

# Perplexity



&lt;img src="dimension-reduction_files/figure-html/jokes_lda_compare_viz-1.png" width="864" /&gt;

---

# `\(k=100\)`


```
## # A tibble: 502 x 3
##    topic term         beta
##    &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;
##  1     1 women      0.191 
##  2     1 change     0.140 
##  3     1 light      0.115 
##  4     1 bulb       0.0665
##  5     1 lightbulb  0.0600
##  6     1 light bulb 0.0600
##  7     2 voice      0.0576
##  8     2 ladder     0.0383
##  9     2 quiet      0.0330
## 10     2 success    0.0296
## # … with 492 more rows
```

&lt;img src="dimension-reduction_files/figure-html/jokes_100_topn-1.png" width="864" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
