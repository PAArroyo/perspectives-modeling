<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Neural networks and sequential data</title>
    <meta charset="utf-8" />
    <meta name="author" content="MACS 30200   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Neural networks and sequential data
### <a href="https://github.com/css-research/course/">MACS 30200</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Sequential data

* Text
* Time series

---

# Working with text data

* Natural language processing
* Document classification
* Sentiment analysis
* Author identification
* **Pattern recognition**

---

# Pre-processing text

* Vectorization
* Tokenizations

---

# `\(N\)`-grams

* Groups of `\(N\)` consecutive words
* Bag of words model
* Shallow vs. deep learning

---

# One-hot encoding

* Associate all unique words with unique integer index
* Convert integer index `\(i\)` into a binary vector of size `\(N\)`

---

# One-hot encoding

## List of integer indicies


```
##   [1]    1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941
##  [15]    4  173   36  256    5   25  100   43  838  112   50  670    2    9
##  [29]   35  480  284    5  150    4  172  112  167    2  336  385   39    4
##  [43]  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147
##  [57] 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16
##  [71]   43  530   38   76   15   13 1247    4   22   17  515   17   12   16
##  [85]  626   18    2    5   62  386   12    8  316    8  106    5    4 2223
##  [99] 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25
## [113]  124   51   36  135   48   25 1415   33    6   22   12  215   28   77
## [127]   52    5   14  407   16   82    2    8    4  107  117 5952   15  256
## [141]    4    2    7 3766    5  723   36   71   43  530  476   26  400  317
## [155]   46    7    4    2 1029   13  104   88    4  381   15  297   98   32
## [169] 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476
## [183]   26  480    5  144   30 5535   18   51   36   28  224   92   25  104
## [197]    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113
## [211]  103   32   15   16 5345   19  178   32
```

---

# Word embeddings

![:scale 70%](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)

---

# Word embeddings

* Lower-dimensional representations
* More dense vectors
* Pack more information into a smaller tensor structure
* Learning word embeddings
    * Jointly with classification task
    * Use pretrained word embeddings

---

# Learning word embeddings

![](https://blogs.mathworks.com/images/loren/2017/vecs.png)

--

&gt; No single ideal word-embedding space

---

# Keras and embedding layer

```r
embedding_layer &lt;- layer_embedding(input_dim = 1000, output_dim = 64) 
```

* Maps integer indicies to dense vectors
* Input: 2D tensor of shape `(samples, sequence_length)`
* All sequences in a single batch must be the same shape
    * Padding
    * Truncation
* Output: 3D tensor of shape `(samples, sequence_length, embedding_dimensionality)`

---

# Training the embedding layer




```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = 10000, output_dim = 8, 
                  input_length = maxlen) %&gt;% 
  layer_flatten() %&gt;% 
  layer_dense(units = 1, activation = "sigmoid") 

model %&gt;% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history &lt;- model %&gt;% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)
```

---

# Training the embedding layer

&lt;img src="sequential-data_files/figure-html/imdb-embed-train-hist-1.png" width="864" /&gt;

---

# Pretrained word embeddings

* Use when not enough training data is available
* Computed using word-occurrence statistics
* Doesn't require neural networks to calculate

--

## Example word embeddings

* word2vec
* Global Vectors for Word Representation (GloVe)

---

# Pretrained word embeddings

* Still use `layer_embedding()`
* Use pre-configured weights for the layer, rather than estimating them directly

---

# Pretrained word embeddings














```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %&gt;% 
  layer_flatten() %&gt;% 
  layer_dense(units = 32, activation = "relu") %&gt;% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

```
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## embedding_1 (Embedding)          (None, 100, 100)              1000000     
## ___________________________________________________________________________
## flatten_1 (Flatten)              (None, 10000)                 0           
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 32)                    320032      
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 1)                     33          
## ===========================================================================
## Total params: 1,320,065
## Trainable params: 1,320,065
## Non-trainable params: 0
## ___________________________________________________________________________
```

---

# Setting `layer_embedding()` weights


```r
get_layer(model, index = 1) %&gt;% 
  set_weights(list(embedding_matrix)) %&gt;% 
  freeze_weights()
```

---

# Train and fit model


```r
model %&gt;% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history &lt;- model %&gt;% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
```

---

# Train and fit model

&lt;img src="sequential-data_files/figure-html/glove-fit-plot-1.png" width="864" /&gt;

---

# Pre-trained vs. task-specific embeddings





&lt;img src="sequential-data_files/figure-html/compare-embeddings-plot-1.png" width="864" /&gt;

---

# Pre-trained vs. task-specific embeddings











&lt;img src="sequential-data_files/figure-html/compare-embeddings-plot-large-1.png" width="864" /&gt;

---

# Neural network structures

* Densely connected networks
* No **memory**
* Feed-forward networks

---

# Recurrent neural networks

![](https://cdn-images-1.medium.com/max/1200/1*K6s4Li0fTl1pSX4-WPBMMA.jpeg)

* State
* Reset the state between sequences

---

# Recurrent neural networks

```python
import numpy as np

timesteps = 100
input_features = 32
output_features = 64

inputs = np.random.random((timesteps, input_features))

state_t = np.zeros((output_features,))

W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features,))

successive_outputs = []
for input_t in inputs:
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    successive_outputs.append(output_t)
    state_t = output_t

final_output_sequence = np.concatenate(successive_outputs, axis=0)
```

---

# Recurrent layers in Keras

```r
layer_simple_rnn(units = 32)
```

---

# Recurrent layers in Keras


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;% 
  layer_simple_rnn(units = 32)

summary(model)
```

```
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## embedding_1 (Embedding)          (None, None, 32)              320000      
## ___________________________________________________________________________
## simple_rnn_1 (SimpleRNN)         (None, 32)                    2080        
## ===========================================================================
## Total params: 322,080
## Trainable params: 322,080
## Non-trainable params: 0
## ___________________________________________________________________________
```

---

# Recurrent layers in Keras


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;% 
  layer_simple_rnn(units = 32, return_sequences = TRUE)

summary(model)
```

```
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## embedding_2 (Embedding)          (None, None, 32)              320000      
## ___________________________________________________________________________
## simple_rnn_2 (SimpleRNN)         (None, None, 32)              2080        
## ===========================================================================
## Total params: 322,080
## Trainable params: 322,080
## Non-trainable params: 0
## ___________________________________________________________________________
```

---

# Stacking recurrent layers


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%
  layer_simple_rnn(units = 32)

summary(model)
```

```
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## embedding_3 (Embedding)          (None, None, 32)              320000      
## ___________________________________________________________________________
## simple_rnn_3 (SimpleRNN)         (None, None, 32)              2080        
## ___________________________________________________________________________
## simple_rnn_4 (SimpleRNN)         (None, None, 32)              2080        
## ___________________________________________________________________________
## simple_rnn_5 (SimpleRNN)         (None, None, 32)              2080        
## ___________________________________________________________________________
## simple_rnn_6 (SimpleRNN)         (None, 32)                    2080        
## ===========================================================================
## Total params: 328,320
## Trainable params: 328,320
## Non-trainable params: 0
## ___________________________________________________________________________
```

---

# RNN for IMDB

## Load the data


```
## Loading data...
```

```
## 25000 train sequences
```

```
## 25000 test sequences
```

```
## Pad sequences (samples x time)
```

```
## input_train shape: 25000 500
```

```
## input_test shape: 25000 500
```

---

# RNN for IMDB


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_embedding(input_dim = max_features, output_dim = 32) %&gt;%
  layer_simple_rnn(units = 32) %&gt;%
  layer_dense(units = 1, activation = "sigmoid")

model %&gt;% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history &lt;- model %&gt;% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

---

# RNN for IMDB

&lt;img src="sequential-data_files/figure-html/imdb-rnn-plot-1.png" width="864" /&gt;

---

# Simple RNN

![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)

* Vanishing gradient problem

---

# Long Short-Term Memory (LSTM)

![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

--

* Carryback
* Modulates both next output and state

---

# Long Short-Term Memory (LSTM)


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(input_dim = max_features, output_dim = 32) %&gt;% 
  layer_lstm(units = 32) %&gt;% 
  layer_dense(units = 1, activation = "sigmoid")

model %&gt;% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history &lt;- model %&gt;% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

---

# Long Short-Term Memory (LSTM)

&lt;img src="sequential-data_files/figure-html/imdb-lstm-plot-1.png" width="864" /&gt;

---

# Advanced techniques for RNNs

* Recurrent dropout
* Stacking recurrent layers
* Bidirectional recurrent layers

---

# Weather station






```
## Observations: 420,551
## Variables: 15
## $ `Date Time`       &lt;chr&gt; "01.01.2009 00:10:00", "01.01.2009 00:20:00", …
## $ `p (mbar)`        &lt;dbl&gt; 996.52, 996.57, 996.53, 996.51, 996.51, 996.50…
## $ `T (degC)`        &lt;dbl&gt; -8.02, -8.41, -8.51, -8.31, -8.27, -8.05, -7.6…
## $ `Tpot (K)`        &lt;dbl&gt; 265.40, 265.01, 264.91, 265.12, 265.15, 265.38…
## $ `Tdew (degC)`     &lt;dbl&gt; -8.90, -9.28, -9.31, -9.07, -9.04, -8.78, -8.3…
## $ `rh (%)`          &lt;dbl&gt; 93.3, 93.4, 93.9, 94.2, 94.1, 94.4, 94.8, 94.4…
## $ `VPmax (mbar)`    &lt;dbl&gt; 3.33, 3.23, 3.21, 3.26, 3.27, 3.33, 3.44, 3.44…
## $ `VPact (mbar)`    &lt;dbl&gt; 3.11, 3.02, 3.01, 3.07, 3.08, 3.14, 3.26, 3.25…
## $ `VPdef (mbar)`    &lt;dbl&gt; 0.22, 0.21, 0.20, 0.19, 0.19, 0.19, 0.18, 0.19…
## $ `sh (g/kg)`       &lt;dbl&gt; 1.94, 1.89, 1.88, 1.92, 1.92, 1.96, 2.04, 2.03…
## $ `H2OC (mmol/mol)` &lt;dbl&gt; 3.12, 3.03, 3.02, 3.08, 3.09, 3.15, 3.27, 3.26…
## $ `rho (g/m**3)`    &lt;dbl&gt; 1307.75, 1309.80, 1310.24, 1309.19, 1309.00, 1…
## $ `wv (m/s)`        &lt;dbl&gt; 1.03, 0.72, 0.19, 0.34, 0.32, 0.21, 0.18, 0.19…
## $ `max. wv (m/s)`   &lt;dbl&gt; 1.75, 1.50, 0.63, 0.50, 0.63, 0.63, 0.63, 0.50…
## $ `wd (deg)`        &lt;dbl&gt; 152.3, 136.1, 171.6, 198.0, 214.3, 192.7, 166.…
```

---

# Weather station

&lt;img src="sequential-data_files/figure-html/weather-plot-1.png" width="864" /&gt;

---

# Weather station

&lt;img src="sequential-data_files/figure-html/weather-plot-short-1.png" width="864" /&gt;

---

# Problem statement

&gt; Given data going as far back as `lookback` timesteps and sampled every `steps` timesteps, can you predict the temperature in `delay` timesteps?

* `lookback = 1440`
* `steps = 6`
* `delay = 144`

--

## Data preparation

* Normalize each variable
* Generate batches of data from the recent past with target temperature in the future









---

# Establish a sensible baseline

* Use temperature from 24 hours previous to predict current temperature
* Naive model
* Mean absolute error (MAE)


```
## [1] 0.2789863
```

```
## [1] 2.469732
```

---

# Densely connected feed-forward model


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %&gt;% 
  layer_dense(units = 32, activation = "relu") %&gt;% 
  layer_dense(units = 1)

model %&gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history &lt;- model %&gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

---

# Densely connected feed-forward model

&lt;img src="sequential-data_files/figure-html/weather-dense-plot-1.png" width="864" /&gt;

---

# Recurrent baseline


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %&gt;% 
  layer_dense(units = 1)

model %&gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history &lt;- model %&gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

---

# Recurrent baseline

&lt;img src="sequential-data_files/figure-html/weather-gru-plot-1.png" width="864" /&gt;

---

# Recurrent dropout

* Prevent overfitting
* Similar to dropout in feed-forward networks
* Maintain same pattern of dropout for each timestep
* Add dropout to inner recurrent activations of the layers (i.e. internal loops)

---

# Recurrent dropout


```r
model &lt;- keras_model_sequential() %&gt;% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %&gt;% 
  layer_dense(units = 1)

model %&gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history &lt;- model %&gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

---

# Recurrent dropout

&lt;img src="sequential-data_files/figure-html/weather-dropout-plot-1.png" width="864" /&gt;

---

# Stacking recurrent layers

* If you aren't overfitting, you are underfitting
* Increase network capacity until overfitting occurs
* Add more hidden units
* Add more layers

---

# Stacking recurrent layers

.center[

![](https://dpzbhybb2pdcj.cloudfront.net/allaire/Figures/06fig19_alt.jpg)

]

---

# Bidirectional RNNs

.center[

![](https://dpzbhybb2pdcj.cloudfront.net/allaire/Figures/06fig21.jpg)

]

* RNNs treat information at end of sequence more strongly
* Order matters, but should direction as well?
* Time series data - probably matters
* Text data - not so much
* Combine results of two RNN layers
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cfss.uchicago.edu/slides/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
