---
title: "More Classification Methods"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2)

library(tidyverse)
library(broom)
library(rsample)
library(knitr)
library(patchwork)
library(FNN)
library(kknn)
library(titanic)
library(rcfss)
library(pROC)
library(here)

set.seed(1234)
theme_set(theme_minimal(base_size = 20))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Naive non-parametric regression

.center[

```{r np-data}
n <- 1e06
wage <- tibble(educ = rpois(n, lambda = 12),
               age = rpois(n, lambda = 40),
               prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

ggplot(wage, aes(wage)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Histogram of simulated income data",
       subtitle = "Binwidth = 5",
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

]

---

# Naive non-parametric regression

$$\mu = \E(\text{Income}|\text{Education}) = f(\text{Education})$$
--

.center[

```{r np-wage-cond, dependson = "np-data", fig.height = 6, fig.width = 12}
{
  wage %>%
    group_by(educ) %>%
    summarize(mean = mean(wage),
              sd = sd(wage)) %>%
    ggplot(aes(educ, mean, ymin = mean - sd, ymax = mean + sd)) +
    geom_errorbar() +
    geom_point() +
    labs(title = "Conditional income",
         subtitle = "Plus/minus SD",
         x = "Education level",
         y = "Income, in thousands of dollars")
} +
{
  wage %>%
    filter(educ == 12) %>%
    ggplot(aes(wage)) +
    geom_density() +
    geom_vline(xintercept = mean(wage$wage[wage$educ == 12]), linetype = 2) +
    labs(title = "Conditional distribution of income",
         subtitle = "Education = 12",
         x = "Income, in thousands of dollars",
         y = "Frequency count")
}
```

]

---

# Naive non-parametric regression

$$\mu = \E(Y|x) = f(x)$$

* Continuous $X$ and $Y$
* How to calculate conditional means?

--
* Divide into narrow intervals (bins)
* Quality of bins driven by sample size

---

# Naive non-parametric regression

* Canadian occupational prestige

```{r prestige}
# get data
prestige <- here("static", "data", "prestige.csv") %>%
  read_csv()
```

.center[

```{r prestige-5bins, dependson="prestige", fig.height = 7}
# bin into 5 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 6)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
               as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[1:(length(.)-1)] %>%
  enframe(name = NULL)

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 5",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

]

---

# Naive non-parametric regression

* Decrease bias

.center[

```{r prestige-50bins, dependson="prestige", fig.height = 6}
# bin into 50 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 51)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
               as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[1:(length(.)-1)] %>%
  enframe(name = NULL)

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2, alpha = .25) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 50",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

]

--
* Overfitting and increased variance

---

# Naive non-parametric regression

* Consistent estimator of the population regression curve as $n \rightarrow \infty$
* Practical considerations
    * Large $n$
    * Increase in number of predictors $p$

---

# Naive non-parametric regression

$$
\begin{align}
X_1 &\in \{1, 2, \dots ,10 \} \\
X_2 &\in \{1, 2, \dots ,10 \} \\
X_3 &\in \{1, 2, \dots ,10 \} \\
\end{align}
$$

--

* $10^3 = 1000$ possible combinations of the explanatory variables
* $1000$ conditional expectations of $Y$ given $X$:

$$\mu = \E(Y|x_1, x_2, x_3) = f(x_1, x_2, x_3)$$

* Need substantial $n$ for every combination of $X$

---

# Naive non-parametric regression

.center[

```{r wage-sim-describe, fig.width = 12}
wage %>%
  gather(var, value, -wage) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = 1) +
  facet_grid(. ~ var) +
  labs(title = "Distribution of simulated wage data")
```

]

---

# $n = 1,000,000$

.center[

```{r wage-sim-np}
wage_np <- wage %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                            wage_sd = NA,
                                            n = 0))

# number of unique combos 
wage_unique <- nrow(wage_np)

# n for each unique combo
ggplot(wage_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

]

---

# $n = 10,000,000$

.center[

```{r wage-sim-np-ten}
n <- 1e07
wage10 <- tibble(educ = rpois(n, lambda = 12),
                 age = rpois(n, lambda = 40),
                 prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

wage10_np <- wage10 %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                            wage_sd = NA,
                                            n = 0))

# number of unique combos 
wage10_unique <- nrow(wage10_np)

# n for each unique combo
ggplot(wage10_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

]

---

# $K$-nearest neighbors regression

* Moving average to generate a regression line
* $K$
* Prediction point $x_0$

$$\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in N_0} y_i$$

---

# $K = 1$

.center[

```{r prestige-knn-1}
prestige_knn1 <- knn.reg(select(prestige, income), y = prestige$prestige,
                         test = select(prestige, income), k = 1)

prestige %>%
  mutate(pred = prestige_knn1$pred) %>%
  ggplot(aes(income, prestige)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "1-nearest neighbor regression",
       x = "Income (in dollars)",
       y = "Occupational prestige")
```

]

---

# $K = 9$

.center[

```{r prestige-knn-9}
prestige_knn9 <- knn.reg(select(prestige, income), y = prestige$prestige,
                         test = select(prestige, income), k = 9)

prestige %>%
  mutate(pred = prestige_knn9$pred) %>%
  ggplot(aes(income, prestige)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "9-nearest neighbor regression",
       x = "Income (in dollars)",
       y = "Occupational prestige")
```

]

---

# Linear regression vs. $K$-nearest neighbors

$$f(X) = 2 + X + \epsilon_i$$

.center[

```{r np-p-line, fig.height = 6}
sim <- tibble(x = runif(100, -1,1),
              y = 2 + x + rnorm(100, 0, .2))

sim_knn9 <- knn.reg(select(sim, x), y = sim$y,
                    test = select(sim, x), k = 9)

sim %>%
  mutate(pred = sim_knn9$pred) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  geom_abline(aes(color = "True"), intercept = 2, slope = 1) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(color = "Method")
```

]

---

# Increasing $K$

.center[

```{r np-p-line2}
# estimate test MSE for LM and KNN models
sim_test <- tibble(x = runif(100, -1,1),
                   y = 2 + x + rnorm(100, 0, .2))
mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- tibble(k = 1:10,
                  knn = map(k, ~ knn.reg(select(sim, x), y = sim$y,
                                         test = select(sim_test, x), k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

]

---

# Linear regression vs. $K$-nearest neighbors

$$f(X) = 2 + X + X^2 + X^3 + \epsilon_i$$

.center[

```{r np-p-cubic, fig.height = 6, fig.width = 12}
x_cube <- function(x) {
  2 + x + x^2 + x^3
}

sim <- tibble(x = runif(100, -1,1),
              y = x_cube(x) + rnorm(100, 0, .2))

sim_knn9 <- knn.reg(select(sim, x), y = sim$y,
                    test = select(sim, x), k = 9)

sim_knn9_plot <- sim %>%
  mutate(pred = sim_knn9$pred) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  stat_function(aes(color = "True"), fun = x_cube) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(color = "Method")

# estimate test MSE for LM and KNN models
sim_test <- tibble(x = runif(100, -1,1),
                   y = x_cube(x) + rnorm(100, 0, .2))

mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- tibble(k = 1:10,
                  knn = map(k, ~ knn.reg(select(sim, x), y = sim$y,
                                         test = select(sim_test, x), k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$pred)^2)))

sim_knn9_plot +
{
  ggplot(mse_knn, aes(k, mse)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = mse_lm, linetype = 2) +
    labs(x = "K",
         y = "Test mean squared error") +
    expand_limits(y = 0)
}
```

]

---

# Increasing $K$

$$f(X) = 2 + X + X^2 + X^3 + \epsilon_i$$

* Increased noise parameters

.center[

```{r knn-nonrobust, fig.height = 6, fig.width = 12}
sim_nr <- tibble(x1 = runif(100, -1,1),
                 y = x_cube(x1) + rnorm(100, 0, .2),
                 x2 = rnorm(100, 0, 1),
                 x3 = rnorm(100, 0, 1),
                 x4 = rnorm(100, 0, 1),
                 x5 = rnorm(100, 0, 1),
                 x6 = rnorm(100, 0, 1))
sim_nr_test <- tibble(x1 = runif(100, -1,1),
                      y = x_cube(x1) + rnorm(100, 0, .2),
                      x2 = rnorm(100, 0, 1),
                      x3 = rnorm(100, 0, 1),
                      x4 = rnorm(100, 0, 1),
                      x5 = rnorm(100, 0, 1),
                      x6 = rnorm(100, 0, 1))

sim_pred_knn <- expand.grid(p = 1:6,
                            k = 1:10) %>%
  as_tibble %>%
  mutate(lm = map(p, ~ lm(formula(str_c("y ~ ", str_c("x", seq.int(.), collapse = " + "))),
                          data = sim_nr)),
         mse_lm = map_dbl(lm, ~ mse(., sim_nr_test)),
         knn = map2(p, k, ~ knn.reg(select_(sim_nr, .dots = str_c("x", seq.int(.x))),
                                    y = sim_nr$y,
                                    test = select_(sim_nr_test, .dots = str_c("x", seq.int(.x))),
                                    k = .y)),
         mse_knn = map_dbl(knn, ~ mean((sim_nr_test$y - .$pred)^2)))

ggplot(sim_pred_knn, aes(k, mse_knn)) +
  facet_grid(. ~ p, labeller = labeller(p = label_both)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = mse_lm), linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

]

---

# Weighted $K$-nearest neighbors

* Minkowski distance

    $$\text{Distance}(x_i, y_i) = \left( \sum_{i = 1}^n |x_i - y_i| ^p \right)^\frac{1}{p}$$

    * $p = 1$
    * $p = 2$
    
---

# Weighted $K$-nearest neighbors

.center[

```{r knn-weight, fig.width = 12}
sim <- tibble(x = runif(100, -1,1),
              y = x_cube(x) + rnorm(100, 0, .2))

sim_wknn <- kknn(y ~ x, train = sim, test = sim, k = 5)

sim_wknn_plot <- sim %>%
  mutate(pred = sim_wknn[["fitted.values"]]) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  stat_function(aes(color = "True"), fun = x_cube) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(title = "5-nearest neighbor regression",
       subtitle = "Euclidean distance weighting",
       color = "Method")

# estimate test MSE for LM and KNN models
sim_test <- tibble(x = runif(100, -1,1),
                   y = x_cube(x) + rnorm(100, 0, .2))

mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- tibble(k = 1:10,
                  knn = map(k, ~ kknn(y ~ x, train = sim, test = sim_test, k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$fitted.values)^2)))

sim_wknn_plot +
{
  ggplot(mse_knn, aes(k, mse)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = mse_lm, linetype = 2) +
    labs(x = "K",
         y = "Test mean squared error") +
    expand_limits(y = 0)
}
```

]

---

# Weighted $K$-nearest neighbors

.center[

```{r wknn-nonrobust, fig.width = 12}
sim_pred_wknn <- sim_pred_knn %>%
  mutate(wknn = map2(p, k, ~ kknn(formula(str_c("y ~ ",
                                                str_c("x", seq.int(.x), collapse = " + "))),
                                  train = sim_nr, test = sim_nr_test, k = .y)),
         mse_wknn = map_dbl(wknn, ~ mean((sim_nr_test$y - .$fitted.values)^2)))
sim_pred_lm <- sim_pred_wknn %>%
  select(p, k, mse_lm) %>%
  distinct

sim_pred_wknn %>%
  select(p, k, contains("mse"), -mse_lm) %>%
  gather(method, mse, contains("mse")) %>%
  mutate(method = str_replace(method, "mse_", "")) %>%
  mutate(method = factor(method, levels = c("knn", "wknn"),
                         labels = c("KNN", "Weighted KNN"))) %>%
  ggplot(aes(k, mse, color = method)) +
  facet_grid(. ~ p, labeller = labeller(p = label_both)) +
  geom_line() +
  geom_point() +
  geom_hline(data = sim_pred_lm, aes(yintercept = mse_lm), linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN",
       subtitle = "Traditional and weighted KNN",
       x = "K",
       y = "Test mean squared error",
       method = NULL) +
  expand_limits(y = 0) +
  theme(legend.position = "bottom")
```

]

---

# Estimating KNN on simulated wage data

.center[

```{r wage-sim-knn}
# split into train/test set
wage_split <- initial_split(wage, prop = 0.5)
wage_train <- training(wage_split)
wage_test <- testing(wage_split)

# estimate test MSE for LM and KNN models
mse_lm <- lm(wage ~ educ + age + prestige, data = wage_train) %>%
  mse(wage_test)

mse_knn <- tibble(k = c(1:10, seq(20, 100, by = 10)),
                  knn = map(k, ~ knn.reg(select(wage_train, -wage), y = wage_train$wage,
                                         test = select(wage_test, -wage), k = .)),
                  mse = map_dbl(knn, ~ mean((wage_test$wage - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "KNN on simulated wage data",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

]

---

# KNN on Biden

.center[

```{r biden-knn, fig.width = 12}
biden <- here("static", "data", "biden.csv") %>%
  read_csv()

# split into train/test set
biden_split <- initial_split(biden, p = 0.7)
biden_train <- training(biden_split)
biden_test <- testing(biden_split)

# estimate test MSE for LM and KNN models
mse_lm <- lm(biden ~ ., data = biden_train) %>%
  mse(biden_test)

mse_knn <- tibble(k = c(1:10, seq(20, 100, by = 10)),
                  knn = map(k, ~ knn.reg(select(biden_train, -biden), y = biden_train$biden,
                                         test = select(biden_test, -biden), k = .)),
                  mse = map_dbl(knn, ~ mean((biden_test$biden - .$pred)^2)))

knn_plot <- ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "KNN for Biden",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

```{r biden-wknn, warning = FALSE}
# estimate test MSE for LM and WKNN models
mse_lm <- lm(biden ~ ., data = biden_train) %>%
  mse(biden_test)

mse_knn <- tibble(k = c(1:10, seq(20, 100, by = 10)),
                  knn = map(k, ~ kknn(biden ~ .,
                                      train = biden_train, test = biden_test, k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$fitted.values)^2)))

knn_plot +
{
  ggplot(mse_knn, aes(k, mse)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = mse_lm, linetype = 2) +
    labs(title = "Weighted KNN for Biden",
         x = "K",
         y = "Test mean squared error") +
    expand_limits(y = 0)
}
```

]

---

# $K$-nearest neighbors classification

* Same basic approach, but for classification
* $K$
* Prediction point $x_0$
* Conditional probability for class $j$

$$\Pr(Y = j| X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)$$

---

# $K = 1$

.center[

```{r knn-class1, dependson="bayes-class"}
knn1 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 1, prob = TRUE)
prob1 <- attr(knn1, "prob")

bayes_bound1 <- bind_rows(mutate(bayes_grid,
                                 prob = attr(knn1, "prob"),
                                 y = as.logical(knn1),
                                 cls = TRUE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)),
                          mutate(bayes_grid,
                                 prob = attr(knn1, "prob"),
                                 y = as.logical(knn1),
                                 cls = FALSE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound1, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==1),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

]

---

# $K = 5$

.center[

```{r knn-class5, dependson="bayes-class"}
knn5 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 5, prob = TRUE)
prob5 <- attr(knn5, "prob")

bayes_bound5 <- bind_rows(mutate(bayes_grid,
                                 prob = attr(knn5, "prob"),
                                 y = as.logical(knn5),
                                 cls = TRUE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)),
                          mutate(bayes_grid,
                                 prob = attr(knn5, "prob"),
                                 y = as.logical(knn5),
                                 cls = FALSE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound5, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==5),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

]

---

# Determining optimal $K$

.center[

```{r knn-class-compare, dependson="bayes-class"}
# estimate test MSE for KNN models
sim_test <- tibble(x1 = runif(5000, -1, 1),
                   x2 = runif(5000, -1, 1),
                   logodds = bayes_rule(x1, x2) + rnorm(5000, 0, .5),
                   y = logodds > .5)

mse_knn <- tibble(k = 1:100,
                  knn_train = map(k, ~ class::knn(select(sim_bayes, x1, x2),
                                                  test = select(sim_bayes, x1, x2),
                                                  cl = sim_bayes$y, k = .)),
                  knn_test = map(k, ~ class::knn(select(sim_bayes, x1, x2),
                                                 test = select(sim_test, x1, x2),
                                                 cl = sim_bayes$y, k = .)),
                  mse_train = map_dbl(knn_train, ~ mean(sim_bayes$y != as.logical(.))),
                  mse_test = map_dbl(knn_test, ~ mean(sim_test$y != as.logical(.))))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = sim_bayes_err, linetype = 2) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0)
```

]

---

# Applying KNN to Titanic

.center[

```{r titanic-data}
titanic <- titanic::titanic_train %>%
  as_tibble %>%
  select(-Name, -Ticket, -Cabin, -PassengerId, -Embarked) %>%
  mutate(Female = ifelse(Sex == "female", 1, 0)) %>%
  select(-Sex) %>%
  na.omit

titanic_split <- initial_split(titanic, prop = 0.7)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

```{r titanic-logit, dependson = "titanic-data"}
titanic_logit <- glm(Survived ~ ., data = titanic_train, family = binomial)
titanic_logit_mse <- mse.glm(titanic_logit, titanic_test)
```

```{r titanic-knn-compare, dependson = c("bayes-class", "titanic-data"), warning = FALSE}
# estimate test MSE for KNN models
mse_knn <- tibble(k = 1:100,
                  knn_train = map(k, ~ class::knn(select(titanic_train, -Survived),
                                                  test = select(titanic_train, -Survived),
                                                  cl = titanic_train$Survived, k = .)),
                  knn_test = map(k, ~ class::knn(select(titanic_train, -Survived),
                                                 test = select(titanic_test, -Survived),
                                                 cl = titanic_train$Survived, k = .)),
                  mse_train = map_dbl(knn_train, ~ mean(titanic_test$Survived != .)),
                  mse_test = map_dbl(knn_test, ~ mean(titanic_test$Survived != .)))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = titanic_logit_mse, linetype = 2) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0)
```

]

---

# Naive Bayes classifier

* Simple probabilistic classifier
* Uses Bayes theorem
* Strong assumptions regarding independence
* Efficient and relatively successful

---

# `attrition`

```{r attrition}
# convert some numeric variables to factors
attrition <- attrition %>%
  mutate(
    JobLevel = factor(JobLevel),
    StockOptionLevel = factor(StockOptionLevel),
    TrainingTimesLastYear = factor(TrainingTimesLastYear)
  )

glimpse(attrition)

# Create training and test data sets
split <- initial_split(attrition, prop = .7)
train <- training(split)
test <- testing(split)
```

---

# Bayes theorem

$$\Pr (C_k | X) = \frac{\Pr (C_k) \times \Pr (X | C_k)}{\Pr(X)}$$

* $\Pr (C_k)$ is the **prior** probability of the outcome
* $\Pr (X)$ is the probability of the predictor variables, or the **evidence**
* $\Pr (X | C_k)$ is the **conditional probability** or the **likelihood**
* $\Pr (C_k | X)$ is the **posterior probability**

--

$$\text{Posterior} = \frac{\text{Prior} \times \text{Likelihood}}{\text{Evidence}}$$

--

* Intractable as $m^p$ increases

---

# Simplified classifier

* Assumes predictor variables are conditionally independent of one another given the response value
* Extremely incorrect

--

```{r attrition-cor, dependson = "attrition", fig.height = 6}
train %>%
  filter(Attrition == "Yes") %>%
  select_if(is.numeric) %>%
  cor() %>%
  corrplot::corrplot()
```

---

# Simplified classifier

$$\Pr (C_k | X) = \prod_{i=1}^n \Pr(X_i | C_k)$$

* Only need $m \times p$ probabilities
* Determining probability distribution functions (PDFs)
    * Categorical variables
    * Continuous variables

---

# Assumption of normality

.center[

```{r attrition-normal, fig.width = 12}
train %>% 
  select(Age, DailyRate, DistanceFromHome, HourlyRate, MonthlyIncome, MonthlyRate) %>% 
  gather(metric, value) %>% 
  ggplot(aes(value, fill = metric)) + 
  geom_density(show.legend = FALSE) + 
  scale_fill_brewer(type = "qual") +
  facet_wrap(~ metric, scales = "free")
```

]

---

# Violation of assumptions

## Assumption of normality

* Transform the variable
* Use a different PDF

--

## Laplace smoother

* New observation with feature value that never occurs in the response class
    * $\Pr (X_i | C_k) = 0$
* Add a small number to each of the counts in the frequencies for each feature
* Ensures non-zero probabiity for each class

---

# Benefits/drawbacks

* Simple
* Computationally efficient
* Doesn't require large training dataset
* Scales well to large datasets

--
* Relies on assumption of independence
* Biased posterior probabilities
    * Exact probabilities
    * Relative probabilities

---

# Implementation with `attrition`

```{r attrition-cv, dependson = "attrition", warning = FALSE}
library(caret)

# create response and feature data
features <- setdiff(names(train), "Attrition")
x <- train[, features]
y <- train$Attrition

# set up 10-fold cross validation procedure
train_control <- trainControl(
  method = "cv", 
  number = 10
  )

# train model
nb.m1 <- train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control
  )

# results
confusionMatrix(nb.m1)
```

---

# Hyperparameter tuning

* Hyperparameters
* Parameters
* Hyperparameters for naive Bayes
    * Kernel density estimate for continuous variables versus a Gaussian (normal) density estimate
    * Bandwidth size for KDE
    * Use of Laplace smoother and exact value

---

# Hyperparameter tuning

```{r attrition-tune, dependson = "attrition", warning = FALSE, fig.height = 4}
# set up tuning grid
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)

# train model
nb.m2 <- train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control,
  tuneGrid = search_grid,
  preProc = c("BoxCox", "center", "scale")
  )

# plot search grid results
plot(nb.m2)

# results for best model
confusionMatrix(nb.m2)
```

---

# Test set performance

```{r attrition-final, dependson = "attrition-tune", warning = FALSE}
pred <- predict(nb.m2, newdata = test)
confusionMatrix(pred, test$Attrition)
```

---

# Discriminant analysis

* Logistic regression

    $$\Pr (Y = k | X = x)$$
* Discriminant analysis
    * Model distribution of $X$ separately for each response class
    * Use Bayes' thorem to flip these into estimates for $\Pr (Y = k | X = x)$

---

# Why discriminant analysis?

* When the classes of the reponse variable $Y$ are well-separated
* If $n$ is small and the distribution of the predictors $X$ is approximately normal
* More than two non-ordinal response classes

--

## More restrictive assumptions

* Predictor variables $X$ are drawn from a multivariate Gaussian (aka *normal*) distribution 
* LDA assumes equality of covariances among the predictor variables $X$ across each all levels of $Y$
* Require the number of predictor variables $p$ to be less then the sample size $n$
    * Performance decline as $p$ approaches $n$
* Flexibility of classifiers
    * Bias
    * Variance
    
---

# Running example

```{r default}
(default <- as_tibble(ISLR::Default))

# split into training/test set
split <- initial_split(default, prop = .6)
train <- training(split)
test <- testing(split)
```

---

# Linear discriminant analysis

* Descriminant scores

    $$\hat\delta_k(x) = x \times \frac{\hat\mu_k}{\hat\sigma^2} - \frac{\hat\mu_k^2}{2\hat\sigma^2} + \log(\hat\pi_k)$$

    * $\hat\delta_k(x)$
    * $\hat\mu_k$
    * $\hat\sigma^2$
    * $\hat\pi_k$

* Assumes predictor is drawn from normal distribution with parameters $\mu, \sigma^2$
* Linear function

---

# Linear discriminant analysis

```{r decision-bound, echo = FALSE, fig.width = 12}
# theoretical bounds
density_plot <- tibble(x = c(-5, 5)) %>%
 ggplot(aes(x)) +
 stat_function(aes(color = "neg"), fun = dnorm, args = list(mean = -1.5)) +
 stat_function(aes(color = "pos"), fun = dnorm, args = list(mean = +1.5)) +
 geom_vline(xintercept = 0, linetype = 2) +
 scale_color_brewer(type = "qual", guide = FALSE) +
 labs(x = NULL, 
    y = NULL)

# simulate data
sim_data <- tibble(neg = rnorm(n = 20, mean = -1.5),
    pos = rnorm(n = 20, mean = +1.5)) %>%
 gather(sample, value)

# estimate LDA decision bound
sim_lda <- MASS::lda(sample ~ value, data = sim_data)

sim_lda_bound <- sim_data %>%
 group_by(sample) %>%
 summarize(mean = mean(value)) %>%
 .$mean %>%
 sum / 2

# plot empirical data
sim_plot <- ggplot(sim_data, aes(value, fill = sample)) +
 geom_histogram(position = "identity", alpha = .7, binwidth = .5) +
 geom_vline(xintercept = 0, linetype = 2) +
 geom_vline(xintercept = sim_lda_bound, linetype = 1) +
 scale_fill_brewer(type = "qual", guide = FALSE) +
 labs(x = NULL, 
    y = NULL)

density_plot +
 sim_plot
```

---

# $p > 1$

* Multivariate normal distribution $N(\mu_k, \mathbf{\Sigma})$

    $$\hat\delta_k(x) = x^T\mathbf{Σ}^{-1}\hat\mu_k - \frac{1}{2}\hat\mu_k^T\mathbf{Σ}^{-1} - \hat\mu_k + \log(\hat\pi_k)$$

---

# Estimate and understand model

```{r default-lda, dependson = "default"}
(lda_m1 <- MASS::lda(default ~ balance + student, data = train))
```

---

# Estimate and understand model

.center[

```{r default-lda-plot, dependson = "default-lda"}
plot(lda_m1)
```

]

---

# Make Predictions

```{r default-lda-predict, dependson = "default-lda"}
(df <- tibble(
 balance = rep(c(1000, 2000), 2), 
    student = c("No", "No", "Yes", "Yes"))
 )

(df_pred <- predict(lda_m1, df))
```

---

# Quadratic discriminant analysis

* Removes assumption of common covariance across all $k$ levels of the response variable $Y$
* Each class gets its own covariance matrix
* Assumes that an observation from the $k$th class is of the form $X ∼ N(\mu_k, \mathbf{\Sigma}_k)$

    $$\hat\delta_k(x) = -\frac{1}{2}x^T\mathbf{Σ}^{-1}_kx+x^T\mathbf{Σ}^{-1}_k\hat\mu_k - \frac{1}{2}\hat\mu_k^T\mathbf{Σ}^{-1}_k\hat\mu_k - \frac{1}{2}\log\big|\mathbf{Σ}_k\big|+\log(\hat\pi_k)$$

---

# Estimate and understand model

```{r default-qda, dependson = "default"}
(qda.m1 <- MASS::qda(default ~ balance + student, data = train))
```

---

# Make predictions

```{r default-qda-predict, dependson = "default-qda"}
predict(qda.m1, df)
```
