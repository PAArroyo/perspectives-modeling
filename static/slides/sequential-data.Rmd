---
title: "Neural networks and sequential data"
author: "[MACS 30200](https://github.com/css-research/course/) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = TRUE, message = FALSE, warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(here)
library(keras)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Sequential data

* Text
* Time series

---

# Working with text data

* Natural language processing
* Document classification
* Sentiment analysis
* Author identification
* **Pattern recognition**

---

# Pre-processing text

* Vectorization
* Tokenizations

---

# $N$-grams

* Groups of $N$ consecutive words
* Bag of words model
* Shallow vs. deep learning

---

# One-hot encoding

* Associate all unique words with unique integer index
* Convert integer index $i$ into a binary vector of size $N$

---

# One-hot encoding

## List of integer indicies

```{r one-hot-sparse, echo = FALSE}
# Number of words to consider as features
max_features <- 10000
# Cut texts after this number of words 
# (among top max_features most common words)
maxlen <- 200

# Load the data as lists of integers.
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
x_train[[1]]
```

---

# Word embeddings

![:scale 70%](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)

---

# Word embeddings

* Lower-dimensional representations
* More dense vectors
* Pack more information into a smaller tensor structure
* Learning word embeddings
    * Jointly with classification task
    * Use pretrained word embeddings

---

# Learning word embeddings

![](https://blogs.mathworks.com/images/loren/2017/vecs.png)

--

> No single ideal word-embedding space

---

# Keras and embedding layer

```r
embedding_layer <- layer_embedding(input_dim = 1000, output_dim = 64) 
```

* Maps integer indicies to dense vectors
* Input: 2D tensor of shape `(samples, sequence_length)`
* All sequences in a single batch must be the same shape
    * Padding
    * Truncation
* Output: 3D tensor of shape `(samples, sequence_length, embedding_dimensionality)`

---

# Training the embedding layer

```{r imdb-import, echo = FALSE}
# Number of words to consider as features
max_features <- 10000
# Cut texts after this number of words 
# (among top max_features most common words)
maxlen <- 20

# Load the data as lists of integers.
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

# This turns our lists of integers
# into a 2D integer tensor of shape `(samples, maxlen)`
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
```

```{r imdb-embed-train, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 8, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 1, activation = "sigmoid") 

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)
```

---

# Training the embedding layer

```{r imdb-embed-train-hist, dependson = "imdb-embed-train", echo = FALSE}
plot(history)
```

---

# Pretrained word embeddings

* Use when not enough training data is available
* Computed using word-occurrence statistics
* Doesn't require neural networks to calculate

--

## Example word embeddings

* word2vec
* Global Vectors for Word Representation (GloVe)

---

# Pretrained word embeddings

* Still use `layer_embedding()`
* Use pre-configured weights for the layer, rather than estimating them directly

---

# Pretrained word embeddings

```{r get-imdb, eval = FALSE, include = FALSE}
dir.create("~/Downloads/aclImdb", recursive = TRUE)
download.file(
  "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
  "~/Downloads/aclImdb/aclImdb_v1.tar.gz"
)
untar(
  "~/Downloads/aclImdb/aclImdb_v1.tar.gz",
  exdir = "/home/rstudio-user/Downloads"
)
```

```{r get-imdb-raw, include = FALSE}
imdb_dir <- "~/Downloads/aclImdb"
train_dir <- file.path(imdb_dir, "train")

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), 
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

```{r imdb-raw-vectorize, dependson = "get-imdb-raw", include = FALSE}
maxlen <- 100                 # We will cut reviews after 100 words
training_samples <- 200       # We will be training on 200 samples
validation_samples <- 10000   # We will be validating on 10000 samples
max_words <- 10000            # We will only consider the top 10,000 words in the dataset

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)

# Split the data into a training set and a validation set
# But first, shuffle the data, since we started from data
# where sample are ordered (all negative first, then all positive).
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

```{r get-glove, eval = FALSE, include = FALSE}
dir.create("~/Downloads/glove.6B", recursive = TRUE)
download.file(
  "http://nlp.stanford.edu/data/glove.6B.zip",
  "~/Downloads/glove.6B/glove.6B.zip"
)
unzip(
  "~/Downloads/glove.6B/glove.6B.zip",
  exdir = "~/Downloads/glove.6B"
)
```

```{r import-glove, include = FALSE}
glove_dir = '~/Downloads/glove.6B'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}
```

```{r glove-embed-matrix, dependson = "import-glove", include = FALSE}
embedding_dim <- 100

embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

```{r glove-model}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

---

# Setting `layer_embedding()` weights

```{r glove-set-weights, dependson = "glove-model"}
get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()
```

---

# Train and fit model

```{r glove-fit, dependson = c("glove-model", "glove-set-weights"), echo=TRUE, results='hide'}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
```

---

# Train and fit model

```{r glove-fit-plot, dependson = "glove-fit", echo = FALSE}
plot(history)
```

---

# Pre-trained vs. task-specific embeddings

```{r plot-training-losses, include = FALSE}
plot_losses <- function(losses) {
  loss_names <- names(losses)
  losses <- as.data.frame(losses)
  losses$epoch <- seq_len(nrow(losses))
  losses %>% 
    gather(model, loss, loss_names[[1]], loss_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = loss, colour = model)) +
    geom_point() +
    geom_line()
}
```

```{r compare-pretrained-task-specific, include = FALSE}
pre_train_loss <- history$metrics$val_loss  # save pretrained validation loss

# fit same model using task-specific embeddings
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

task_specific_loss <- history$metrics$val_loss
```

```{r compare-embeddings-plot, echo = FALSE}
plot_losses(losses = list(
  task_specific = task_specific_loss,
  glove_embeddings = pre_train_loss
)) +
  labs(title = "Validation loss",
       subtitle = "200 training observations") +
  theme(legend.position = "bottom")
```

---

# Pre-trained vs. task-specific embeddings

```{r imdb-raw-vectorize-large, dependson = "get-imdb-raw", include = FALSE}
maxlen <- 100                 # We will cut reviews after 100 words
training_samples <- 2000       # We will be training on 200 samples
validation_samples <- 10000   # We will be validating on 10000 samples
max_words <- 10000            # We will only consider the top 10,000 words in the dataset

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)

# Split the data into a training set and a validation set
# But first, shuffle the data, since we started from data
# where sample are ordered (all negative first, then all positive).
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

```{r glove-model-large, include = FALSE}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r glove-set-weights-large, dependson = "glove-model-large", include = FALSE}
get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()
```

```{r glove-fit-large, dependson = c("glove-model-large", "glove-set-weights-large"), include = FALSE}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

large_glove_loss <- history$metrics$val_loss
```

```{r compare-pretrained-task-specific-large, include = FALSE}
# fit same model using task-specific embeddings
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

large_task_specific_loss <- history$metrics$val_loss
```

```{r compare-embeddings-plot-large, echo = FALSE}
plot_losses(losses = list(
  task_specific = large_task_specific_loss,
  glove_embeddings = large_glove_loss
)) +
  labs(title = "Validation loss",
       subtitle = "2000 training observations") +
  theme(legend.position = "bottom")
```

---

# Neural network structures

* Densely connected networks
* No **memory**
* Feed-forward networks

---

# Recurrent neural networks

![](https://cdn-images-1.medium.com/max/1200/1*K6s4Li0fTl1pSX4-WPBMMA.jpeg)

---

# Recurrent neural networks

```python
import numpy as np

timesteps = 100
input_features = 32
output_features = 64

inputs = np.random.random((timesteps, input_features))

state_t = np.zeros((output_features,))

W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features,))

successive_outputs = []
for input_t in inputs:
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    successive_outputs.append(output_t)
    state_t = output_t

final_output_sequence = np.concatenate(successive_outputs, axis=0)
```

---

# Recurrent layers in Keras

```r
layer_simple_rnn(units = 32)
```

---

# Recurrent layers in Keras

```{r recurrent-layer-2d}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32)

summary(model)
```

---

# Recurrent layers in Keras

```{r recurrent-layer-3d}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE)

summary(model)
```

---

# Stacking recurrent layers

```{r recurrent-stack}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32)

summary(model)
```

---

# RNN for IMDB

## Load the data

```{r imdb-load-rnn, echo = FALSE, cache = FALSE}
max_features <- 10000  # Number of words to consider as features
maxlen <- 500  # Cuts off texts after this many words (among the max_features most common words)
batch_size <- 32

cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb 
cat(length(input_train), "train sequences\n")
cat(length(input_test), "test sequences")

cat("Pad sequences (samples x time)\n")
input_train <- pad_sequences(input_train, maxlen = maxlen)
input_test <- pad_sequences(input_test, maxlen = maxlen)
cat("input_train shape:", dim(input_train), "\n")
cat("input_test shape:", dim(input_test), "\n")
```

---

# RNN for IMDB

```{r imdb-rnn-fit, dependson = "imdb-load-rnn", echo=TRUE, results='hide'}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

---

# RNN for IMDB

```{r imdb-rnn-plot, dependson = "imdb-rnn-fit", echo = FALSE}
plot(history)
```

---

# Simple RNN

![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)

---

# Long Short-Term Memory (LSTM)

![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

---

# Long Short-Term Memory (LSTM)

```{r imdb-lstm, dependson = "imdb-load-rnn", echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

---

# Long Short-Term Memory (LSTM)

```{r imdb-lstm-plot, dependson = "imdb-lstm", echo = FALSE}
plot(history)
```

---

# Advanced techniques for RNNs

* Recurrent dropout
* Stacking recurrent layers
* Bidirectional recurrent layers

---

# Weather station

```{r, include = FALSE, eval = FALSE}
dir.create("~/Downloads/jena_climate", recursive = TRUE)
download.file(
  "https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip",
  "~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip"
)
unzip(
  "~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip",
  exdir = "~/Downloads/jena_climate"
)
```

```{r weather-import, include = FALSE}
data_dir <- "~/Downloads/jena_climate"
fname <- file.path(data_dir, "jena_climate_2009_2016.csv")
data <- read_csv(fname)
```

```{r weather-import-glimpse, dependson = "weather-import", echo = FALSE}
glimpse(data)
```

---

# Weather station

```{r weather-plot, dependson = "weather-import", echo = FALSE}
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) +
  geom_line()
```

---

# Weather station

```{r weather-plot-short, dependson = "weather-import", echo = FALSE}
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) +
  geom_line()
```

---

# Problem statement

> Given data going as far back as `lookback` timesteps and sampled every `steps` timesteps, can you predict the temperature in `delay` timesteps?

* `lookback = 1440`
* `steps = 6`
* `delay = 144`

--

## Data preparation

* Normalize each variable
* Generate batches of data from the recent past with target temperature in the future

```{r weather-convert-to-matrix, dependson = "weather-import", include = FALSE}
data <- data.matrix(data[,-1])
```

```{r weather-normalize, dependson = "weather-convert-to-matrix", include = FALSE}
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)
```

```{r weather-generator-function, include = FALSE}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}
```

```{r weather-generate-batches, dependson = c("weather-normalize", "weather-generator"), include = FALSE}
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (300000 - 200001 - lookback) / batch_size

  # This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

---

# Establish a sensible baseline

* Use temperature from 24 hours previous to predict current temperature
* Naive model
* Mean absolute error (MAE)

```{r weather-naive-model, dependson = "weather-generate-batches", echo = FALSE}
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_gen()
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}
mae_naive <- evaluate_naive_method()
mae_naive * std[[2]]
```

---

# Densely connected feed-forward model

```{r weather-dense, dependson = "weather-generate-batches", echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

---

# Densely connected feed-forward model

```{r weather-dense-plot, dependson = "weather-dense", echo = FALSE}
plot(history)
```

---

# Recurrent baseline

```{r weather-gru, dependson = "weather-generate-batches", echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

---

# Recurrent baseline

```{r weather-gru-plot, dependson = "weather-gru", echo = FALSE}
plot(history)
```

---

# Recurrent dropout

* Prevent overfitting
* Similar to dropout in feed-forward networks
* Maintain same pattern of dropout for each timestep
* Add dropout to inner recurrent activations of the layers (i.e. internal loops)

---

# Recurrent dropout

```{r weather-dropout, dependson = "weather-generate-batches", echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

---

# Recurrent dropout

```{r weather-dropout-plot, dependson = "weather-dropout", echo = FALSE}
plot(history)
```

---

# Stacking recurrent layers

* If you aren't overfitting, you are underfitting
* Increase network capacity until overfitting occurs
* Add more hidden units
* Add more layers

---

# Stacking recurrent layers

.center[

![](https://dpzbhybb2pdcj.cloudfront.net/allaire/Figures/06fig19_alt.jpg)

]

---

# Bidirectional RNNs

.center[

![](https://dpzbhybb2pdcj.cloudfront.net/allaire/Figures/06fig21.jpg)

]

* RNNs treat information at end of sequence more strongly
* Order matters, but should direction as well?
* Time series data - probably matters
* Text data - not so much
* Combine results of two RNN layers

