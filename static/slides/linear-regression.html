<!DOCTYPE html>
<html>
  <head>
    <title>Linear Regression</title>
    <meta charset="utf-8">
    <meta name="author" content="MACS 30100   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/plotly-binding/plotly.js"></script>
    <script src="libs/typedarray/typedarray.min.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
    <script src="libs/plotly-main/plotly-latest.min.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Linear Regression
### <a href="https://model.uchicago.edu">MACS 30100</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Linear regression

* Assumes `\(\E(Y|X)\)` is linear in the inputs `\(X_1, \ldots, X_p\)`
* Popular for a multitude of reasons
    * Simple to interpret
    * Easy(ish) to estimate
    * Highly adaptable, even to non-linear relationships

---

# Key concepts

* Simple linear regression model

    `$$\E(Y|X) = \beta_0 + \beta_1 X$$`
--

* Further assume that `\(\Var (\epsilon_i | X = x) = \sigma^2\)` does not depend on `\(x\)`

    `$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$`

    * `\(\E (\epsilon_i | X_i) = 0\)`
    * `\(\Var (\epsilon_i | X_i) = \sigma^2\)`
    * `\(\beta_0\, \beta_1, \sigma^2\)`

---

# Key concepts

* Let `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)` denote estimates of `\(\beta_0\)` and `\(\beta_1\)`
* Fitted line

    `$$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$$`

* Fitted/predicted values

    `$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$`

* Residuals

    `$$\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X)$$`

---

# Residual sum of squares (RSS)

$$
`\begin{align}
\text{RSS}(\beta) &amp;= \sum_{i=1}^N \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^N (Y_i - f(X_i))^2 \\
&amp;= \sum_{i=1}^N \left(Y_i - \beta_0 - \sum_{j=1}^p X_{ij} \beta_j\right)^2
\end{align}`
$$

---

class: center

# Estimating `\(\hat{\beta}\)`

&lt;img src="linear-regression_files/figure-html/sim-plot-1.png" width="504" /&gt;

---

class: center

# Estimating `\(\hat{\beta}\)`

&lt;img src="linear-regression_files/figure-html/sim-random-fit-1.png" width="504" /&gt;

---

# Estimating `\(\hat{\beta}\)`

* Classically desired properties
* Unbiased
    * `\(E(\hat{\beta}) = \beta\)`
    * Estimator that "gets it right" vis-a-vis `\(\beta\)`
* Efficient
    * `\(\min(Var(\hat{\beta}))\)`
    * Not only do we get it right, but for any given sample used to generate the model we never want to be too far off from "right"

---

# Least squares estimator

* Values `\(\hat{\beta}_0, \hat{\beta}_1\)` that minimize the RSS

    `$$\min(\text{RSS})$$`

---

# Least squares estimator

$$
`\begin{aligned}
\text{RSS} &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
\sum_{i=1}^n (\hat{\epsilon}_i)^2 &amp;= \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i))^2\\
f(\beta_0, \beta_1 | x_i, y_i) &amp; = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i )^2
\end{aligned}`
$$

---

# Least squares estimator

$$
`\begin{aligned}
\dfrac{\partial{ f(\beta_0, \beta_1 | x_i, y_i)}}{\partial \beta_0} &amp; = -2 (\sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i))\\
&amp; = \sum_{i=1}^n -2Y_i + 2\beta_0 + 2\beta_1 X_i\\
0 &amp; = \sum_{i=1}^n -2Y_{i} + 2\beta_0 + 2\beta_1 X_i\\
0 &amp; = -2 \sum_{i=1}^n Y_{i} +  2\sum_{i=1}^n \beta_0 +  2\beta_1 \sum_{i=1}^n X_i\\
0 &amp; = -2 \sum_{i=1}^n Y_{i} +  (n \times 2\beta_0) +  2\beta_1 \sum_{i=1}^n X_i\\
n \times 2\beta_0 &amp; = 2 \sum_{i=1}^n Y_i - 2\beta_1 \sum_{i=1}^n X_i\\
\hat \beta_0 &amp; = \dfrac{2 \sum_{i=1}^n Y_i}{2n} - \dfrac{2\beta_1 \sum_{i=1}^n X_i}{2n}\\
&amp; =  \dfrac{\sum_{i=1}^n Y_i}{n} - \beta_1\dfrac{ \sum_{i=1}^n X_i}{n}\\
 \hat \beta_0 &amp; = \bar{Y}_n - \beta_1 \bar{X}_n
\end{aligned}`
$$

---

# Least squares estimator

$$
`\begin{aligned}
\dfrac{\partial{ f(\beta_0, \beta_1 | x_i, y_i)}}{\partial \beta_1} &amp; = \sum_{i=1}^n -2X_i(Y_i - \beta_0 - \beta_1 X_i) \\
&amp; =  \sum_{i=1}^n -2Y_iX_i + 2\beta_0X_i + 2\beta_1 X_i^2\\
0 &amp; =  \sum_{i=1}^n -2Y_iX_i + 2\beta_0 \sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
&amp; =  \sum_{i=1}^n -2Y_iX_i + 2 (\bar{Y}_n - \beta_1 \bar{X}_n) \sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
&amp; = \sum_{i=1}^n -2Y_iX_i + 2\bar{Y}_n \sum_{i=1}^nX_i - 2\beta_1 \bar{X}_n\sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
2\beta_1  \sum_{i=1}^n X_i^2 - 2\beta_1 \bar{X}_n\sum_{i=1}^nX_i  &amp; = \sum_{i=1}^n 2Y_iX_i  - 2\bar{Y}_n \sum_{i=1}^nX_i\\
\beta_1 ( \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i ) &amp; = \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i\\
\hat \beta_1 &amp; = \dfrac{ \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i}{ \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i}\\
 \hat \beta_0 &amp; = \bar{Y}_n - \hat{\beta}_1 \bar{X}_n
\end{aligned}`
$$

`$$\hat{\sigma}^2 = \left( \frac{1}{n - 2} \right) \sum_{i=1}^n \hat{\epsilon}_i^2$$`

---

class: center

# Least squares estimator

&lt;img src="linear-regression_files/figure-html/sim-lm-1.png" width="504" /&gt;

---

# Gauss-Markov theorem

* Among all linear unbiased estimates, the least squares estimate of `\(\beta\)` will have the smallest variance
* Best Linear Unbiased Estimator (BLUE)

---

# Gauss-Markov theorem

$$
`\begin{aligned}
\hat{\beta}_1 &amp;= \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2} \\
&amp;= \frac{\sum (X_i - \bar{X})Y_i}{\sum (X_i - \bar{X})^2} - \frac{\bar{Y}(X_i - \bar{X})}{\sum (X_i - \bar{X})^2} \\
&amp;= \frac{\sum (X_i - \bar{X})Y_i}{\sum (X_i - \bar{X})^2} \\
&amp;= \sum k_i Y_i
\end{aligned}`
$$

---

# Gauss-Markov theorem

* Consider now a new weighting function `\(w_i\)`:

    `$$\tilde{\beta}_1 = \sum w_i Y_i$$`

* What happens to `\(\hat{\beta}_1\)`?

$$
`\begin{align}
\E(\tilde{\beta}_1) &amp;= \sum w_i \E(Y_i) \\
&amp;= \sum w_i (\beta_0 + \beta_1 X_i) \\
&amp;= \beta_0 \sum w_i + \beta_1 \sum w_i X_i
\end{align}`
$$

---

# Do we want this?

$$
`\begin{align}
\text{MSE} (\tilde{\theta}) &amp;= \E (\tilde{\theta} - \theta)^2 \\
&amp;= \Var (\tilde{\theta}) + [\E (\tilde{\theta}) - \theta]^2 + \text{Irreducible error}
\end{align}`
$$

---

# Extensions

* Additive assumption
* Linearity assumption

---

class: center

# Removing the additive assumption

`$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \epsilon_i$$`

&lt;img src="linear-regression_files/figure-html/sim-linear-1.png" width="504" /&gt;

---

# Additive model

`$$\E[Y] = \beta_0 + \beta_1 X + \beta_2 Z$$`

`$$\frac{\delta \E[Y]}{\delta X} = \beta_1$$`

`$$\frac{\delta \E[Y]}{\delta Z} = \beta_2$$`

---

# Multiplicative interaction model

`$$Y = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 XZ + \epsilon_i$$`

* Direct effects
* Constitutive terms
* Interaction term

---

# Multiplicative interaction model

$$
`\begin{align}
\E[Y] &amp; = \beta_0 + \beta_1 X + \beta_2 Z + \beta_3 XZ \\
     &amp; = \beta_0 + \beta_2 Z + (\beta_1 + \beta_3 Z) X
\end{align}`
$$

`$$\frac{\delta \E[Y]}{\delta X} = \beta_1 + \beta_3 Z$$`

`$$\E[Y] = \beta_0 + \beta_2 Z + \psi_1 X$$`

$$
`\begin{align}
\E[Y] &amp; = \beta_0 + \beta_1 X + (\beta_2 + \beta_3 X) Z \\
     &amp; = \beta_0 + \beta_1 X + \psi_2 Z
\end{align}`
$$

---

# Multiplicative interaction model

* Conditional impact
* If `\(Z = 0\)`, then:
    
$$
`\begin{align}
\E[Y] &amp; = \beta_0 + \beta_1 X + \beta_2 (0) + \beta_3 X (0) \\
     &amp; = \beta_0 + \beta_1 X
\end{align}`
$$
        
* If `\(X = 0\)`, then:
    
$$
`\begin{align}
\E[Y] &amp; = \beta_0 + \beta_1 (0) + \beta_2 Z + \beta_3 (0) Z \\
     &amp; = \beta_0 + \beta_2 Z
\end{align}`
$$
* `\(\psi_1 = \beta_1\)` and `\(\psi_2 = \beta_2\)`
* `\(+\beta_3\)` and `\(-\beta_3\)`
* `\(\psi_1\)` and `\(\psi_2\)`

---

# Conducting inference

* Obtaining estimates of parameters

    `$$\hat{\psi}_1 = \hat{\beta}_1 + \hat{\beta}_3 Z$$`
    `$$\hat{\psi}_2 = \hat{\beta}_2 + \hat{\beta}_3 X$$`
    
* Obtaining estimates of standard errors

---

# Conducting inference

1. `\(\text{Var}(aX) = a^2 \text{Var}(X)\)`
1. `\(\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X,Y)\)`
1. `\(\text{Cov}(X, aY) = a \text{Cov}(X,Y)\)`

--

`$$\widehat{\text{Var}(\hat{\psi}_1}) = \widehat{\text{Var} (\hat{\beta}_1)} +Z^2  \widehat{\text{Var} (\hat{\beta}_3)} + 2 Z \widehat{\text{Cov} (\hat{\beta}_1, \hat{\beta}_3)}$$`

`$$\widehat{\text{Var}(\hat{\psi}_2}) = \widehat{\text{Var} (\hat{\beta}_2)} + X^2  \widehat{\text{Var} (\hat{\beta}_3)} + 2 X \widehat{\text{Cov} (\hat{\beta}_2, \hat{\beta}_3)}$$`

* Depend on `\(\beta_1\)`, `\(\beta_2\)`, and/or `\(\beta_3\)`
* Both also depend on the level/value of the interacted variable

---

# Two dichtomous covariates

`$$Y = \beta_0 + \beta_1 D_1 + \beta_2 D_2 + \beta_3 D_1 D_2 + \epsilon_i$$`

$$
`\begin{align}
\E[Y | D_1 = 0, D_2 = 0] &amp; = \beta_0 \\
\E[Y | D_1 = 1, D_2 = 0] &amp; = \beta_0 + \beta_1 \\
\E[Y | D_1 = 0, D_2 = 1] &amp; = \beta_0 + \beta_2 \\
\E[Y | D_1 = 1, D_2 = 1] &amp; = \beta_0 + \beta_1 + \beta_2 + \beta_3 \\
\end{align}`
$$

---

class: center

# Two dichtomous covariates



&lt;img src="linear-regression_files/figure-html/sim-two-dich-hist-1.png" width="504" /&gt;

---

class: center

# Two dichtomous covariates

&lt;img src="linear-regression_files/figure-html/sim-two-dich-box-1.png" width="504" /&gt;

---

class: center

# One dichotomous and one continuous covariate

`$$Y = \beta_0 + \beta_1 X + \beta_2 D + \beta_3 XD + \epsilon_i$$`

$$
`\begin{align}
\E[Y | X, D = 0] &amp; = \beta_0 + \beta_1 X \\
\E[Y | X, D = 1] &amp; = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X
\end{align}`
$$

---

class: center

# One dichotomous and one continuous covariate

&lt;img src="linear-regression_files/figure-html/sim-cont-dich-00-1.png" width="504" /&gt;
    
---

class: center

# One dichotomous and one continuous covariate

&lt;img src="linear-regression_files/figure-html/sim-cont-dich-01-1.png" width="504" /&gt;

---

class: center

# One dichotomous and one continuous covariate

&lt;img src="linear-regression_files/figure-html/sim-cont-dich-10-1.png" width="504" /&gt;
    
---

class: center

# One dichotomous and one continuous covariate

&lt;img src="linear-regression_files/figure-html/sim-cont-dich-11-1.png" width="504" /&gt;

---

class: center

# Two continuous covariates

`$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon_i$$`

<div id="htmlwidget-05b9c782a8fd3363d461" style="width:504px;height:504px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-05b9c782a8fd3363d461">{"x":{"visdat":{"10ccc61f91706":["function () ","plotlyVisDat"]},"cur_data":"10ccc61f91706","attrs":{"10ccc61f91706":{"x":{},"y":{},"z":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"mesh3d"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"No interactive effects","scene":{"xaxis":{"title":"x"},"yaxis":{"title":"z"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"colorbar":{"title":"y","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.146493574896191","rgba(70,52,125,1)"],["0.190616373410984","rgba(66,65,132,1)"],["0.224253007568705","rgba(62,75,137,1)"],["0.250228836886514","rgba(60,82,138,1)"],["0.291934486305727","rgba(56,93,140,1)"],["0.326209193619907","rgba(50,102,142,1)"],["0.379280600792954","rgba(46,115,142,1)"],["0.397583410785944","rgba(44,119,142,1)"],["0.420139777465221","rgba(42,124,142,1)"],["0.429333810706445","rgba(40,126,142,1)"],["0.470045202601279","rgba(38,136,141,1)"],["0.498080797919937","rgba(37,143,140,1)"],["0.541000700006211","rgba(33,154,138,1)"],["0.562182110680781","rgba(33,159,136,1)"],["0.575822903144742","rgba(37,162,134,1)"],["0.611568316733104","rgba(45,171,129,1)"],["0.645450882536929","rgba(50,178,124,1)"],["0.655170901704633","rgba(52,180,123,1)"],["0.696253654305391","rgba(72,189,113,1)"],["0.751316719428228","rgba(99,200,97,1)"],["0.789775434668972","rgba(118,207,85,1)"],["0.814117346400847","rgba(135,211,77,1)"],["0.862073786516716","rgba(165,218,58,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[8.04816533345729,8.22434928501025,3.3654445852153,0.364946236368269,3.79996915580705,6.28987656906247,4.7640117816627,7.23885566694662,0.226492369547486,5.03811662318185,5.61761360615492,8.88117753900588,5.16974742757156,9.58019895711914,3.96645607193932,6.94533568108454,1.53581443941221,3.28084918903187,8.56642819475383,9.48152024298906,3.23623212752864,4.54934092238545,5.53814206970856,2.28626343188807,9.82690526172519,4.68837124295533,2.76980978203937,6.44882505759597,2.51831340137869,9.86949179554358,5.61147326603532,2.54807645222172,3.87672072742134,0.0240206206217408,3.11113549163565,4.2972324648872,6.59013230120763,0.687797029968351,5.90704060159624,7.71460205782205,1.41271182335913,5.31186855630949,0.836777836084366,7.95876267133281,7.25157843204215,1.27059959573671,2.00603187317029,3.0917016020976,1.69681399362162,0.552339260466397,8.91811236739159,7.47520240489393,5.302841484081,3.72481879778206,2.94745797291398,9.26358682336286,7.80398009112105,9.69950777245685,9.03934428002685,6.70921927317977,8.37714975466952,3.47076844424009,1.55835615238175,0.201823045499623,2.72247741930187,0.540669213514775,6.24274265253916,0.644436955917627,0.861750803887844,1.37696248712018,3.51243186742067,7.15639826841652,4.94298217818141,4.22957162605599,4.73920539021492,6.51713666506112,5.88400508509949,9.15643419837579,5.72672461858019,0.307246898300946,8.1344453827478,1.15895134862512,4.7883732826449,8.09884369140491,2.03309499425814,7.27921452606097,4.51833995990455,0.989518007263541,4.16967286728323,9.22004960710183,5.15789825469255,4.73077965434641,9.30695411982015,0.114410580135882,7.14527416741475,7.47487868880853,9.63239298202097,4.75232352502644,8.67370666004717,7.37117777578533],"y":[9.63599362876266,2.06762383226305,0.86197440745309,2.16028020251542,2.39646551199257,1.97160936193541,2.89008239516988,8.04114406695589,3.78249637782574,9.12817219272256,3.53391784243286,9.31487116962671,6.76712712505832,3.50315240211785,1.03627557400614,3.5716314194724,2.98935299972072,9.31865554535761,2.88827181560919,2.86704181926325,3.54038962861523,8.81332290824503,2.72788872011006,9.19631339376792,8.8553642295301,9.42655049962923,7.79015716165304,4.30765833472833,5.94195581041276,0.244148254860193,0.785301218274981,6.33084589382634,9.9832566245459,2.09497768199071,8.25112488353625,3.85581434238702,8.91377026215196,8.93786470405757,0.29286497971043,9.30968592409045,4.11903461208567,0.323756404686719,7.78794288402423,9.36081312596798,6.50974835967645,4.57767816726118,7.287818712648,4.81431701686233,7.22578493179753,6.30069606937468,3.31730134086683,6.24294903595001,4.0099601005204,1.02817067643628,1.77153003169224,3.42947133118287,8.61577875213698,5.1508446293883,0.259501787368208,8.34732032148167,3.89500771416351,2.80427009565756,7.91626436170191,5.04535502754152,4.17730174725875,2.43640792556107,9.26576276542619,5.80275064567104,7.99964147852734,3.49179653450847,1.93316974677145,4.17926222085953,4.76126482244581,1.08395306626335,5.04600166808814,3.7629901827313,6.0607804171741,6.22590514132753,2.98693968681619,9.14626398123801,2.68593011656776,6.76698096096516,3.96517131710425,3.33101626019925,8.19207000080496,9.66553154401481,8.6941087176092,4.78425047360361,2.73081498919055,8.29305890016258,5.22059587528929,6.96373501326889,1.58495984040201,9.90470201941207,2.24171966779977,0.524662870448083,2.70455550635234,3.52500399574637,0.939658659044653,4.61729394970462],"z":[-5.87828295305371,71.567254527472,35.0347017776221,-7.95333966147155,24.0350364381447,53.1826720712706,28.7392938649282,1.97711599990726,-25.5600400827825,-30.9005556954071,30.8369576372206,5.66306369379163,-5.97379697486758,70.7704655500129,39.3018049793318,43.7370426161215,-4.53538560308516,-50.3780635632575,66.7815637914464,76.1447842372581,6.95842498913407,-32.6398198585957,38.102533495985,-59.1004996187985,19.7154103219509,-37.381792566739,-40.2034737961367,31.4116672286764,-24.2364240903407,106.253435406834,58.2617204776034,-27.8276944160461,-51.0653589712456,-10.7095706136897,-41.399893919006,14.4141812250018,-13.2363796094432,-72.5006767408922,66.1417562188581,-5.95083866268396,-17.0632278872654,59.8811215162277,-59.5116504793987,-4.02050454635173,17.418300723657,-23.0707857152447,-42.8178683947772,-7.22615414764732,-45.2897093817592,-47.4835680890828,66.0081102652475,22.3225336894393,22.928813835606,36.9664812134579,21.7592794122174,68.3411549217999,1.88201338984072,55.4866314306855,97.7984249265864,-6.38101048301905,54.8214204050601,16.6649834858254,-53.5790820932016,-38.435319820419,-4.54824327956885,-8.95738712046295,-20.2302011288702,-41.5831368975341,-61.3789067463949,-11.1483404738829,25.7926212064922,39.77136047557,11.817173557356,41.4561855979264,6.93203722126782,37.5414648232982,8.23224667925388,39.3052905704826,37.3978493176401,-78.3901708293706,64.4851526618004,-46.0802961234003,18.2320196554065,57.6782743120566,-51.5897500654683,-13.8631701795384,-31.7576875770465,-27.9473246634007,24.3885787809268,19.2699070693925,9.37302379403263,-12.3295535892248,87.2199427941814,-87.9029143927619,59.0355449961498,79.5021581836045,79.2783747566864,22.2731952928007,87.3404800100252,37.5388382608071],"type":"mesh3d","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":[]}</script>

---

class: center

# Two continuous covariates

`$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon_i$$`

<div id="htmlwidget-f7ed013cdf6fe3058fe0" style="width:504px;height:504px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-f7ed013cdf6fe3058fe0">{"x":{"visdat":{"10ccc44b5d18":["function () ","plotlyVisDat"]},"cur_data":"10ccc44b5d18","attrs":{"10ccc44b5d18":{"x":{},"y":{},"z":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"mesh3d"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Interactive effects","scene":{"xaxis":{"title":"x"},"yaxis":{"title":"z"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"colorbar":{"title":"y","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0507019648088091","rgba(71,22,100,1)"],["0.0789108954096858","rgba(72,31,109,1)"],["0.0980475934599441","rgba(72,36,116,1)"],["0.111526107689558","rgba(72,40,120,1)"],["0.121765801544983","rgba(71,44,122,1)"],["0.132246834884107","rgba(71,47,123,1)"],["0.145805934706166","rgba(70,51,125,1)"],["0.16176100320118","rgba(69,56,128,1)"],["0.171017729752841","rgba(68,59,129,1)"],["0.193918934714037","rgba(66,66,133,1)"],["0.203679085947353","rgba(64,69,134,1)"],["0.225716621172721","rgba(62,75,137,1)"],["0.246712264713393","rgba(60,81,138,1)"],["0.268889290602457","rgba(58,87,139,1)"],["0.284364162762182","rgba(56,91,140,1)"],["0.309405936742705","rgba(53,98,141,1)"],["0.350334847155724","rgba(48,108,142,1)"],["0.375114044055318","rgba(46,114,142,1)"],["0.395938325538962","rgba(45,119,142,1)"],["0.502606413337254","rgba(36,145,140,1)"],["0.569081263638122","rgba(35,161,135,1)"],["0.651669734940523","rgba(51,180,123,1)"],["0.767235580835482","rgba(105,203,92,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[0.223527464549989,7.24546941462904,7.78757037129253,5.10730871465057,6.57541000517085,9.89327820250764,2.0278467121534,3.87163226725534,8.30987721681595,3.7952266796492,9.64907835004851,9.29065940203145,0.0538592296652496,2.37790702376515,8.74980587046593,4.35205332469195,8.90256559709087,0.211085590999573,5.60447209049016,8.76458688639104,3.00457965582609,5.62319245189428,1.35319241788238,9.16642866330221,1.8950272211805,5.94576412579045,2.62160054408014,6.62714179838076,0.176051512826234,8.36984591092914,2.59056031238288,0.92674967367202,8.38128042872995,4.34833128470927,2.25154445739463,6.08842223649845,9.41232555545866,6.11149207688868,4.80041567701846,3.93611857900396,5.44203210156411,1.9871486001648,6.82774366112426,2.84165931632742,1.73864367185161,6.92668381379917,2.19451911281794,3.67857848759741,3.35943928686902,3.68544883560389,9.61875658016652,8.92282051499933,6.6514667798765,9.82503053499386,4.62367837550119,6.41860629199073,1.32083673030138,4.20888785971329,4.65162813896313,4.11023752298206,6.52665229281411,4.63413012446836,3.5919095762074,1.88931362470612,1.87274577328935,8.69125063065439,1.71968087088317,3.50374131696299,4.12970016011968,5.59329241514206,6.33907371666282,8.29093524487689,8.33957844646648,0.953640267252922,0.90864043449983,1.62666414165869,2.11600709240884,7.57770561613142,0.203390447422862,0.590739296749234,5.43817604659125,3.90724822413176,7.63035317184404,1.59477358218282,6.0029880492948,9.09086988307536,8.90419509494677,4.47317949030548,7.20189318992198,1.45272738533095,2.78559957863763,2.96410248847678,9.2600466683507,1.64285036036745,6.71259621158242,8.06792298331857,7.1835001418367,2.8940702858381,1.78307336755097,1.78389850305393],"y":[4.0155403339304,6.30165838403627,1.10872981837019,2.99098595278338,4.71086231758818,1.85701438924298,9.13857951760292,3.74130328884348,3.23644710937515,0.820580208674073,5.25992709212005,0.460175748448819,7.83474172232673,8.01036011427641,0.561761823482811,6.93193548591807,0.854427104350179,6.55715543078259,1.76828360417858,3.90734400134534,6.18232341483235,3.60103343613446,5.32067853724584,4.02409123955294,8.93754825461656,5.63205895014107,1.08517796965316,7.25248777773231,3.30305663403124,7.17049364931881,0.17794425599277,9.67045193305239,9.47250132914633,5.8380112843588,8.608904175926,0.7022262387909,2.60826027952135,0.517706184182316,4.00203726720065,4.4927718443796,4.46373428218067,6.46212553605437,9.63701500557363,8.42071380000561,5.15314315678552,0.13957210816443,6.60196067299694,0.535278252791613,7.57902720710263,3.22596723446622,7.54837168613449,6.91079803276807,0.473549603484571,9.97576673515141,7.70364659372717,2.03142317477614,1.77710645133629,0.780022761318833,4.87360207131132,2.64178652316332,0.269404356367886,7.19225793844089,5.84844623692334,4.26066811429337,1.57439067494124,9.81155110755935,0.146675603464246,8.02317429799587,0.472080428153276,4.2678777105175,0.776987785939127,3.68216092931107,7.09022606024519,6.54058401240036,6.21884216088802,5.55773558327928,7.5335135939531,8.61201020656154,2.97393995570019,3.966229474172,5.72124456986785,0.979236497078091,6.59682283410802,2.52717602066696,4.66107221553102,5.06962716346607,1.34210487827659,5.31116580823436,8.70579259935766,2.59111403021961,9.52343684621155,9.9863171717152,9.78261734824628,9.42831391235813,8.13950371928513,5.14731921488419,4.25750073045492,9.69304307829589,4.49194221757352,3.29965838929638],"z":[-18.9442931973874,476.022841135683,163.131520362306,183.922113839152,338.403989036343,274.082237921053,124.20905623074,156.152795130868,329.679082050672,70.8893437172708,561.425998853377,141.058197974955,-63.5890933887077,144.154384881148,141.033509478967,285.882707168138,166.547318371506,-39.6194881040021,147.464845940018,401.034988798557,163.975393989363,232.7146305285,42.3241573522714,430.288827057307,108.943761996766,348.005992357352,53.8132573009348,484.379189147039,-15.4549700383307,622.152792117048,38.7359138378148,12.1838591383689,793.004691006762,248.959271086136,140.259707630163,106.616458450147,323.538601601638,97.5774313549286,210.098208472999,181.274294625128,262.700830758629,93.6622677717009,639.897967720153,193.497453362347,65.4498025475663,87.5388356811513,110.806873187852,61.1236330040375,222.416938355561,133.486187890975,756.763347195808,646.758329440312,103.277166325932,988.61476582681,335.392159496937,184.260886885285,28.9099775354561,77.1189342880932,234.482106006726,133.268210950329,90.1555649662879,317.717313615031,197.505533836231,66.7838382909814,42.4678858035049,851.543492743681,28.2524049692163,245.916943000507,66.0717035170036,261.969027316159,114.874687827385,361.373321416996,613.78848818817,16.5042054042823,13.4048971676696,61.0949774052038,115.234616940189,652.249735180124,-11.6567853007874,-0.324825671043119,318.30066653269,77.5413179126705,523.696183740653,40.9787111680145,303.222766404965,511.085636183837,205.124538907247,239.198116450768,621.942890247514,36.257956653266,207.906443984151,235.78252896161,910.649225034821,87.0384535658429,542.102943225317,454.487787646758,345.097565124746,222.53375159787,63.0059368671064,53.7049577501272],"type":"mesh3d","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":[]}</script>

---

# Removing the linear assumption

* Linear model

    `$$Y_i = \beta_0 + \beta_{1}X_{i} + \epsilon_{i}$$`

* Polynomial model

    `$$Y_i = \beta_0 + \beta_{1}X_{i} + \beta_{2}X_i^2 + \beta_{3}X_i^3 + \dots + \beta_{d}X_i^d + \epsilon_i$$`

---

class: center

# Joe Biden feeling thermometer

&lt;img src="linear-regression_files/figure-html/biden-1.png" width="504" /&gt;

---

class: center

# Joe Biden feeling thermometer

`$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age}$$`

&lt;img src="linear-regression_files/figure-html/biden-age-1.png" width="504" /&gt;

---

class: center

# Joe Biden feeling thermometer

`$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$`

&lt;img src="linear-regression_files/figure-html/biden-poly-1.png" width="504" /&gt;

---

# Removing the linear assumption

## Pointwise standard error

&lt;table&gt;
&lt;caption&gt;Variance-covariance matrix of Biden polynomial regression&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; (Intercept) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)3 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)4 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 620.00316 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.20765 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00533 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Assumptions of linear regression models

`$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$`

&gt; The key assumptions of linear regression concern the behavior of the errors

---

# Assumptions of linear regression models

* Linearity
* Constant variance
* Normality
* Independence
* Fixed `\(X\)`
* `\(X\)` is not invariant

---

# Linearity

`$$\E(\epsilon_i) \equiv E(\epsilon_i | X_i) = 0$$`

$$
`\begin{aligned}
\mu_i \equiv E(Y_i) \equiv E(Y | X_i) &amp;= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
\mu_i &amp;= \beta_0 + \beta_1 X_i + E(\epsilon_i) \\
\mu_i &amp;= \beta_0 + \beta_1 X_i + 0 \\
\mu_i &amp;= \beta_0 + \beta_1 X_i
\end{aligned}`
$$

---

# Constant variance

* The variance of the errors is the same regardless of the values of `\(X\)`

    `$$\Var(\epsilon_i | X_i) = \sigma^2$$`

---

# Normality

* The errors are assumed to be normally distributed

    `$$\epsilon_i \mid X_i \sim N(0, \sigma^2)$$`

---

# Independence

* Observations are sampled independently from one another
* Any pair of errors `\(\epsilon_i\)` and `\(\epsilon_j\)` are independent for `\(i \neq j\)`.

---

# Fixed `\(X\)`

* `\(X\)` is assumed to be fixed or measured without error and independent of the error
* Experimental design
* Observational study

    `$$\epsilon_i \sim N(0, \sigma^2), \text{for } i = 1, \dots, n$$`

---

# `\(X\)` is not invariant

&lt;img src="linear-regression_files/figure-html/invariant-1.png" width="504" /&gt;

---

# Handling violations of assumptions

* Ooopsie
* Inference can be
    * Tricky
    * Biased
    * Inefficient
    * Error prone
* Move to more robust inferential method
* Diagnose assumption violations and solve them in the OLS framework
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
