<!DOCTYPE html>
<html>
  <head>
    <title>Linear Regression</title>
    <meta charset="utf-8">
    <meta name="author" content="MACS 30100   University of Chicago" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Linear Regression
### <a href="https://model.uchicago.edu">MACS 30100</a> <br /> University of Chicago

---




`$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$`

---

# Linear regression

* Assumes `\(\E(Y|X)\)` is linear in the inputs `\(X_1, \ldots, X_p\)`
* Popular for a multitude of reasons
    * Simple to interpret
    * Easy(ish) to estimate
    * Highly adaptable, even to non-linear relationships

---

# Key concepts

* Simple linear regression model

    `$$\E(Y|X) = \beta_0 + \beta_1 X$$`
--

* Further assume that `\(\Var (\epsilon_i | X = x) = \sigma^2\)` does not depend on `\(x\)`

    `$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$`

    * `\(\E (\epsilon_i | X_i) = 0\)`
    * `\(\Var (\epsilon_i | X_i) = \sigma^2\)`
    * `\(\beta_0\, \beta_1, \sigma^2\)`

---

# Key concepts

* Let `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)` denote estimates of `\(\beta_0\)` and `\(\beta_1\)`
* Fitted line

    `$$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$$`

* Fitted/predicted values

    `$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$`

* Residuals

    `$$\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X)$$`

---

# Residual sum of squares (RSS)

$$
`\begin{align}
\text{RSS}(\beta) &amp;= \sum_{i=1}^N \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^N (Y_i - f(X_i))^2 \\
&amp;= \sum_{i=1}^N \left(Y_i - \beta_0 - \sum_{j=1}^p X_{ij} \beta_j\right)^2
\end{align}`
$$

---

class: center

# Estimating `\(\hat{\beta}\)`

&lt;img src="linear-regression_files/figure-html/sim-plot-1.png" width="504" /&gt;

---

class: center

# Estimating `\(\hat{\beta}\)`

&lt;img src="linear-regression_files/figure-html/sim-random-fit-1.png" width="504" /&gt;

---

# Estimating `\(\hat{\beta}\)`

* Classically desired properties
* Unbiased
    * `\(E(\hat{\beta}) = \beta\)`
    * Estimator that "gets it right" vis-a-vis `\(\beta\)`
* Efficient
    * `\(\min(Var(\hat{\beta}))\)`
    * Not only do we get it right, but for any given sample used to generate the model we never want to be too far off from "right"

---

# Least squares estimator

* Values `\(\hat{\beta}_0, \hat{\beta}_1\)` that minimize the RSS

    `$$\min(\text{RSS})$$`

---

# Least squares estimator

$$
`\begin{aligned}
\text{RSS} &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
\sum_{i=1}^n (\hat{\epsilon}_i)^2 &amp;= \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i))^2\\
f(\beta_0, \beta_1 | x_i, y_i) &amp; = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i )^2
\end{aligned}`
$$

---

# Least squares estimator

$$
`\begin{aligned}
\dfrac{\partial{ f(\beta_0, \beta_1 | x_i, y_i)}}{\partial \beta_0} &amp; = -2 (\sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i))\\
&amp; = \sum_{i=1}^n -2Y_i + 2\beta_0 + 2\beta_1 X_i\\
0 &amp; = \sum_{i=1}^n -2Y_{i} + 2\beta_0 + 2\beta_1 X_i\\
0 &amp; = -2 \sum_{i=1}^n Y_{i} +  2\sum_{i=1}^n \beta_0 +  2\beta_1 \sum_{i=1}^n X_i\\
0 &amp; = -2 \sum_{i=1}^n Y_{i} +  (n \times 2\beta_0) +  2\beta_1 \sum_{i=1}^n X_i\\
n \times 2\beta_0 &amp; = 2 \sum_{i=1}^n Y_i - 2\beta_1 \sum_{i=1}^n X_i\\
\hat \beta_0 &amp; = \dfrac{2 \sum_{i=1}^n Y_i}{2n} - \dfrac{2\beta_1 \sum_{i=1}^n X_i}{2n}\\
&amp; =  \dfrac{\sum_{i=1}^n Y_i}{n} - \beta_1\dfrac{ \sum_{i=1}^n X_i}{n}\\
 \hat \beta_0 &amp; = \bar{Y}_n - \beta_1 \bar{X}_n
\end{aligned}`
$$

---

# Least squares estimator

$$
`\begin{aligned}
\dfrac{\partial{ f(\beta_0, \beta_1 | x_i, y_i)}}{\partial \beta_1} &amp; = \sum_{i=1}^n -2X_i(Y_i - \beta_0 - \beta_1 X_i) \\
&amp; =  \sum_{i=1}^n -2Y_iX_i + 2\beta_0X_i + 2\beta_1 X_i^2\\
0 &amp; =  \sum_{i=1}^n -2Y_iX_i + 2\beta_0 \sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
&amp; =  \sum_{i=1}^n -2Y_iX_i + 2 (\bar{Y}_n - \beta_1 \bar{X}_n) \sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
&amp; = \sum_{i=1}^n -2Y_iX_i + 2\bar{Y}_n \sum_{i=1}^nX_i - 2\beta_1 \bar{X}_n\sum_{i=1}^nX_i + 2\beta_1  \sum_{i=1}^n X_i^2\\
2\beta_1  \sum_{i=1}^n X_i^2 - 2\beta_1 \bar{X}_n\sum_{i=1}^nX_i  &amp; = \sum_{i=1}^n 2Y_iX_i  - 2\bar{Y}_n \sum_{i=1}^nX_i\\
\beta_1 ( \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i ) &amp; = \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i\\
\hat \beta_1 &amp; = \dfrac{ \sum_{i=1}^n Y_iX_i  - \bar{Y}_n \sum_{i=1}^nX_i}{ \sum_{i=1}^n X_i^2 - \bar{X}_n\sum_{i=1}^nX_i}\\
 \hat \beta_0 &amp; = \bar{Y}_n - \hat{\beta}_1 \bar{X}_n
\end{aligned}`
$$

`$$\hat{\sigma}^2 = \left( \frac{1}{n - 2} \right) \sum_{i=1}^n \hat{\epsilon}_i^2$$`

---

class: center

# Least squares estimator

&lt;img src="linear-regression_files/figure-html/sim-lm-1.png" width="504" /&gt;

---

# Multivariate formulation

`$$\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}$$`

* `\(\mathbf{Y}\)`: `\(N\times 1\)` vector
* `\(\mathbf{X}\)`: `\(N \times K\)` matrix
* `\(\boldsymbol{\beta}\)`: `\(K \times 1\)` vector
* `\(\mathbf{u}\)`: `\(N\times 1\)` vector
* `\(i \in \{1,\ldots,N \}\)`
* `\(k \in \{1,\ldots,K \}\)`

--

    `$$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2 X_{2i} + \ldots + \beta_K X_{Ki} + u_i$$`

---

# Multivariate formulation

$$
`\begin{aligned}
\mathbf{u} &amp;= \mathbf{Y} - \mathbf{X}\boldsymbol{\beta} \\
\mathbf{u}'\mathbf{u} &amp;= (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}) \\
&amp;= \mathbf{Y'Y} - 2 \boldsymbol{\beta}' \mathbf{X'Y'} + \boldsymbol{\beta}' \mathbf{X'X} \boldsymbol{\beta}
\end{aligned}`
$$

$$
`\begin{aligned}
\frac{\partial\mathbf{u}' \mathbf{u}}{\partial \boldsymbol{\beta}}  &amp;= -2\mathbf{X'Y} + 2\boldsymbol{X'X\beta} \\
0  &amp;= -2\mathbf{X'Y} + 2\mathbf{X'X} \boldsymbol{\beta} \\
0 &amp;= -\mathbf{X'Y} + \mathbf{X'X}\boldsymbol{\beta} \\
\mathbf{X'Y} &amp;= \mathbf{X'X\beta} \\
(\mathbf{X'X})^{-1}\mathbf{X'Y} &amp;= (\mathbf{X'X})^{-1}\mathbf{X'X}\boldsymbol{\beta} \\
(\mathbf{X'X})^{-1}\mathbf{X'Y} &amp;= \mathbf{I}\boldsymbol{\beta} \\
(\mathbf{X'X})^{-1}\mathbf{X'Y} &amp;= \boldsymbol{\beta} \\
\end{aligned}`
$$

---

# Gauss-Markov theorem

* Among all linear unbiased estimates, the least squares estimate of `\(\beta\)` will have the smallest variance
* Best Linear Unbiased Estimator (BLUE)

---

# Gauss-Markov theorem

$$
`\begin{aligned}
\hat{\beta}_1 &amp;= \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2} \\
&amp;= \frac{\sum (X_i - \bar{X})Y_i}{\sum (X_i - \bar{X})^2} - \frac{\bar{Y}(X_i - \bar{X})}{\sum (X_i - \bar{X})^2} \\
&amp;= \frac{\sum (X_i - \bar{X})Y_i}{\sum (X_i - \bar{X})^2} \\
&amp;= \sum k_i Y_i
\end{aligned}`
$$


---

# Gauss-Markov theorem

* Consider now a new weighting function `\(w_i\)`:

    `$$\tilde{\beta}_1 = \sum w_i Y_i$$`

* What happens to `\(\hat{\beta}_1\)`?

$$
`\begin{align}
\E(\tilde{\beta}_1) &amp;= \sum w_i \E(Y_i) \\
&amp;= \sum w_i (\beta_0 + \beta_1 X_i) \\
&amp;= \beta_0 \sum w_i + \beta_1 \sum w_i X_i
\end{align}`
$$

---

# Do we want this?

$$
`\begin{align}
\text{MSE} (\tilde{\theta}) &amp;= \E (\tilde{\theta} - \theta)^2 \\
&amp;= \Var (\tilde{\theta}) + [\E (\tilde{\theta}) - \theta]^2 + \text{Irreducible error}
\end{align}`
$$

---

# Extensions

* Additive assumption
* Linearity assumption

---

# Removing the linear assumption

* Linear model

    `$$Y_i = \beta_0 + \beta_{1}X_{i} + \epsilon_{i}$$`

* Polynomial model

    `$$Y_i = \beta_0 + \beta_{1}X_{i} + \beta_{2}X_i^2 + \beta_{3}X_i^3 + \dots + \beta_{d}X_i^d + \epsilon_i$$`

---

class: center

# Joe Biden feeling thermometer

&lt;img src="linear-regression_files/figure-html/biden-1.png" width="504" /&gt;

---

class: center

# Joe Biden feeling thermometer

`$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age}$$`

&lt;img src="linear-regression_files/figure-html/biden-age-1.png" width="504" /&gt;

---

class: center

# Joe Biden feeling thermometer

`$$\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4$$`

&lt;img src="linear-regression_files/figure-html/biden-poly-1.png" width="504" /&gt;

---

# Removing the linear assumption

## Pointwise standard error

&lt;table&gt;
&lt;caption&gt;Variance-covariance matrix of Biden polynomial regression&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; (Intercept) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)3 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; poly(x = age, degree = 4, raw = TRUE)4 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 620.00316 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -56.31558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.20765 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.76432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.16556 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00533 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02291 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00218 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00007 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; poly(x = age, degree = 4, raw = TRUE)4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00011 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00001 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Assumptions of linear regression models

`$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$`

&gt; The key assumptions of linear regression concern the behavior of the errors

---

# Assumptions of linear regression models

* Linearity
* Constant variance
* Normality
* Independence
* Fixed `\(X\)`
* `\(X\)` is not invariant

---

# Linearity

`$$\E(\epsilon_i) \equiv E(\epsilon_i | X_i) = 0$$`

$$
`\begin{aligned}
\mu_i \equiv E(Y_i) \equiv E(Y | X_i) &amp;= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
\mu_i &amp;= \beta_0 + \beta_1 X_i + E(\epsilon_i) \\
\mu_i &amp;= \beta_0 + \beta_1 X_i + 0 \\
\mu_i &amp;= \beta_0 + \beta_1 X_i
\end{aligned}`
$$

---

# Constant variance

* The variance of the errors is the same regardless of the values of `\(X\)`

    `$$\Var(\epsilon_i | X_i) = \sigma^2$$`

---

# Normality

* The errors are assumed to be normally distributed

    `$$\epsilon_i \mid X_i \sim N(0, \sigma^2)$$`

---

# Independence

* Observations are sampled independently from one another
* Any pair of errors `\(\epsilon_i\)` and `\(\epsilon_j\)` are independent for `\(i \neq j\)`.

---

# Fixed `\(X\)`

* `\(X\)` is assumed to be fixed or measured without error and independent of the error
* Experimental design
* Observational study

    `$$\epsilon_i \sim N(0, \sigma^2), \text{for } i = 1, \dots, n$$`

---

# `\(X\)` is not invariant

&lt;img src="linear-regression_files/figure-html/invariant-1.png" width="504" /&gt;

---

# Handling violations of assumptions

* Ooopsie
* Inference can be
    * Tricky
    * Biased
    * Inefficient
    * Error prone
* Move to more robust inferential method
* Diagnose assumption violations and solve them in the OLS framework
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
