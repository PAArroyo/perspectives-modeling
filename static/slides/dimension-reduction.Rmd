---
title: "Dimension Reduction"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(tidymodels)
library(patchwork)
library(here)
library(ggfortify)
library(Rtsne)
library(tidytext)
library(tm)
library(topicmodels)
library(rjson)
library(furrr)
library(tictoc)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Supervised learning

* Predict $Y = Y_1, \ldots, Y_m$ given $X^T = (X_1, \ldots, X_p)$
* Use training sample of known outcomes and predictors
* Minimize loss function $L(Y, \hat{Y})$
* $\Pr(X,Y)$
* Estimate properties of $\Pr(Y | X)$

    $$\mu(x) = \underset{\theta}{\text{argmin}} \E_{Y | X} L(Y, \theta)$$
    
    $$\Pr(X,Y) = \Pr(Y | X) \cdot \Pr(X)$$
    
    * Ignore $\Pr(X)$

---

# Unsupervised learning

* $X^T = (X_1, \ldots, X_p)$
* Estimate properties of $\Pr(X)$
* No objective function
* Much higher dimensionality
* Not worried about how $\Pr(X)$ changes conditional on changes in $X$

---

# Unsupervised learning with low $p$

```{r infant}
infant <- read_csv(here("static", "data", "infant.csv")) %>%
  # remove non-countries
  filter(is.na(`Value Footnotes`) | `Value Footnotes` != 1) %>%
  select(`Country or Area`, Year, Value) %>%
  rename(country = `Country or Area`,
         year = Year,
         mortal = Value)
```

```{r infant-epan-optimal, dependson = "infant"}
ggplot(infant, aes(mortal)) +
  geom_density(kernel = "epanechnikov") +
  labs(title = "Epanechnikov kernel function",
       subtitle = str_c("Optimal bandwidth = ",
                        round(density(x = infant$mortal, kernel = "epanechnikov")$bw,
                              digits = 2)),
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

---

# Unsupervised learning

* Dimension reduction
* Cluster analysis

--

* Metric for success
    

---

# Principal components analysis

* Reduce the dimensionality of the data set through linear combinations of features $X_1, X_2, \ldots, X_p$
* Common purposes
    * Feature reduction
    * Reduce multicollinearity
    * Data visualization

---

# `USArrests`

```{r usarrests}
(df <- USArrests %>%
  rownames_to_column(var = "State") %>%
  as_tibble())
```

---

# Data preparation

* Mean center
* Standardize variance

##### Unscaled

```{r usarrests-unscale, dependson = "usarrests"}
df %>%
  summarize_at(.vars = vars(-State), var)
```

##### Scaled

```{r usarrests-scale, dependson = "usarrests"}
scaled_df <- df %>%
  mutate_at(.vars = vars(-State), scale)

scaled_df %>%
  summarize_at(.vars = vars(-State), var)
```

--

* When not to scale

---

# Principal components

* Explain most of the variability in the data with a smaller number of variables
* Low-dimensional representation of high-dimensional data, with minimal information loss
* Small number of dimensions
    * Amount that the observations vary along each dimension
* Linear combination of $p$ features

---

# Principal components

## First principal component

* Data set $X_1, X_2, \ldots, X_p$

    $$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + \dots + \phi_{p1}X_p$$
    
    $$\max \Var(Z_1)$$
    
* $\phi_1$ - first principal component loading vector
    * $p$ individual loadings $\phi_{11}, \dots, \phi_{p1}$
* Normalized $\phi$
    
    $$\sum_{j=1}^p \phi_{j1}^2 = 1$$

---

# Principal components

## Second principal component

$$Z_2 = \phi_{12}X_1 + \phi_{22}X_2 + \dots + \phi_{p2}X_p$$

$$\max \Var(Z_2)$$

* $\sum_{j=1}^p \phi_{j2}^2 = 1$
* $Z_1, Z_2$ uncorrelated
    * Orthogonality

---

# Principal components

## $n$th principal component

* $\min(p, n)$ unique principal components
* As $j$ increases, variance of $Z_j$ constantly decreasing

---

# Eigen-analysis

* $\mathbf{X}_{p \times p}$
* $\lambda_i, i = 1, \ldots, p$

    $$\mathbf{X} \mathbf{h}_i = \lambda_i \mathbf{h}_i$$

* $\lambda_i$ - eigenvalue of $\mathbf{X}$
* $\mathbf{h}_i$ - eigenvector of $\mathbf{X}$
* Multiple possible eigenvector/eigenvalues

---

# Estimating principal components

1. Compute the covariance matrix $\mathbf{S}$

    $$\mathbf{S} = \dfrac{1}{N} \mathbf{X}' \mathbf{X}$$
    
1. Compute the $K$ largest **eigenvectors** of $\mathbf{S}$
    * Principal components of the dataset
    * Vector elements - PC loadings
    * Eigenvalue - variance of each PC

---

# Estimating principal components

## Covariance matrix

```{r usarrests-cov, dependson = "usarrests-scale"}
(arrests_cov <- scaled_df %>%
  select(-State) %>%
  cov())
```

## Eigen

```{r usarrests-eigen, dependson = "usarrests-cov"}
(arrests_eigen <- eigen(arrests_cov))
```

---

# Recover $\phi$

```{r usarrests-phi, dependson = "usarrests-eigen"}
# extract first two loadings
phi <- arrests_eigen$vectors[, 1:2]

phi <- -phi
row.names(phi) <- c("Murder", "Assault", "UrbanPop", "Rape")
colnames(phi) <- c("PC1", "PC2")
phi
```

* Principal component vector
* Interpreting PC vectors

---

# Calculating PC scores

* Project $x_1, \ldots, x_n$ onto eigenvector
* Calculate principal component scores

--

```{r usarrests-pc-scores, dependson = "usarrests-phi"}
# Calculate Principal Components scores
PC1 <- as.matrix(select_if(scaled_df, is.numeric)) %*% phi[,1]
PC2 <- as.matrix(select_if(scaled_df, is.numeric)) %*% phi[,2]

# Create data frame with Principal Components scores
(PC <- tibble(
  State = scaled_df$State,
  PC1 = PC1[,1],
  PC2 = PC2[,1]
))
```

---

# Visualizing PC scores

```{r usarrests-pc-scores-plot, dependson = "usarrests-pc-scores"}
ggplot(PC, aes(PC1, PC2)) + 
  geom_vline(xintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_hline(yintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = State)) +
  labs(title = "First two principal components of USArrests",
       x = "First principal component",
       y = "Second principal component")
```

---

# Biplot

```{r usarrests-pc-scores-biplot, dependson = "usarrests-pc-scores"}
phi_df <- (phi * 1.75) %>%
  as.data.frame %>%
  rownames_to_column(var = "variable")

ggplot(PC, aes(PC1, PC2)) + 
  geom_vline(xintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_hline(yintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = State)) +
  geom_segment(data = phi_df,
               aes(x = 0, y = 0,
                   xend = PC1,
                   yend = PC2),
               arrow = arrow(length = unit(0.03, "npc")),
               color = "purple") +
  ggrepel::geom_text_repel(data = phi_df,
                           aes(x = PC1, y = PC2, label = variable),
                           color = "purple") +
  labs(title = "First two principal components of USArrests",
       x = "First principal component",
       y = "Second principal component")
```

---

# Selecting the number of PCs

* Total variance in a dataset

    $$\sum_{j=1}^p \Var(X_j) = \sum_{j=1}^p \frac{1}{N} \sum_{i=1}^N x_{ij}^2$$

* Variance explained by the $m$th principal component

    $$\frac{1}{N} \sum_{i=1}^N z_{im}^2 = \frac{1}{N} \sum_{i=1}^N \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2$$
    
* Proportion of variance explained by the $m$th principal component

    $$\text{PVE} = \frac{\sum_{i=1}^N \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2}{\sum_{j=1}^p \sum_{i=1}^N x_{ij}^2}$$
    
--

    $$\text{PVE} = \frac{\lambda_m}{\sum_{j=1}^p \lambda_j}$$

---

# Variance explained

```{r pca-usarrests-pve, dependson = "pca-usarrests"}
usarrest_pve <- tibble(
  var = arrests_eigen$values,
  var_exp = var / sum(var),
  cum_var_exp = cumsum(var_exp)
) %>%
  mutate(pc = row_number())

# PVE plot
us_pve <- ggplot(usarrest_pve, aes(pc, var_exp)) +
  geom_line() +
  geom_point() +
  ylim(0, 1) +
  labs(title = "Scree plot",
       x = "Principal component",
       y = "Proportion of variance explained")

# CVE plot
us_cve <- ggplot(usarrest_pve, aes(pc, cum_var_exp)) +
  geom_line() +
  geom_point() +
  ylim(0, 1) +
  labs(title = "Cumulative scree plot",
       x = "Principal component",
       y = "Cumulative proportion of variance explained")

us_pve + us_cve
```

---

# Deciding on $m$ principal components

.center[

<iframe src="https://giphy.com/embed/y65VoOlimZaus" width="600" height="549" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/idk-shrug-power-rangers-y65VoOlimZaus">via GIPHY</a></p>

]

---

# MNIST data set

.center[

![MNIST digits](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

]

---

# Import data

```{r mnist-raw}
mnist_raw <- read_csv("https://pjreddie.com/media/files/mnist_train.csv", col_names = FALSE)
```

```{r mnist-raw-print, dependson = "mnist-raw"}
mnist_raw
```

---

# Import data

```{r pixels-gathered, dependson = "mnist-raw"}
pixels <- mnist_raw %>%
  rename(label = X1) %>%
  group_by(label) %>%
  sample_n(100) %>%
  ungroup() %>%
  # remove pixels with no variance - cannot rescale for PCA
  select_if(.predicate = ~ !(min(.) == max(.)))

pixels_gathered <- mnist_raw %>%
  rename(label = X1) %>%
  group_by(label) %>%
  sample_n(100) %>%
  ungroup() %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
```

```{r instances_12_graph, dependson = "pixels_gathered"}
mnist_raw %>%
  rename(label = X1) %>%
  sample_n(12) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance + label)
```

---

# Interpret PCA

```{r pixels-pca, dependson = "pixels-gathered"}
pixels_pca <- prcomp(pixels[, -1], center = TRUE, scale = TRUE)
```

```{r pixels-pca-pc12, dependson = "pixels-pca"}
pixels_pca_plot <- augment(pixels_pca, data = pixels) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, color = factor(label))) +
  geom_point() +
  scale_color_brewer(type = "qual", palette = "Paired") +
  labs(x = "First principal component",
       y = "Second principal component",
       color = "Label")
pixels_pca_plot
```

---

# Interpret PCA

```{r pixels-pca-pc12-facet, dependson = "pixels-pca", message = FALSE}
pixels_pca_plot +
  facet_wrap(~ label) +
  scale_color_brewer(type = "qual", palette = "Paired", guide = FALSE)
```

---

# Interpret PCA

```{r pixels-pca-pc12-biplot, dependson = "pixels-pca"}
autoplot(pixels_pca, loadings = TRUE)
```

---

# Scree plots

```{r pixels-pca-pve, dependson = "pixels-pca"}
pixels_var <- tibble(
  var = pixels_pca$sdev^2,
  var_exp = var / sum(var),
  cum_var_exp = cumsum(var_exp)
) %>%
  mutate(pc = row_number())

# PVE plot
pixels_pve <- ggplot(pixels_var, aes(pc, var_exp)) +
  geom_line() +
  labs(title = "Scree plot",
       x = "Principal component",
       y = "Proportion of variance explained")

# CVE plot
pixels_cve <- ggplot(pixels_var, aes(pc, cum_var_exp)) +
  geom_line() +
  ylim(0, 1) +
  labs(title = "Cumulative scree plot",
       x = "Principal component",
       y = "Cumulative proportion of variance explained")

pixels_pve + pixels_cve
```

---

# Non-linear dimension reduction

* Linear combination of features
* Non-linear combination of features
* Global vs. local
    * Global - preserve geometry at all scales
    * Local - map nearby points in high dimensions to nearby points in low dimensions
    
---

# Stochastic Neighbor Embedding (SNE)

* Convert high-demensional Euclidean distances between data points into conditional probabilities representing similarities
* Similarity of $x_j$ to $x_i$
* $p_{j | i}$
    * Conditional probability $x_i$ would choose $x_j$ as its neighbor proportionally to their probability density under a Gaussian centered at $x_i$
* $p_{j | i}$ for close data points
* $p_{j | i}$ for distant data points

$$p_{j|i} = \frac{\exp(- \| x_i - x_j \|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2 / 2\sigma_i^2)}$$

---

# Stochastic Neighbor Embedding (SNE)

* Convert low-demensional counterparts' Euclidean distances between data points into conditional probabilities representing similarities
* Similarity of $y_j$ to $y_i$
* $q_{j | i}$
    * Conditional probability $y_i$ would choose $y_j$ as its neighbor proportionally to their probability density under a Gaussian centered at $y_i$
* $q_{j | i}$ for close data points
* $q_{j | i}$ for distant data points

$$q_{j|i} = \frac{\exp(- \| x_i - x_j \|^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2)}$$

---

# Stochastic Neighbor Embedding (SNE)

* Minimize the difference between $p_{j | i}$ and $q_{j | i}$
* Complex cost function
* Difficult to compute
* $t$-SNE
    * Use $t$-distribution for $q_{j|i}$
    * Resolves many optimization problems
    
---

# $t$-SNE

* Non-linear dimension reduction algorithm
* Finds patterns in the data by identifying observed clusters based on similarity across feature space
* Maps the multi-dimensional data to a lower dimensional space
* Data exploration/visualization
* Construct features from output of $t$-SNE
* Major tuning parameters
    * Number of dimensions for reduction
    * Effective size of local neighborhood
    * Initial PCA step

---

# MNIST data set

```{r pixels-tsne, dependson = "pixels-gathered"}
pixels_tsne <- Rtsne(pixels[, -1])
```

```{r pixels-tsne-dim, dependson = "pixels-tsne"}
pixels_tsne_plot <- pixels %>%
  mutate(tsne1 = pixels_tsne$Y[,1],
         tsne2 = pixels_tsne$Y[,2]) %>%
  ggplot(aes(tsne1, tsne2, color = factor(label))) +
  geom_point() +
  # facet_wrap(~ label) +
  scale_color_brewer(type = "qual", palette = "Paired") +
  labs(x = "First dimension",
       y = "Second dimension",
       color = "Label")
pixels_tsne_plot
```

---

# MNIST data set

```{r pixels-tsne-dim-facet, dependson = "pixels-tsne", message = FALSE}
pixels_tsne_plot +
  facet_wrap(~ label) +
  scale_color_brewer(type = "qual", palette = "Paired", guide = FALSE)
```

---

# Comparison of PCA and $t$-SNE

```{r pca-tsne-compare, dependson = c("pixels-pca-pc12", "pixels-tsne-dim")}
{
  pixels_pca_plot +
    ggtitle("PCA") +
    theme(legend.position = "none")
} + {
  pixels_tsne_plot +
    ggtitle("t-SNE") +
    theme(legend.position = "none")
}
```

