---
title: "Dimension Reduction"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2, fig.width = 12)

library(tidyverse)
library(tidymodels)
library(patchwork)
library(here)
library(ggfortify)
library(Rtsne)
library(tidytext)
library(tm)
library(topicmodels)
library(furrr)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Supervised learning

* Predict $Y = Y_1, \ldots, Y_m$ given $X^T = (X_1, \ldots, X_p)$
* Use training sample of known outcomes and predictors
* Minimize loss function $L(Y, \hat{Y})$
* $\Pr(X,Y)$
* Estimate properties of $\Pr(Y | X)$

    $$\mu(x) = \underset{\theta}{\text{argmin}} \E_{Y | X} L(Y, \theta)$$
    
    $$\Pr(X,Y) = \Pr(Y | X) \cdot \Pr(X)$$
    
    * Ignore $\Pr(X)$

---

# Unsupervised learning

* $X^T = (X_1, \ldots, X_p)$
* Estimate properties of $\Pr(X)$
* No objective function
* Much higher dimensionality
* Not worried about how $\Pr(X)$ changes conditional on changes in $X$

---

# Unsupervised learning with low $p$

```{r infant}
infant <- read_csv(here("static", "data", "infant.csv")) %>%
  # remove non-countries
  filter(is.na(`Value Footnotes`) | `Value Footnotes` != 1) %>%
  select(`Country or Area`, Year, Value) %>%
  rename(country = `Country or Area`,
         year = Year,
         mortal = Value)
```

```{r infant-epan-optimal, dependson = "infant"}
ggplot(infant, aes(mortal)) +
  geom_density(kernel = "epanechnikov") +
  labs(title = "Epanechnikov kernel function",
       subtitle = str_c("Optimal bandwidth = ",
                        round(density(x = infant$mortal, kernel = "epanechnikov")$bw,
                              digits = 2)),
       x = "Infant mortality rate (per 1,000)",
       y = "Density")
```

---

# Unsupervised learning

* Dimension reduction
* Cluster analysis

--

* Metric for success
    

---

# Principal components analysis

* Reduce the dimensionality of the data set through linear combinations of features $X_1, X_2, \ldots, X_p$
* Common purposes
    * Feature reduction
    * Reduce multicollinearity
    * Data visualization

---

# `USArrests`

```{r usarrests}
(df <- USArrests %>%
  rownames_to_column(var = "State") %>%
  as_tibble())
```

---

# Data preparation

* Mean center
* Standardize variance

##### Unscaled

```{r usarrests-unscale, dependson = "usarrests"}
df %>%
  summarize_at(.vars = vars(-State), var)
```

##### Scaled

```{r usarrests-scale, dependson = "usarrests"}
scaled_df <- df %>%
  mutate_at(.vars = vars(-State), scale)

scaled_df %>%
  summarize_at(.vars = vars(-State), var)
```

--

* When not to scale

---

# Principal components

* Explain most of the variability in the data with a smaller number of variables
* Low-dimensional representation of high-dimensional data, with minimal information loss
* Small number of dimensions
    * Amount that the observations vary along each dimension
* Linear combination of $p$ features

---

# Principal components

## First principal component

* Data set $X_1, X_2, \ldots, X_p$

    $$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + \dots + \phi_{p1}X_p$$
    
    $$\max \Var(Z_1)$$
    
* $\phi_1$ - first principal component loading vector
    * $p$ individual loadings $\phi_{11}, \dots, \phi_{p1}$
* Normalized $\phi$
    
    $$\sum_{j=1}^p \phi_{j1}^2 = 1$$

---

# Principal components

## Second principal component

$$Z_2 = \phi_{12}X_1 + \phi_{22}X_2 + \dots + \phi_{p2}X_p$$

$$\max \Var(Z_2)$$

* $\sum_{j=1}^p \phi_{j2}^2 = 1$
* $Z_1, Z_2$ uncorrelated
    * Orthogonality

---

# Principal components

## $n$th principal component

* $p$ unique principal components
* As $j$ increases, variance of $Z_j$ constantly decreasing

---

# Estimating principal components

1. Compute the covariance matrix $\mathbf{S}$

    $$\mathbf{S} = \dfrac{1}{N} \mathbf{X}' \mathbf{X}$$
    
1. Compute the $K$ largest **eigenvectors** of $\mathbf{S}$
    * Principal components of the dataset
    * Vector elements - PC scores
    * Eigenvalue - variance of each PC

---

# Estimating principal components

## Covariance matrix

```{r usarrests-cov, dependson = "usarrests-scale"}
(arrests_cov <- scaled_df %>%
  select(-State) %>%
  cov())
```

## Eigen

```{r usarrests-eigen, dependson = "usarrests-cov"}
(arrests_eigen <- eigen(arrests_cov))
```

---

# Recover $\phi$

```{r usarrests-phi, dependson = "usarrests-eigen"}
# extract first two loadings
phi <- arrests_eigen$vectors[, 1:2]

phi <- -phi
row.names(phi) <- c("Murder", "Assault", "UrbanPop", "Rape")
colnames(phi) <- c("PC1", "PC2")
phi
```

* Principal component vector
* Interpreting PC vectors

---

# Calculating PC scores

* Project $x_1, \ldots, x_n$ onto eigenvector
* Calculate principal component scores

--

```{r usarrests-pc-scores, dependson = "usarrests-phi"}
# Calculate Principal Components scores
PC1 <- as.matrix(select_if(scaled_df, is.numeric)) %*% phi[,1]
PC2 <- as.matrix(select_if(scaled_df, is.numeric)) %*% phi[,2]

# Create data frame with Principal Components scores
(PC <- tibble(
  State = scaled_df$State,
  PC1 = PC1[,1],
  PC2 = PC2[,1]
))
```

---

# Visualizing PC scores

```{r usarrests-pc-scores-plot, dependson = "usarrests-pc-scores"}
ggplot(PC, aes(PC1, PC2)) + 
  geom_vline(xintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_hline(yintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = State)) +
  labs(title = "First two principal components of USArrests",
       x = "First principal component",
       y = "Second principal component")
```

---

# Biplot

```{r usarrests-pc-scores-biplot, dependson = "usarrests-pc-scores"}
phi_df <- (phi * 1.75) %>%
  as.data.frame %>%
  rownames_to_column(var = "variable")

ggplot(PC, aes(PC1, PC2)) + 
  geom_vline(xintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_hline(yintercept = 0, size = 2, color = "black", alpha = .5) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = State)) +
  geom_segment(data = phi_df,
               aes(x = 0, y = 0,
                   xend = PC1,
                   yend = PC2),
               arrow = arrow(length = unit(0.03, "npc")),
               color = "purple") +
  ggrepel::geom_text_repel(data = phi_df,
                           aes(x = PC1, y = PC2, label = variable),
                           color = "purple") +
  labs(title = "First two principal components of USArrests",
       x = "First principal component",
       y = "Second principal component")
```

---

# Selecting the number of PCs

* Total variance in a dataset

    $$\sum_{j=1}^p \Var(X_j) = \sum_{j=1}^p \frac{1}{N} \sum_{i=1}^N x_{ij}^2$$

* Variance explained by the $m$th principal component

    $$\frac{1}{N} \sum_{i=1}^N z_{im}^2 = \frac{1}{N} \sum_{i=1}^N \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2$$
    
* Proportion of variance explained by the $m$th principal component

    $$\text{PVE} = \frac{\sum_{i=1}^N \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2}{\sum_{j=1}^p \sum_{i=1}^N x_{ij}^2}$$
    
--

    $$\text{PVE} = \frac{\lambda_m}{\sum_{j=1}^p \lambda_j}$$

---

# Variance explained

```{r pca-usarrests-pve, dependson = "pca-usarrests"}
usarrest_pve <- tibble(
  var = arrests_eigen$values,
  var_exp = var / sum(var),
  cum_var_exp = cumsum(var_exp)
) %>%
  mutate(pc = row_number())

# PVE plot
us_pve <- ggplot(usarrest_pve, aes(pc, var_exp)) +
  geom_line() +
  geom_point() +
  ylim(0, 1) +
  labs(title = "Scree plot",
       x = "Principal component",
       y = "Proportion of variance explained")

# CVE plot
us_cve <- ggplot(usarrest_pve, aes(pc, cum_var_exp)) +
  geom_line() +
  geom_point() +
  ylim(0, 1) +
  labs(title = "Cumulative scree plot",
       x = "Principal component",
       y = "Cumulative proportion of variance explained")

us_pve + us_cve
```

---

# Deciding on $m$ principal components

.center[

<iframe src="https://giphy.com/embed/y65VoOlimZaus" width="600" height="549" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/idk-shrug-power-rangers-y65VoOlimZaus">via GIPHY</a></p>

]

---

# MNIST data set

.center[

![MNIST digits](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

]

---

# Import data

```{r mnist-raw}
mnist_raw <- read_csv("https://pjreddie.com/media/files/mnist_train.csv", col_names = FALSE)
```

```{r mnist-raw-print, dependson = "mnist-raw"}
mnist_raw
```

---

# Import data

```{r pixels-gathered, dependson = "mnist-raw"}
pixels <- mnist_raw %>%
  rename(label = X1) %>%
  group_by(label) %>%
  sample_n(100) %>%
  ungroup() %>%
  # remove pixels with no variance - cannot rescale for PCA
  select_if(.predicate = ~ !(min(.) == max(.)))

pixels_gathered <- mnist_raw %>%
  rename(label = X1) %>%
  group_by(label) %>%
  sample_n(100) %>%
  ungroup() %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
```

```{r instances_12_graph, dependson = "pixels_gathered"}
mnist_raw %>%
  rename(label = X1) %>%
  sample_n(12) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance + label)
```

---

# Interpret PCA

```{r pixels-pca, dependson = "pixels-gathered"}
pixels_pca <- prcomp(pixels[, -1], center = TRUE, scale = TRUE)
```

```{r pixels-pca-pc12, dependson = "pixels-pca"}
pixels_pca_plot <- augment(pixels_pca, data = pixels) %>%
  ggplot(aes(.fittedPC1, .fittedPC2, color = factor(label))) +
  geom_point() +
  scale_color_brewer(type = "qual", palette = "Paired") +
  labs(x = "First principal component",
       y = "Second principal component",
       color = "Label")
pixels_pca_plot
```

---

# Interpret PCA

```{r pixels-pca-pc12-facet, dependson = "pixels-pca", message = FALSE}
pixels_pca_plot +
  facet_wrap(~ label) +
  scale_color_brewer(type = "qual", palette = "Paired", guide = FALSE)
```

---

# Interpret PCA

```{r pixels-pca-pc12-biplot, dependson = "pixels-pca"}
autoplot(pixels_pca, loadings = TRUE)
```

---

# Scree plots

```{r pixels-pca-pve, dependson = "pixels-pca"}
pixels_var <- tibble(
  var = pixels_pca$sdev^2,
  var_exp = var / sum(var),
  cum_var_exp = cumsum(var_exp)
) %>%
  mutate(pc = row_number())

# PVE plot
pixels_pve <- ggplot(pixels_var, aes(pc, var_exp)) +
  geom_line() +
  labs(title = "Scree plot",
       x = "Principal component",
       y = "Proportion of variance explained")

# CVE plot
pixels_cve <- ggplot(pixels_var, aes(pc, cum_var_exp)) +
  geom_line() +
  ylim(0, 1) +
  labs(title = "Cumulative scree plot",
       x = "Principal component",
       y = "Cumulative proportion of variance explained")

pixels_pve + pixels_cve
```

---

# Non-linear dimension reduction

* Linear combination of features
* Non-linear combination of features
* Global vs. local
    * Global - preserve geometry at all scales
    * Local - map nearby points in high dimensions to nearby points in low dimensions
    
---

# Stochastic Neighbor Embedding (SNE)

* Convert high-demensional Euclidean distances between data points into conditional probabilities representing similarities
* Similarity of $x_j$ to $x_i$
* $p_{j | i}$
    * Conditional probability $x_i$ would choose $x_j$ as its neighbor proportionally to their probability density under a Gaussian centered at $x_i$
* $p_{j | i}$ for close data points
* $p_{j | i}$ for distant data points

$$p_{j|i} = \frac{\exp(- \| x_i - x_j \|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2 / 2\sigma_i^2)}$$

---

# Stochastic Neighbor Embedding (SNE)

* Convert low-demensional counterparts' Euclidean distances between data points into conditional probabilities representing similarities
* Similarity of $y_j$ to $y_i$
* $q_{j | i}$
    * Conditional probability $y_i$ would choose $y_j$ as its neighbor proportionally to their probability density under a Gaussian centered at $y_i$
* $q_{j | i}$ for close data points
* $q_{j | i}$ for distant data points

$$q_{j|i} = \frac{\exp(- \| x_i - x_j \|^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2)}$$

---

# Stochastic Neighbor Embedding (SNE)

* Minimize the difference between $p_{j | i}$ and $q_{j | i}$
* Complex cost function
* Difficult to compute
* $t$-SNE
    * Use $t$-distribution for $q_{j|i}$
    * Resolves many optimization problems
    
---

# $t$-SNE

* Non-linear dimension reduction algorithm
* Finds patterns in the data by identifying observed clusters based on similarity across feature space
* Maps the multi-dimensional data to a lower dimensional space
* Data exploration/visualization
* Construct features from output of $t$-SNE
* Major tuning parameters
    * Number of dimensions for reduction
    * Effective size of local neighborhood
    * Initial PCA step

---

# MNIST data set

```{r pixels-tsne, dependson = "pixels-gathered"}
pixels_tsne <- Rtsne(pixels[, -1])
```

```{r pixels-tsne-dim, dependson = "pixels-tsne"}
pixels_tsne_plot <- pixels %>%
  mutate(tsne1 = pixels_tsne$Y[,1],
         tsne2 = pixels_tsne$Y[,2]) %>%
  ggplot(aes(tsne1, tsne2, color = factor(label))) +
  geom_point() +
  # facet_wrap(~ label) +
  scale_color_brewer(type = "qual", palette = "Paired") +
  labs(x = "First dimension",
       y = "Second dimension",
       color = "Label")
pixels_tsne_plot
```

---

# MNIST data set

```{r pixels-tsne-dim-facet, dependson = "pixels-tsne", message = FALSE}
pixels_tsne_plot +
  facet_wrap(~ label) +
  scale_color_brewer(type = "qual", palette = "Paired", guide = FALSE)
```

---

# Comparison of PCA and $t$-SNE

```{r pca-tsne-compare, dependson = c("pixels-pca-pc12", "pixels-tsne-dim")}
{
  pixels_pca_plot +
    ggtitle("PCA") +
    theme(legend.position = "none")
} + {
  pixels_tsne_plot +
    ggtitle("t-SNE") +
    theme(legend.position = "none")
}
```

---

# Latent semantic analysis

* Bag-of-words approach

```
 a abandoned abc ability able about above abroad absorbed absorbing abstract
43         0   0       0    0    10     0      0        0         0        1
```

* Properties of bag-of-words representation
    * Sparsity
    * Stop words
    * Correlation between words
* Reduce dimensions of corpus using PCA
    * Identifies words closely related to one another in lower dimensional space
    * Enables keyword searches

---

# Interpretation: `NYTimes`

```{r nytimes}
# get NYTimes data
load(here("static", "data", "pca-examples.Rdata"))
```

```{r nytimes-words}
colnames(nyt.frame)[sample(ncol(nyt.frame),30)]
```

---

# Interpretation: `NYTimes`

```{r nytimes-pca}
# Omit the first column of class labels
nyt.pca <- prcomp(nyt.frame[,-1])

# Extract the actual component directions/weights for ease of reference
nyt.latent.sem <- nyt.pca$rotation

# convert to data frame
nyt.latent.sem <- nyt.latent.sem %>%
  as_tibble %>%
  mutate(word = names(nyt.latent.sem[,1])) %>%
  select(word, everything())
```

```{r nytimes-PC1}
nyt.latent.sem %>%
  select(word, PC1) %>%
  arrange(PC1) %>%
  slice(c(1:10, (n() - 10):n())) %>%
  mutate(pos = ifelse(PC1 > 0, TRUE, FALSE),
         word = fct_reorder(word, PC1)) %>%
  ggplot(aes(word, PC1, fill = pos)) +
  geom_col() +
  scale_fill_brewer(type = "qual") +
  labs(title = "LSA analysis of NYTimes articles",
       x = NULL,
       y = "PC1 scores") +
  coord_flip() +
  theme(legend.position = "none")
```

---

# Interpretation: `NYTimes`

```{r nytimes-PC2}
nyt.latent.sem %>%
  select(word, PC2) %>%
  arrange(PC2) %>%
  slice(c(1:10, (n() - 10):n())) %>%
  mutate(pos = ifelse(PC2 > 0, TRUE, FALSE),
         word = fct_reorder(word, PC2)) %>%
  ggplot(aes(word, PC2, fill = pos)) +
  geom_col() +
  scale_fill_brewer(type = "qual") +
  labs(title = "LSA analysis of NYTimes articles",
       x = NULL,
       y = "PC2 scores") +
  coord_flip() +
  theme(legend.position = "none")
```

---

# Interpretation: `NYTimes`

```{r nytimes-biplot}
biplot(nyt.pca, scale = 0, cex = .6)
```

---

# Interpretation: `NYTimes`

```{r nytimes-plot-dim}
cbind(type = nyt.frame$class.labels, as_tibble(nyt.pca$x[,1:2])) %>%
  mutate(type = factor(type, levels = c("art", "music"),
                       labels = c("A", "M"))) %>%
  ggplot(aes(PC1, PC2, label = type, color = type)) +
  geom_text() +
  scale_color_brewer(type = "qual") +
  labs(title = "") +
  theme(legend.position = "none")
```

---

# Topic modeling

* Themes
* Probabilistic topic models
* Latent Dirichlet allocation

---

# Food and animals

1. I ate a banana and spinach smoothie for breakfast.
1. I like to eat broccoli and bananas.
1. Chinchillas and kittens are cute.
1. My sister adopted a kitten yesterday.
1. Look at this cute hamster munching on a piece of broccoli.

---

# LDA document structure

* Decide on the number of words N the document will have
    * [Dirichlet probability distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)
    * Fixed set of $k$ topics
* Generate each word in the document:
    * Pick a topic
    * Generate the word
* LDA backtracks from this assumption

---

# How does LDA learn?

* Randomly assign each word in the document to one of $K$ topics
* For each document $d$:
    * Go through each word $w$ in $d$
        * And for each topic $t$, compute two things:
            1. $p(t | d)$
            1. $p(w | t)$
        * Reassign $w$ a new topic $t$ with probability $p(t|d) \times p(w|t)$
* Rinse and repeat

* Estimate from LDA
    1. The topic mixtures of each document
    1. The words associated to each topic
---

# `USCongress`

```{r get-docs}
# get USCongress data
data(USCongress, package = "RTextTools")

# topic labels
major_topics <- tibble(
  major = c(1:10, 12:21, 99),
  label = c("Macroeconomics", "Civil rights, minority issues, civil liberties",
            "Health", "Agriculture", "Labor and employment", "Education", "Environment",
            "Energy", "Immigration", "Transportation", "Law, crime, family issues",
            "Social welfare", "Community development and housing issues",
            "Banking, finance, and domestic commerce", "Defense",
            "Space, technology, and communications", "Foreign trade",
            "International affairs and foreign aid", "Government operations",
            "Public lands and water management", "Other, miscellaneous")
)

(congress <- as_tibble(USCongress) %>%
    mutate(text = as.character(text)) %>%
    left_join(major_topics))
```

---

# `USCongress`

```{r convert-tidytext, dependson = "get-docs"}
congress_tokens <- congress %>%
  unnest_tokens(output = word, input = text) %>%
  # remove numbers
  filter(!str_detect(word, "^[0-9]*$")) %>%
  # remove stop words
  anti_join(stop_words) %>%
  # stem the words
  mutate(word = SnowballC::wordStem(word))
```

```{r dtm, dependson = "convert-tidytext"}
# remove terms with low tf-idf for future LDA model
congress_dtm <- congress_tokens %>%
  count(major, word) %>%
  bind_tf_idf(term = word, document = major, n = n) %>%
  group_by(major) %>%
  top_n(40, wt = tf_idf) %>%
  ungroup %>%
  count(word) %>%
  select(-n) %>%
  left_join(congress_tokens) %>%
  # get count of each token in each document
  count(ID, word) %>%
  # create a document-term matrix with all features and tf weighting
  cast_dtm(document = ID, term = word, value = n)
```

```{r bind-tf-idf, dependson = "convert-tidytext"}
congress_tfidf <- congress_tokens %>%
  count(label, word) %>%
  bind_tf_idf(term = word, document = label, n = n)
```

```{r plot-tf-idf, dependson = "bind-tf-idf"}
# sort the data frame and convert word to a factor column
plot_congress <- congress_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

# graph the top 10 tokens for 4 categories
plot_congress %>%
  filter(label %in% c("Macroeconomics",
                      "Civil rights, minority issues, civil liberties",
                      "Health", "Education")) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ label, scales = "free") +
  coord_flip()
```

---

# 20 topic LDA model

```{r congress-20-topic, dependson = "dtm"}
congress_lda <- LDA(congress_dtm, k = 20, control = list(seed = 1234))
```

```{r congress-20-topn, fig.asp = 2, dependson = "congress-20-topic"}
congress_lda_td <- tidy(congress_lda)

top_terms <- congress_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip()
```

---

# Document classification

```{r uscongress-gamma, dependson = "congress-20-topic"}
congress_gamma <- tidy(congress_lda, matrix = "gamma")
```

```{r congress-model-compare, dependson = "uscongress-gamma"}
congress_tokens %>%
  count(label, word) %>%
  bind_tf_idf(term = word, document = label, n = n) %>%
  group_by(label) %>%
  top_n(40, wt = tf_idf) %>%
  ungroup %>%
  count(word) %>%
  select(-n) %>%
  left_join(congress_tokens) %>%
  distinct(ID) %>%
  left_join(congress) %>%
  mutate(document = as.character(row_number())) %>%
  filter(label %in% c("Macroeconomics",
                      "Civil rights, minority issues, civil liberties",
                      "Health", "Education")) %>%
  left_join(congress_gamma) %>%
  na.omit %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ label) +
  labs(x = "LDA topic",
       y = expression(gamma))
```

---

# Associated Press articles

```{r associated_press}
data("AssociatedPress", package = "topicmodels")

# tidy and remove stop words
ap_td <- tidy(AssociatedPress)
```

```{r ap_stopwords, dependson = "associated_press"}
ap_dtm <- ap_td %>%
  anti_join(stop_words, by = c(term = "word")) %>%
  cast_dtm(document, term, count)
ap_dtm
```

---

# $k=4$

```{r ap_topic_4, dependson = "associated_press"}
ap_lda <- LDA(ap_dtm, k = 4, control = list(seed = 1234))
```

```{r ap_4_topn, dependson = "ap_topic_4"}
ap_lda_td <- tidy(ap_lda)

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip()
```

---

# $k=12$

```{r ap_topic_12, dependson = "associated_press"}
ap_lda <- LDA(ap_dtm, k = 12, control = list(seed = 1234))
```

```{r ap_12_topn, dependson="ap_topic_12"}
ap_lda_td <- tidy(ap_lda)

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```

---

# Perplexity

* A statistical measure of how well a probability model predicts a sample
* Given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents
* Perplexity for LDA model with 12 topics
    * `r topicmodels::perplexity(ap_lda)`

---

# Perplexity

```{r ap_lda_compare, dependson="associated_press"}
n_topics <- c(2, 4, 10, 20, 50, 100)

if(file.exists(here("static", "data", "ap_lda_compare.Rdata"))){
  load(file = here("static", "data", "ap_lda_compare.Rdata"))
} else{
  plan(multiprocess)
  
  ap_lda_compare <- n_topics %>%
    future_map(LDA, x = ap_dtm, control = list(seed = 1234))
  save(ap_lda_compare, file = here("static", "data", "ap_lda_compare.Rdata"))
}
```

```{r ap_lda_compare_viz, dependson="ap_lda_compare"} 
tibble(k = n_topics,
           perplex = map_dbl(ap_lda_compare, perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
```

---

# $k=100$

```{r ap_100_topn, dependson="ap_lda_compare"}
ap_lda_td <- tidy(ap_lda_compare[[6]])

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  filter(topic <= 12) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```
