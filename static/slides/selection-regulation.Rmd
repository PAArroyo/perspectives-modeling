---
title: "Linear Model Selection and Regulation"
author: "[MACS 30100](https://model.uchicago.edu) <br /> University of Chicago"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE,
                      fig.retina = 2)

library(tidyverse)
library(tidymodels)
library(glmnet)
library(leaps)
library(here)
library(rcfss)
library(patchwork)

set.seed(1234)
theme_set(theme_minimal(base_size = 16))
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}$$

---

# Linear model

$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p + \epsilon$$

* Least squares
* Adaptations of the linear model
    * Non-linear, additive, relationships
    * Alternative model fitting strategies
* Prediction accuracy
* Model interpretability

---

# Alternatives to least squares

* Subset selection
* Shrinkage (regularization)
* Dimension reduction

---

# Ames housing data

```{r ames}
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)

glimpse(ames_train)
```

---

# Subset selection

* Fit separate least squares regression to each possible combination of the $p$ predictors

1. Let $M_0$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.
1. For $k = 1, 2, \ldots, p$:
    * Fit $\binom{p}{k}$ models, and call it $M_k$
    * Pick the best among the $\binom{p}{k}$ models, and call it $M_k$
1. Select a single best model from among $M_0, \ldots, M_p$

---

# Subset selection

```{r ames-subset, dependson = "ames"}
ames_lite <- ames_train %>%
  select_if(is.numeric)
best_subset <- regsubsets(Sale_Price ~ ., ames_lite, nvmax = 40)
summary(best_subset)$outmat
```

---

# Subset selection

* Computationally intractable - search space is too large
* Requires estimating $2^p$ models
    * $p = 10 \leadsto 2^{10} = `r 2^10`$ possible models
    * $p = `r ncol(ames_train) - 1` \leadsto 2^{`r ncol(ames_train) - 1`} = `r 2^(ncol(ames_train) - 1)`$ possible models
* Does not evaluate statistical significance of individual coefficients

---

# Forward stepwise selection

* Begin with null model, add predictors one at at time
* Only keep the variable which improves the model the most
* Requires fitting $1 + \sum_{k=0}^{p-1} (p-k) = 1 + \frac{p(p+1)}{2}$ models

--

1. Let $M_0$ denote the null model, which contains no predictors
1. For $k = 0, \ldots, p-1$:
    * Consider all $p-k$ models that augment the predictors in $M_k$ with one additional predictor
    * Choose the best among these $p-k$ models and call it $M_{k+1}$
1. Select a single best model from among $M_0, \ldots, M_p$

---

# Forward stepwise selection

```{r ames-forward, dependson = c("ames", "ames-subset")}
forward <- regsubsets(Sale_Price ~ ., ames_lite, nvmax = 308, method = "forward")
summary(forward)$outmat
```

---

# Backward stepwise selection

* Begin with full least squares model, remove predictors one at at time

--

1. Let $M_p$ denote the full model, which contains all $p$ predictors
1. For $k = p, p-1, , \ldots, 1$:
    * Consider all $k$ models that contain all but one of the predictors in $M_k$ for a total of $k - 1$ predictors
    * Choose the best among these $k$ models and call it $M_{k-1}$
1. Select a single best model from among $M_0, \ldots, M_p$

---

# Backward stepwise selection

```{r ames-backward, dependson = c("ames", "ames-subset")}
backward <- regsubsets(Sale_Price ~ ., ames_lite, nvmax = 308, method = "backward")
summary(backward)$outmat
```

---

# Comparing models

* Select the **best** model based on test error
* Indirectly estimate test error
* Directly estimate test error

---

# Indirect tests

| Statistic | Objective | Equation |
|--------------------------------------|-----------|--------------------------------------------------------------|
| $C_p$ | Minimize | $C_p = \frac{1}{n} (RSS + 2d\hat{\sigma}^2)$ |
| Akaike information criterion (AIC) | Minimize | $AIC = \frac{1}{n\hat{\sigma}^2} (RSS + 2d\hat{\sigma}^2)$ |
| Bayesian information criterion (BIC) | Minimize | $BIC = \frac{1}{n}(RSS + \log(n) d\hat{\sigma}^2)$ |
| Adjusted $R^2$ | Maximize | $\text{adj} \, R^2 = 1 - \frac{RSS / (n - d - 1)}{TSS / (n-1)}$ |

* $d$ - number of predictors
* $\sigma^2$ - estimate of the variance of the error $\epsilon$

---

# Subset selection

```{r ames-subset-stats, dependson = "ames-subset", fig.width = 12}
results <- summary(best_subset)

# best models per each metric
results_best <- tibble(
  `adj_r2` = which.max(results$adjr2),
  BIC = which.min(results$bic),
  `c_p` = which.min(results$cp)
) %>%
  gather(statistic, best)

# extract and plot results
tibble(`c_p` = results$cp,
       `adj_r2` = results$adjr2,
       BIC = results$bic) %>%
  mutate(predictors = row_number()) %>%
  gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line() +
  geom_point() +
  geom_vline(data = results_best,
             aes(xintercept = best, color = statistic), linetype = 2) +
  facet_wrap(~ statistic, scales = "free") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  ggtitle("Subset selection")
```

---

# Subset selection

```{r ames-best-coef, dependson = "ames-subset-stats", fig.width = 12}
coef(best_subset, results_best$best) %>%
  set_names(results_best$best) %>%
  map(enframe) %>%
  bind_rows(.id = "num_vars") %>%
  filter(name != "(Intercept)") %>%
  ggplot(aes(name, value)) +
  geom_point(aes(color = num_vars), position = "dodge") +
  scale_color_brewer(type = "qual") +
  coord_flip() +
  labs(x = "Variable",
       y = "Estimated coefficient",
       color = "Number of variables\nin model") +
  theme(legend.position = "bottom")
```

---

# Stepwise forward selection

```{r ames-forward-stats, dependson = "ames-forward", fig.width = 12}
results <- summary(forward)

# best models per each metric
results_best <- tibble(
  `adj_r2` = which.max(results$adjr2),
  BIC = which.min(results$bic),
  `c_p` = which.min(results$cp)
) %>%
  gather(statistic, best)

# extract and plot results
tibble(`c_p` = results$cp,
       `adj_r2` = results$adjr2,
       BIC = results$bic) %>%
  mutate(predictors = row_number()) %>%
  gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line() +
  geom_point() +
  geom_vline(data = results_best,
             aes(xintercept = best, color = statistic), linetype = 2) +
  facet_wrap(~ statistic, scales = "free") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  ggtitle("Stepwise forward selection")
```

---

# Stepwise backward selection

```{r ames-backward-stats, dependson = "ames-backward", fig.width = 12}
results <- summary(backward)

# best models per each metric
results_best <- tibble(
  `adj_r2` = which.max(results$adjr2),
  BIC = which.min(results$bic),
  `c_p` = which.min(results$cp)
) %>%
  gather(statistic, best)

# extract and plot results
tibble(`c_p` = results$cp,
       `adj_r2` = results$adjr2,
       BIC = results$bic) %>%
  mutate(predictors = row_number()) %>%
  gather(statistic, value, -predictors) %>%
  ggplot(aes(predictors, value, color = statistic)) +
  geom_line() +
  geom_point() +
  geom_vline(data = results_best,
             aes(xintercept = best, color = statistic), linetype = 2) +
  facet_wrap(~ statistic, scales = "free") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  ggtitle("Stepwise backward selection")
```

---

# Indirect tests

1. Different subsetting procedures (best subset vs. forward stepwise vs. backward stepwise) will likely identify different "best" models
1. Different indirect error test estimate statistics will likely identify different "best" models

---

# Direct tests

* Validation/cross-validation approach

---

# Subset selection

```{r ames-model-mat, dependson = c("ames", "ames-subset")}
test_m <- model.matrix(Sale_Price ~ ., data = select_if(ames_test, is.numeric))
```

```{r ames-subset-test-mse, dependson = "ames-model-mat", fig.width = 12}
tibble(
  nvars = seq(from = 1, to = 33)
) %>%
  mutate(coef = map(nvars, ~ coef(best_subset, id = .x)),
         pred = map(coef, ~ test_m[ , names(.x)] %*% .x)) %>%
  unnest(pred) %>%
  group_by(nvars) %>%
  mutate(truth = ames_test$Sale_Price) %>%
  mse(truth = truth, estimate = pred) %>%
  ggplot(aes(nvars, .estimate)) +
  geom_line() +
  geom_point() +
  labs(x = "Number of variables",
       y = "Validation set test MSE")
```

---

# Subset selection cross-validation

* Perform best subset selection within each of the $k$ training sets
* May lead to different selection of variables for each model

```{r ames-subset-cv, dependson = "ames-subset", include = FALSE, fig.width = 12}
# predict function for regsubsets object
predict.regsubsets <- function(object, newdata, id ,...) {
  form <- as.formula(Sale_Price ~ .) 
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  as.vector(mat[, xvars] %*% coefi, mode = "integer")
}

predict_all <- function(object, newdata, k){
  map(k, ~ predict.regsubsets(object, newdata, id = .x)) %>%
    set_names(k)
}

ames_lite_cv <- vfold_cv(ames_lite, v = 10)

ames_lite_cv <- ames_lite_cv %>%
  mutate(model = map(splits, ~ regsubsets(Sale_Price ~ ., data = analysis(.x), nvmax = 40)),
         pred = map2(model, splits, ~ predict_all(.x, assessment(.y), k = 1:33))) %>%
  unnest(pred, .preserve = splits) %>%
  group_by(id) %>%
  mutate(k = 1:33,
         truth = map(splits, ~ assessment(.x)$Sale_Price)) %>%
  unnest(pred, truth) %>%
  group_by(id, k) %>%
  mse(truth = truth, estimate = pred) %>%
  group_by(k) %>%
  summarize(.estimate = mean(.estimate))
```

```{r ames-subset-cv-plot, dependson = "ames-subset-cv", fig.width = 12}
ggplot(ames_lite_cv, aes(k, .estimate)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.min(ames_lite_cv$.estimate), linetype = 2) +
  labs(title = "Subset selection",
       x = "Number of variables",
       y = "10-fold CV MSE")
```

---

# Subset selection

```{r ames-subset-final, dependson = "ames-subset"}
final_best <- regsubsets(Sale_Price ~ ., data = ames_lite, nvmax = 40)
coef(final_best, which.min(ames_lite_cv$.estimate))
```

---

# Least squares

* Least squares objective function

    $$\min \left\{ \text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \right\}$$

## Works well with

* Linear relationship
* Multivariate normality
* No autocorrelation
* Homoscedastic error terms
* $n > p$
* No or little multicollinearity

---

# As $p$ increases

* Multicollinearity

```{r ames-colin, dependson = "ames"}
# fit with two strongly correlated variables
list(
  both = lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train),
  living_area = lm(Sale_Price ~ Gr_Liv_Area, data = ames_train),
  rooms_abv = lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)
) %>%
  map_df(tidy, .id = "model") %>%
  select(model, term, estimate) %>%
  spread(model, estimate) %>%
  knitr::kable(format = "html")
```

* Insufficient solution
* Interpretability
    * Hard thresholding
    * Soft thresholding

---

# Regularized regression

* Penalized or shrinkage models
* Constrain the magnitude of the coefficients
* Shrink the coefficients towards zero

    $$\min \{ \text{RSS} + P \}$$

---

# Ridge regression

$$\min \left\{ \text{RSS} + \lambda \sum_{j=1}^p \beta_j^2 \right\}$$

* $L_2$ penalty
* Tuning parameter $\lambda$
    * $\lambda = 0$
    * $\lambda \rightarrow \infty$
* Importance of standardizing predictors

---

# Ridge regression with $p > n$

$$\text{RSS}(\lambda) = (\mathbf{y} - \mathbf{X} \beta)' (\mathbf{y} - \mathbf{X}\beta) + \lambda \beta' \beta$$
$$\hat{\beta}^{\text{ridge}} = (\mathbf{X}' \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}' \mathbf{y}$$

* Ridge regression solution is linear function of $\mathbf{y}$
* Adds positive constant to $\mathbf{X}' \mathbf{X}$ before inversion
* Makes the matrix non-singular, even if $\mathbf{X}' \mathbf{X}$ is not full rank

---

# Ridge regression

```{r ridge-example, fig.width = 12}
credit <- ISLR::Credit %>%
  as_tibble

credit_x <- model.matrix(Balance ~ ., data = credit)[, -1]
credit_y <- credit$Balance

credit_ridge <- glmnet(
  x = credit_x,
  y = credit_y,
  alpha = 0
)

plot(credit_ridge, xvar = "lambda")
```

---

# Ridge regression

```{r ames-train-matrix, dependson = "ames"}
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
ames_train_x <- model.matrix(Sale_Price ~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Sale_Price)

ames_test_x <- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y <- log(ames_test$Sale_Price)
```

```{r ames-ridge, dependson = "ames-train-matrix", fig.width = 12}
# Apply Ridge regression to ames data
ames_ridge <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge, xvar = "lambda")
```

---

# Ridge regression

## Largest lambda parameter

```{r ames-ridge-lambda-large, dependson = "ames-ridge"}
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 1]
```

## Smallest lambda parameter

```{r ames-ridge-lambda-small, dependson = "ames-ridge"}
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]
```

---

# Tuning $\lambda$

* $10$-fold cross-validation
* $\lambda$ with minimum MSE
* Largest $\lambda$ within one SE of the minimum MSE

```{r ames-ridge-tune, dependson = "ames-train-matrix", fig.width = 12}
# Apply CV Ridge regression to ames data
ames_ridge <- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

# plot results
plot(ames_ridge)
```

---

# Ridge regression

```{r ames-ridge-min-mse, dependson = "ames-ridge-tune", fig.width = 12}
ames_ridge_min <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge_min, xvar = "lambda")
abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")
```

---

# Ridge regression

* Pushes correlated features towards each other
* Non-important features pushed towards zero
* Reducing the noise, increasing the signal
* Retains all variables (i.e. not feature selection)

---

# Ridge regression

```{r ames-ridge-imp-vars, dependson = "ames-ridge", fig.width = 12}
coef(ames_ridge, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  labs(title = "Top 25 influential variables",
       x = "Coefficient",
       y = NULL)
```

---

# Lasso regression

* Least absolute shrinkage and selection operator (LASSO)

    $$\min \left\{ \text{RSS} + \lambda \sum_{j=1}^p | \beta_j | \right\}$$

* $L_1$ penalty
* Pushes variables to actual zero
* Increased regulation
* Automated feature selection

---

# Lasso regression

```{r lasso-example, dependson = "ridge-example", fig.width = 12}
credit_lasso <- glmnet(
  x = credit_x,
  y = credit_y,
  alpha = 1
)

plot(credit_lasso, xvar = "lambda")
```

---

# Lasso regression

```{r ames-lasso, dependson = "ames-train-matrix", fig.width = 12}
ames_lasso <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso, xvar = "lambda")
```

---

# Tuning $\lambda$

* $10$-fold cross-validation
* $\lambda$ with minimum MSE
* Largest $\lambda$ within one SE of the minimum MSE

```{r ames-lasso-tune, dependson = "ames-train-matrix", fig.width = 12}
ames_lasso <- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

# plot results
plot(ames_lasso)
```

---

# Lasso regression

```{r ames-lasso-min-mse, dependson = "ames-lasso-tune", fig.width = 12}
ames_lasso_min <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso_min, xvar = "lambda")
abline(v = log(ames_lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(ames_lasso$lambda.1se), col = "red", lty = "dashed")
```

---

# Lasso regression

* Pushes collinear features towards each other
* Pushes coefficients to zero
* Simplifies feature selection
* Oversimplification leads to less accuracy
    * Ridge MSE: `r min(ames_ridge$cvm)`
    * Lasso MSE: `r min(ames_lasso$cvm)`

---

# Lasso regression

```{r ames-lasso-imp-vars, dependson = "ames-lasso", fig.asp = .7, fig.width = 12}
coef(ames_lasso, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  labs(title = "Influential variables",
       x = "Coefficient",
       y = NULL) +
  theme_minimal(base_size = 8)
```

---

# Variable selection with ridge/lasso

.center[

![](/img/esl-3-11.png)

]

---

# Elastic nets

* Generalization of the ridge and lasso models

    $$\min \left\{ \text{RSS} + \lambda_1 \sum_{j=1}^p \beta_j^2 + \lambda_2 \sum_{j=1}^p | \beta_j | \right\}$$
    
* Balances strengths/weaknesses of both approaches
    * Effective regularization with ridge ($L_2$ penalty)
    * Feature selection with lasso ($L_1$ penalty)

---

# Elastic nets

```{r ames-elastic-net, dependson = "ames-model-matrix", fig.width = 12}
lasso    <- glmnet(ames_train_x, ames_train_y, alpha = 1.0) 
elastic1 <- glmnet(ames_train_x, ames_train_y, alpha = 0.25) 
elastic2 <- glmnet(ames_train_x, ames_train_y, alpha = 0.75) 
ridge    <- glmnet(ames_train_x, ames_train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
```

---

# Tuning $\lambda$ and $\alpha$

* Grid search

```{r ames-elastic-net-tune, dependson = "ames-model-matrix", fig.width = 12}
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(ames_train_y), replace = TRUE)

# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)

for(i in seq_along(tuning_grid$alpha)) {
  # fit CV model for each alpha value
  fit <- cv.glmnet(ames_train_x, ames_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) +
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")
```






