---
title: Bagging
date: 2019-02-25T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Tree-based inference
    weight: 2
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(tidymodels)
library(tree)
library(randomForest)
library(patchwork)
library(titanic)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}

# Bagging

[Decision trees](/notes/decision-trees/) suffer from **high variance**: even a small change in the training/test set partitions can lead to substantial changes in the estimated model and resulting fit. However a method implementing **low variance** should provide more consistent estimates, regardless of the sample split. By **bootstrap aggregating**, or simply **bagging**, is a general method for reducing variance in estimates.

We already met the [bootstrap](/notes/bootstrap/). Recall that this involves repeatedly sampling with replacement from a sample, estimating a parameter or set of parameters for each bootstrap sample, then averaging across the bootstrap samples to form our bootstrap estimate of the parameter. By averaging across all the bootstrap samples, we reduce the variance $\sigma^2$ in our final estimate.^[The variance for each observation in an independent sample $Z_1, Z_2, \dots, Z_n$ is $\sigma^2$. The variance for the average of the sample $\bar{Z}$ is $\frac{\sigma^2}{n}$. By averaging across the observations, we reduce the estimated variance. Intuitively this makes sense because our estimate of $\bar{Z}$ is based on more information, and should therefore be more stable.]

As this applies to statistical learning methods, we estimate $\hat{f}^1(x), \hat{f}^2(x), \dots, \hat{f}^B(x)$ using $B$ separate training sets, and average across the models to generate a single low-variance model:

$$\hat{f}_{\text{avg}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^b(x)$$

Since we don't have that many training sets, we bootstrap them, just like how we [estimated bootstrap parameters for a linear regression model](/notes/bootstrap/#estimating-the-accuracy-of-a-linear-regression-model). We estimate a decision tree model on each bootstrap sample and average the results of the models to generate the bagged estimate:

$$\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^b(x)$$

Each tree is grown without pruning, so they are high-variance but low-bias. Then by averaging across the results, we should get an estimate that has low-bias **and** low-variance. For regression trees this is straight-forward. For classification trees, we estimate $B$ trees and for a given test observation assign it the **majority-class result**: the overall prediction is the most commonly occurring predicted outcome across all the $B$ predictions. Compared to the error rate for the corresponding classification tree, bagged estimates generally have slightly lower error rates.

## Out-of-bag estimates

Fortunately using a bagged approach also allows us to avoid using any type of resampling method to calculate the test MSE or error rate. This is because we have a natural test set as a result of the bootstrapping process. Recall that in a bootstrap sampling process, we **sample with replacement**. This means that in some bootstrap samples, an observation may never be drawn. In fact, there is a pattern to this phenomenon. On average, each bagged tree uses approximately two-thirds of the original observations. Therefore observations not appearing in a given bag are considered **out-of-bag observations** (OOB).

```{r boot-prop}
# generate sample index
samp <- tibble(x = seq.int(1000))

# generate bootstrap sample and count proportion of observations in each draw
prop_drawn <- bootstraps(samp, times = nrow(samp)) %>%
  mutate(strap = map(splits, analysis)) %>%
  unnest(strap) %>%
  mutate(drawn = TRUE) %>%
  complete(id, x, fill = list(drawn = FALSE)) %>%
  distinct %>%
  group_by(x) %>%
  mutate(n_drawn = cumsum(drawn),
         id = parse_number(id),
         n_prop = n_drawn / id)

ggplot(prop_drawn, aes(id, n_prop, group = x)) +
  geom_line(alpha = .05) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "b-th bootstrap sample ",
       y = "Proportion i-th observation in samples 1:b")
```

Because of this, we can calculate the **out-of-bag error estimate**, or the average error estimate for out-of-bag observations. First we generate bagged predictions for each observation $i$ using only its OOB estimates, then we average across all $i$ observations to get the OOB error estimate. This is a valid estimate of the test error rate/MSE because it only uses observations that were not part of the training observations for a given bag $b$. This is far more computationally advantageous than calculating a cross-validated error rate for a bagged model. Consider the following example predicting survival on the Titanic using all available predictors in the dataset:^[This includes all variables in the data frame that are not merely text values.]

```{r titanic}
titanic <- titanic_train %>%
  as_tibble() %>%
  mutate(Survived = factor(Survived, levels = 0:1, labels = c("Died", "Survived")),
         Female = factor(Sex, levels = c("male", "female")))
```

```{r titanic-bag-oob, dependson = "titanic"}
titanic_rf_data <- titanic %>%
    select(-Name, -Ticket, -Cabin, -Sex, -PassengerId) %>%
    mutate_each(funs(as.factor(.)), Pclass, Embarked) %>%
    na.omit

(titanic_bag <- randomForest(Survived ~ ., data = titanic_rf_data,
                             mtry = 7, ntree = 500))
```

##### Estimation time for OOB error rate

```{r titanic-bag-oob-time}
system.time({
  randomForest(Survived ~ ., data = titanic_rf_data,
                              mtry = 7, ntree = 500)
})
```

##### Estimation time for $10$-fold CV error rate

```{r titanic-bag-cv-time}
system.time({
  vfold_cv(titanic_rf_data, v = 10) %>%
    mutate(model = map(splits, ~ randomForest(x = select(analysis(.x), -Survived),
                                              y = analysis(.x)$Survived,
                                              mtry = 7, ntree = 500)),
           estimate = map2(model, splits, ~predict(.x, newdata = assessment(.y))),
           truth = map(splits, ~ assessment(.x)$Survived)) %>%
    unnest(truth, estimate) %>%
    group_by(id) %>%
    accuracy(truth = truth, estimate = estimate) %>%
    summarize(mean(.estimate))
})
```

For our Titanic bagged model with all available predictors, we estimate an OOB error rate of $`r tail(titanic_bag$err.rate[,1], n = 1) * 100`\%$. Likewise, we obtain a [confusion matrix](/notes/logistic-regression/#confusion-matrix) to identify our error rate for each class.

## Variable importance measures

Interpreting a bagged model is much more difficult than interpreting a single decision tree. Because each tree is unique, we cannot plot an "average" of the trees like we might with a bootstrapped linear model. The most common method of interpretation (beyond prediction accuracy) is **variable importance**, or attempting to assess how important each variable is to the model. In regression trees, for each predictor we calculate the total amount of reduction in the RSS attributable to splits caused by the predictor, averaged over the $B$ trees. For classification trees, we do the same thing using average reduction in the Gini index.

```{r titanic-varimp}
tibble(var = rownames(importance(titanic_bag)),
           MeanDecreaseGini = importance(titanic_bag)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting survival on the Titanic",
       subtitle = "Bagging",
       x = NULL,
       y = "Average decrease in the Gini Index")
```

For classification trees, larger values are better. So for the Titanic bagged model, gender, age, and fare are the most important predictors, whereas number of siblings/parents aboard and the port of departure are relatively unimportant.

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* @james2013introduction
* @friedman2001elements
