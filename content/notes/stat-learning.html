---
title: Statistical learning
date: 2019-01-09T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Selecting and fitting a model
    weight: 1
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#statistical-learning"><span class="toc-section-number">1</span> Statistical learning</a><ul>
<li><a href="#what-is-statistical-learning"><span class="toc-section-number">1.1</span> What is statistical learning?</a></li>
<li><a href="#why-estimate-f"><span class="toc-section-number">1.2</span> Why estimate <span class="math inline">\(f\)</span>?</a><ul>
<li><a href="#prediction"><span class="toc-section-number">1.2.1</span> Prediction</a></li>
<li><a href="#inference"><span class="toc-section-number">1.2.2</span> Inference</a></li>
</ul></li>
<li><a href="#how-do-we-estimate-f"><span class="toc-section-number">1.3</span> How do we estimate <span class="math inline">\(f\)</span>?</a><ul>
<li><a href="#parametric-methods"><span class="toc-section-number">1.3.1</span> Parametric methods</a></li>
<li><a href="#non-parametric-methods"><span class="toc-section-number">1.3.2</span> Non-parametric methods</a></li>
</ul></li>
<li><a href="#supervised-vs.unsupervised-learning"><span class="toc-section-number">1.4</span> Supervised vs. unsupervised learning</a></li>
</ul></li>
<li><a href="#statistical-learning-vs.machine-learning"><span class="toc-section-number">2</span> Statistical learning vs. machine learning</a></li>
<li><a href="#classification-vs.regression"><span class="toc-section-number">3</span> Classification vs. regression</a></li>
<li><a href="#model-estimation-strategies"><span class="toc-section-number">4</span> Model estimation strategies</a></li>
<li><a href="#session-info"><span class="toc-section-number">5</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">6</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(here)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<div id="statistical-learning" class="section level1">
<h1><span class="header-section-number">1</span> Statistical learning</h1>
<div id="what-is-statistical-learning" class="section level2">
<h2><span class="header-section-number">1.1</span> What is statistical learning?</h2>
<p><strong>Statistical models</strong> attempt to summarize relationships between variables by reducing the dimensionality of the data.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> For example, here we have some simulated data on sales of <a href="https://www.shamwow.com/">Shamwow</a> in 200 different markets.</p>
<div class="figure">
<img src="/img/shamwow.jpg" />

</div>
<p>Our goal is to improve sales of the Shamwow. Since we cannot directly increase sales of the product (unless we go out and buy it ourselves), our only option is to increase advertising across three potential mediums: newspaper, radio, and TV.</p>
<p>In this example, the advertising budgets are our <strong>input variables</strong>, also called <strong>independent variables</strong>, <strong>features</strong>, or <strong>predictors</strong>. The sales of Shamwows is the <strong>output</strong>, also called the <strong>dependent variable</strong> or <strong>response</strong>.</p>
<p>By plotting the variables against one another using a scatterplot, we can see there is some sort of relationship between each medium’s advertising spending and Shamwow sales:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get advertising data</span>
(advertising &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;Advertising.csv&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tbl_df</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># remove id column</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>X1))</code></pre></div>
<pre><code>## # A tibble: 200 x 4
##       TV Radio Newspaper Sales
##    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
##  1 230.   37.8      69.2  22.1
##  2  44.5  39.3      45.1  10.4
##  3  17.2  45.9      69.3   9.3
##  4 152.   41.3      58.5  18.5
##  5 181.   10.8      58.4  12.9
##  6   8.7  48.9      75     7.2
##  7  57.5  32.8      23.5  11.8
##  8 120.   19.6      11.6  13.2
##  9   8.6   2.1       1     4.8
## 10 200.    2.6      21.2  10.6
## # … with 190 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot separate facets for relationship between ad spending and sales</span>
plot_ad &lt;-<span class="st"> </span>advertising <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(method, spend, <span class="op">-</span>Sales) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(spend, Sales)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>method, <span class="dt">scales =</span> <span class="st">&quot;free_x&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Spending (in thousands of dollars)&quot;</span>)
plot_ad</code></pre></div>
<p><img src="/notes/stat-learning_files/figure-html/plot_ad-1.png" width="672" /></p>
<p>But there seems to be a lot of noise in the data. How can we summarize this? We can do so by estimating a mathematical equation following the general form:</p>
<p><span class="math display">\[Y = f(X) + \epsilon\]</span></p>
<p>where <span class="math inline">\(f\)</span> is some fixed, unknown function of the relationship between the independent variable(s) <span class="math inline">\(X\)</span> and the dependent variable <span class="math inline">\(Y\)</span>, with some random error <span class="math inline">\(\epsilon\)</span>.</p>
<p>Statistical learning refers to the set of approaches for estimating <span class="math inline">\(f\)</span>. There are many potential approaches to defining the functional form of <span class="math inline">\(f\)</span>. One approach widely used is called <strong>least squares</strong> - it means that the overall solution minimizes the sum of the squares of the errors made in the results of the equation. The errors are simply the vertical difference between the actual values for <span class="math inline">\(y\)</span> and the predicted values for <span class="math inline">\(y\)</span>. Applied here, the results would look like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plot_ad <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="/notes/stat-learning_files/figure-html/plot_ad_fit-1.png" width="672" /></p>
<p>However statistical learning (and machine learning) allows us to use a wide range of functional forms beyond a simple linear model.</p>
</div>
<div id="why-estimate-f" class="section level2">
<h2><span class="header-section-number">1.2</span> Why estimate <span class="math inline">\(f\)</span>?</h2>
<p>There are two major goals of statistical modeling: prediction and inference.</p>
<div id="prediction" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Prediction</h3>
<p>Under a system of <strong>prediction</strong>, we use our knowledge of the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to predict <span class="math inline">\(Y\)</span> for given values of <span class="math inline">\(X\)</span>. Often the function <span class="math inline">\(f\)</span> is treated as a <strong>black box</strong> - we don’t care what the function is, as long as it makes accurate predictions. If we are trying to boost sales of Shamwow, we may not care why specific factors drive an increase in sales - we just want to know how to adjust our advertising budgets to maximize sales.</p>
</div>
<div id="inference" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Inference</h3>
<p>Under a system of <strong>inference</strong>, we use our knowledge of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to understand the relationship between the variables. Here we are most interested in the explanation, not the prediction. So in the Shamwow example, we may not care about actual sales of the product - instead, we may be economists who wish to understand how advertising spending influences product sales. We don’t care about the actual product, we simply want to learn more about the process and <strong>generalize</strong> it to a wider range of settings.</p>
</div>
</div>
<div id="how-do-we-estimate-f" class="section level2">
<h2><span class="header-section-number">1.3</span> How do we estimate <span class="math inline">\(f\)</span>?</h2>
<p>There are two major approaches to estimating <span class="math inline">\(f\)</span>: parametric and non-parametric methods.</p>
<div id="parametric-methods" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Parametric methods</h3>
<p>Parametric methods involve a two-stage process:</p>
<ol style="list-style-type: decimal">
<li>First make an assumption about the functional form of <span class="math inline">\(f\)</span>. For instance, OLS assumes that the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <strong>linear</strong>. This greatly simplifies the problem of estimating the model because we know a great deal about the properties of linear models.</li>
<li>After a model has been selected, we need to <strong>fit</strong> or <strong>train</strong> the model using the actual data. We demonstrated this previously with ordinary least squares. The estimation procedure minimizes the sum of the squares of the differences between the observed responses <span class="math inline">\(Y\)</span> and those predicted by a linear function <span class="math inline">\(\hat{Y}\)</span>.</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">method_model &lt;-<span class="st"> </span><span class="cf">function</span>(df) {
  <span class="kw">lm</span>(Sales <span class="op">~</span><span class="st"> </span>spend, <span class="dt">data =</span> df)
}

ad_pred &lt;-<span class="st"> </span>advertising <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(method, spend, <span class="op">-</span>Sales) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(method) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">nest</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(data, method_model),
         <span class="dt">pred =</span> <span class="kw">map</span>(model, broom<span class="op">::</span>augment)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(pred)

plot_ad <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="dt">data =</span> ad_pred,
                 <span class="kw">aes</span>(<span class="dt">ymin =</span> Sales, <span class="dt">ymax =</span> .fitted),
                 <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>,
                 <span class="dt">alpha =</span> .<span class="dv">5</span>) </code></pre></div>
<p><img src="/notes/stat-learning_files/figure-html/plot_parametric-1.png" width="672" /></p>
<p>This is only one possible estimation procedure, but is popular because it is relatively intuitive. This model-based approach is referred to as <strong>parametric</strong>, because it simplifies the problem of estimating <span class="math inline">\(f\)</span> to estimating a set of parameters in the function:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_{1}X_1\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is the sales, <span class="math inline">\(X_1\)</span> is the advertising spending in a given medium (newspaper, radio, or TV), and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are parameters defining the intercept and slope of the line.</p>
<p>The downside to parametric methods is that they assume a specific functional form of the relationship between the variables. Sometimes relationships really are linear - often however they are not. They could be curvilinear, parbolic, interactive, etc. Unless we know this <em>a priori</em> or test for all of these potential functional forms, it is possible our parametric method will not accurately summarize the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="non-parametric-methods" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Non-parametric methods</h3>
<p>Non-parametric methods do not make any assumptions about the functional form of <span class="math inline">\(f\)</span>. Instead, they use the data itself to estimate <span class="math inline">\(f\)</span> so that it gets as close as possible to the data points without becoming overly complex. By avoiding any assumptions about the functional form, non-parametric methods avoid the issues caused by parametic models. However, by doing so non-parametric methods require a large set of observations to avoid <strong>overfitting</strong> the data and obtain an accurate estimate of <span class="math inline">\(f\)</span>.</p>
<p>One non-parametric method is <strong>locally weighted scatterplot smoothing</strong> (LOWESS or LOESS). This method estimates a regression line based on localized subsets of the data, building up the global function <span class="math inline">\(\hat{f}\)</span> point-by-point. Here is an example of a LOESS on the <code>ethanol</code> dataset in the <code>lattice</code> package:</p>
<p><img src="/notes/stat-learning_files/figure-html/loess-1.png" width="672" /></p>
<p>The LOESS is built up point-by-point:</p>
<p><img src="/notes/stat-learning_files/figure-html/loess_buildup-1.gif" /><!-- --></p>
<p>One important argument you can control with LOESS is the <strong>span</strong>, or how smooth the LOESS function will become. A larger span will result in a smoother curve, but may not be as accurate.</p>
<p><img src="/notes/stat-learning_files/figure-html/loess_span-1.gif" /><!-- --></p>
</div>
</div>
<div id="supervised-vs.unsupervised-learning" class="section level2">
<h2><span class="header-section-number">1.4</span> Supervised vs. unsupervised learning</h2>
<p>All the examples above implement <strong>supervised</strong> learning. That is, for each observation we have both the predictor measurements and the response measurements (i.e. an <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>). We seek to fit a model that summarizes the relationship between the predictors and response.</p>
<p>In <strong>unsupervised</strong> learning, all we have is a set of measurements <span class="math inline">\(X\)</span> for a series of observations, but no corresponding response <span class="math inline">\(Y\)</span>. Without an outcome measure, we cannot fit a linear regression model or employ a similar method. That does not mean we cannot use statistical learning to understand the data better. One example of unsupervised learning is <strong>cluster analysis</strong>. The goal is to determine whether the observations fall into distinct categories. <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Direchlet allocation (LDA)</a> is an example of cluster analysis applied to text data. In LDA, the individual words are the features or measurements we use to determine the best fitting clusters.</p>
</div>
</div>
<div id="statistical-learning-vs.machine-learning" class="section level1">
<h1><span class="header-section-number">2</span> Statistical learning vs. machine learning</h1>
<p>I prefer to use the term <strong>statistical learning</strong>, but the field of statistical learning is strongly related to that of <strong>machine learning</strong>. Statistical learning is a subfield of statistics that focuses predominantly on <strong>inference</strong>. It is ues to identify underlying relationships between variables and emphasizes models and their interpretability. People employing statistical learning methods are usually concerned with uncertainty and precision of their estimates.</p>
<p>Machine learning is a subfield of computer science and focuses more strongly on <strong>prediction</strong>. It typically employs larger scale applications (think of predictive analytics at Google or Netflix) and emphasizes prediction accuracy. Machine learning is happy to sacrifice model interpretability in exchange for more accurate predictions.</p>
<p>In truth, both are quite similar approaches to inference and prediction. Both use the same major methods of modeling (parametric and non-parametric). I think of them as different languages, speaking the same thing.</p>
</div>
<div id="classification-vs.regression" class="section level1">
<h1><span class="header-section-number">3</span> Classification vs. regression</h1>
<p>Variables can be classified as <strong>quantitative</strong> or <strong>qualitative</strong>. Quantitative variables take on numeric values. In contrast, qualitative variables take on different <strong>classes</strong>, or discrete categories. Qualitative variables can have any number of classes, though binary categories are frequent:</p>
<ul>
<li>Yes/no</li>
<li>Male/female</li>
</ul>
<p>Problems with a quantitative dependent variable are typically called <strong>regression</strong> problems, whereas qualitative dependent variables are called <strong>classification</strong> problems. Part of this distinction is merely semantic, but different methods may be employed depending on the type of response variable. For instance, you would not use linear regression on a qualitative response variable. Conceptually, how would you define a linear function for a response variable that takes on the values “male” or “female”? It doesn’t make any conceptual sense. Instead, you can employ classification methods such as <strong>logistic regression</strong> to estimate the probability that based on a set of predictors a specific observation is part of a response class.</p>
<p>That said, whether <strong>predictors</strong> are qualitative or quantitative is not important in determining whether the problem is one of regression or classification. As long as qualitative predictors are properly coded before the analysis is conducted, they can be used for either type of problem.</p>
</div>
<div id="model-estimation-strategies" class="section level1">
<h1><span class="header-section-number">4</span> Model estimation strategies</h1>
<p><strong>Statistical learning</strong> defines the modeling strategy used to estimate the functional form of <span class="math inline">\(\hat{f}\)</span>. <strong>Model estimation</strong> defines the procedure for fitting the data to a given functional form so that we estimate the “correct” or optimal parameters for the model.</p>
<p>For example, <strong>generalized linear models</strong> are a flexible class of models that allow us to estimate linear regression for response variables that have error distribution models other than the normal distribution. A GLM consists of three components:</p>
<ol style="list-style-type: decimal">
<li>A <strong>random component</strong> specifying the conditional distribution of the response variable, <span class="math inline">\(Y_i\)</span>, given the values of the predictor variables in the model. Typically these distributions are a member of the <a href="https://en.wikipedia.org/wiki/Exponential_family"><strong>exponential family</strong></a>, a set of related probability distributions.</li>
<li><p>A <strong>linear predictor</strong> that is a linear function of regressors:</p>
<p><span class="math display">\[\eta_i = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik}\]</span></p>
The regressors are prespecified functions of the explanatory variables. This is exactly like the form you’ve seen for <a href="persp003_linear_regression.html">linear</a> and <a href="persp004_logistic_regression.html">logistic</a> regression, because in fact linear and logistic regression are types of GLMs.</li>
<li><p>A <strong>link function</strong> <span class="math inline">\(g(\cdot)\)</span> which transforms the expectation of the response variable, <span class="math inline">\(\mu_i \equiv E(Y_i)\)</span> to the linear predictor:</p>
<p><span class="math display">\[g(\mu_i) = \eta_i = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik}\]</span></p>
<p>Because the link function must also be <strong>invertible</strong>, we can also write it as:</p>
<p><span class="math display">\[\mu_i = g^{-1}(\eta_i) = g^{-1}(\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik})\]</span></p>
<p>The inverted link function is also known as the <strong>mean function</strong>. The purpose of the link function is to relate the linear predictor to the mean of the distribution function.</p></li>
</ol>
<p>GLM is the statistical learning model. The major parameters of interest are the <span class="math inline">\(\beta\)</span>s. How do we estimate the appropriate values for those parameters? That is, we could plug in any set of values to fit the functional form. Some values will be more accurate than others at explaining the relationship between <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(Y\)</span>. Estimation strategies will define how to calculate these optimal values. For instance, <a href="https://css18.github.io/parametric-inference.html"><strong>maximum likelihood</strong></a> is perhaps the most commonly employed estimation strategy, which identifies the set of parameter values that minimizes the log-likelihood function through the use of the first derivative (also known as the <strong>gradient</strong>). Alternatively, one could use a Bayesian estimation strategy, or the <a href="https://github.com/UC-MACSS/persp-model_W18/blob/master/Notebooks/GMM/GMMest.ipynb">generalized method of moments</a>. Notice that the statistical learning algorithm is agnostic in some respects to the estimation strategy. Some estimation procedures are better than others when paired with specific statistical learning strategies. In this class, we focus primarily on the statistical learning algorithm but will discuss model estimation/optimization as necessary.</p>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">5</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-01-08                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package     * version date       lib source                           
##  assertthat    0.2.0   2017-04-11 [2] CRAN (R 3.5.0)                   
##  backports     1.1.3   2018-12-14 [2] CRAN (R 3.5.0)                   
##  bindr         0.1.1   2018-03-13 [2] CRAN (R 3.5.0)                   
##  bindrcpp      0.2.2   2018-03-29 [1] CRAN (R 3.5.0)                   
##  blogdown      0.9.4   2018-11-26 [1] Github (rstudio/blogdown@b2e1ed4)
##  bookdown      0.9     2018-12-21 [1] CRAN (R 3.5.0)                   
##  broom         0.5.1   2018-12-05 [2] CRAN (R 3.5.0)                   
##  callr         3.1.1   2018-12-21 [2] CRAN (R 3.5.0)                   
##  cellranger    1.1.0   2016-07-27 [2] CRAN (R 3.5.0)                   
##  cli           1.0.1   2018-09-25 [1] CRAN (R 3.5.0)                   
##  colorspace    1.3-2   2016-12-14 [2] CRAN (R 3.5.0)                   
##  crayon        1.3.4   2017-09-16 [2] CRAN (R 3.5.0)                   
##  desc          1.2.0   2018-05-01 [2] CRAN (R 3.5.0)                   
##  devtools      2.0.1   2018-10-26 [1] CRAN (R 3.5.1)                   
##  digest        0.6.18  2018-10-10 [1] CRAN (R 3.5.0)                   
##  dplyr       * 0.7.8   2018-11-10 [1] CRAN (R 3.5.0)                   
##  evaluate      0.12    2018-10-09 [2] CRAN (R 3.5.0)                   
##  forcats     * 0.3.0   2018-02-19 [2] CRAN (R 3.5.0)                   
##  fs            1.2.6   2018-08-23 [1] CRAN (R 3.5.0)                   
##  generics      0.0.2   2018-11-29 [1] CRAN (R 3.5.0)                   
##  ggplot2     * 3.1.0   2018-10-25 [1] CRAN (R 3.5.0)                   
##  glue          1.3.0   2018-07-17 [2] CRAN (R 3.5.0)                   
##  gtable        0.2.0   2016-02-26 [2] CRAN (R 3.5.0)                   
##  haven         2.0.0   2018-11-22 [2] CRAN (R 3.5.0)                   
##  here        * 0.1     2017-05-28 [2] CRAN (R 3.5.0)                   
##  hms           0.4.2   2018-03-10 [2] CRAN (R 3.5.0)                   
##  htmltools     0.3.6   2017-04-28 [1] CRAN (R 3.5.0)                   
##  httr          1.4.0   2018-12-11 [2] CRAN (R 3.5.0)                   
##  jsonlite      1.6     2018-12-07 [2] CRAN (R 3.5.0)                   
##  knitr         1.21    2018-12-10 [2] CRAN (R 3.5.1)                   
##  lattice       0.20-38 2018-11-04 [2] CRAN (R 3.5.2)                   
##  lazyeval      0.2.1   2017-10-29 [2] CRAN (R 3.5.0)                   
##  lubridate     1.7.4   2018-04-11 [2] CRAN (R 3.5.0)                   
##  magrittr      1.5     2014-11-22 [2] CRAN (R 3.5.0)                   
##  memoise       1.1.0   2017-04-21 [2] CRAN (R 3.5.0)                   
##  modelr        0.1.2   2018-05-11 [2] CRAN (R 3.5.0)                   
##  munsell       0.5.0   2018-06-12 [2] CRAN (R 3.5.0)                   
##  nlme          3.1-137 2018-04-07 [2] CRAN (R 3.5.2)                   
##  pillar        1.3.1   2018-12-15 [2] CRAN (R 3.5.0)                   
##  pkgbuild      1.0.2   2018-10-16 [1] CRAN (R 3.5.0)                   
##  pkgconfig     2.0.2   2018-08-16 [2] CRAN (R 3.5.1)                   
##  pkgload       1.0.2   2018-10-29 [1] CRAN (R 3.5.0)                   
##  plyr          1.8.4   2016-06-08 [2] CRAN (R 3.5.0)                   
##  prettyunits   1.0.2   2015-07-13 [2] CRAN (R 3.5.0)                   
##  processx      3.2.1   2018-12-05 [2] CRAN (R 3.5.0)                   
##  ps            1.3.0   2018-12-21 [2] CRAN (R 3.5.0)                   
##  purrr       * 0.2.5   2018-05-29 [2] CRAN (R 3.5.0)                   
##  R6            2.3.0   2018-10-04 [1] CRAN (R 3.5.0)                   
##  Rcpp          1.0.0   2018-11-07 [1] CRAN (R 3.5.0)                   
##  readr       * 1.3.1   2018-12-21 [2] CRAN (R 3.5.0)                   
##  readxl        1.2.0   2018-12-19 [2] CRAN (R 3.5.0)                   
##  remotes       2.0.2   2018-10-30 [1] CRAN (R 3.5.0)                   
##  rlang         0.3.0.1 2018-10-25 [1] CRAN (R 3.5.0)                   
##  rmarkdown     1.11    2018-12-08 [2] CRAN (R 3.5.0)                   
##  rprojroot     1.3-2   2018-01-03 [2] CRAN (R 3.5.0)                   
##  rstudioapi    0.8     2018-10-02 [1] CRAN (R 3.5.0)                   
##  rvest         0.3.2   2016-06-17 [2] CRAN (R 3.5.0)                   
##  scales        1.0.0   2018-08-09 [1] CRAN (R 3.5.0)                   
##  sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 3.5.0)                   
##  stringi       1.2.4   2018-07-20 [2] CRAN (R 3.5.0)                   
##  stringr     * 1.3.1   2018-05-10 [2] CRAN (R 3.5.0)                   
##  testthat      2.0.1   2018-10-13 [2] CRAN (R 3.5.0)                   
##  tibble      * 2.0.0   2019-01-04 [2] CRAN (R 3.5.2)                   
##  tidyr       * 0.8.2   2018-10-28 [2] CRAN (R 3.5.0)                   
##  tidyselect    0.2.5   2018-10-11 [1] CRAN (R 3.5.0)                   
##  tidyverse   * 1.2.1   2017-11-14 [2] CRAN (R 3.5.0)                   
##  usethis       1.4.0   2018-08-14 [1] CRAN (R 3.5.0)                   
##  withr         2.1.2   2018-03-15 [2] CRAN (R 3.5.0)                   
##  xfun          0.4     2018-10-23 [1] CRAN (R 3.5.0)                   
##  xml2          1.2.0   2018-01-24 [2] CRAN (R 3.5.0)                   
##  yaml          2.2.0   2018-07-25 [2] CRAN (R 3.5.0)                   
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> References</h1>
<ul>
<li>This page is derived in part from <a href="http://varianceexplained.org/files/loess.html">“Creating a LOESS animation with <code>gganimate</code>”</a> by David Robinson.</li>
</ul>
<div id="refs" class="references">
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>These notes are a substantial summary of <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
