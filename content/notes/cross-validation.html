---
title: Cross-validation
date: 2019-01-28T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Resampling methods
    weight: 1
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#trainingtest-set-split"><span class="toc-section-number">1</span> Training/test set split</a></li>
<li><a href="#validation-set"><span class="toc-section-number">2</span> Validation set</a><ul>
<li><a href="#regression"><span class="toc-section-number">2.1</span> Regression</a></li>
<li><a href="#classification"><span class="toc-section-number">2.2</span> Classification</a></li>
<li><a href="#drawbacks-to-validation-sets"><span class="toc-section-number">2.3</span> Drawbacks to validation sets</a></li>
</ul></li>
<li><a href="#leave-one-out-cross-validation"><span class="toc-section-number">3</span> Leave-one-out cross-validation</a><ul>
<li><a href="#loocv-in-linear-regression"><span class="toc-section-number">3.1</span> LOOCV in linear regression</a></li>
<li><a href="#loocv-in-classification"><span class="toc-section-number">3.2</span> LOOCV in classification</a></li>
</ul></li>
<li><a href="#k-fold-cross-validation"><span class="toc-section-number">4</span> <span class="math inline">\(K\)</span>-fold cross-validation</a><ul>
<li><a href="#k-fold-cv-in-linear-regression"><span class="toc-section-number">4.1</span> <span class="math inline">\(K\)</span>-fold CV in linear regression</a></li>
<li><a href="#computational-speed-of-loocv-vs.-k-fold-cv"><span class="toc-section-number">4.2</span> Computational speed of LOOCV vs. <span class="math inline">\(K\)</span>-fold CV</a><ul>
<li><a href="#loocv"><span class="toc-section-number">4.2.1</span> LOOCV</a></li>
<li><a href="#fold-cv"><span class="toc-section-number">4.2.2</span> 10-fold CV</a></li>
</ul></li>
<li><a href="#k-fold-cv-in-logistic-regression"><span class="toc-section-number">4.3</span> <span class="math inline">\(K\)</span>-fold CV in logistic regression</a></li>
</ul></li>
<li><a href="#appropriate-value-for-k"><span class="toc-section-number">5</span> Appropriate value for <span class="math inline">\(K\)</span></a></li>
<li><a href="#variations-on-cross-validation"><span class="toc-section-number">6</span> Variations on cross-validation</a></li>
<li><a href="#session-info"><span class="toc-section-number">7</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">8</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(magrittr)
<span class="kw">library</span>(here)
<span class="kw">library</span>(rcfss)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<p><strong>Resampling methods</strong> are essential to test and evaluate statistical models. Because you likely do not have the resources or capabilities to repeatedly sample from your population of interest, instead you can repeatedly draw from your original sample to obtain additional information about your model. For instance, you could repeatedly draw samples from your data, estimate a linear regression model on each sample, and then examine how the estimated model differs across each sample. This allows you to assess the variability and stability of your model in a way not possible if you can only fit the model once.</p>
<p>There are two major types of resampling methods we will consider:</p>
<ul>
<li>Cross-validation - frequently used for model assessment and evaluating a model’s performance relative to other models</li>
<li><a href="/notes/bootstrap/">Bootstrap</a> - commonly used to provide a non-parametric measure of the accuracy of a parameter estimate or a given statistical learning method</li>
</ul>
<div id="trainingtest-set-split" class="section level1">
<h1><span class="header-section-number">1</span> Training/test set split</h1>
<p>In most modeling situations, we can immediately partition the dataset into a <strong>training</strong> set and a <strong>test</strong> set. The training set will be used for model construction, and the test set will be used to evaluate the performance of the final model. This is most important – while you can reuse the training set many times to build different statistical models, you can only use the test set of data once. If you reuse it, you introduce <strong>data leakage</strong> into your modeling process and no longer have unbiased estimates of the test error. This is why collaborative platforms such as <a href="https://www.kaggle.com/">Kaggle</a> hold back a portion of the dataset in their competitions. You can use the training set to build the strongest performing model, but you cannot tune your model based on the test error because you do not have access to it.</p>
</div>
<div id="validation-set" class="section level1">
<h1><span class="header-section-number">2</span> Validation set</h1>
<p>Even accounting for the training/test set split, one issue with using the same data to both fit and evaluate our model is that we will bias our model towards fitting the data that we have. We may fit our function to create the results we expect or desire, rather than the “true” function. Instead, we can further split our training set into distinct <strong>training</strong> and <strong>validation</strong> sets. The training set can be used repeatedly to train different models. We then use the validation set to evaluate the model’s performance, generating metrics such as the mean squared error (MSE) or the error rate. Unlike the test set, we are permitted to use the validation set multiple times. The important thing is that we do not use the validation set to train or fit the model, only evaluate its performance after it has been fit.</p>
<div id="regression" class="section level2">
<h2><span class="header-section-number">2.1</span> Regression</h2>
<p>Here we will examine the relationship between horsepower and car mileage in the <code>Auto</code> dataset (found in <code>library(ISLR)</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)

Auto &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(Auto)
Auto</code></pre></div>
<pre><code>## # A tibble: 392 x 9
##      mpg cylinders displacement horsepower weight acceleration  year origin
##    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1    18         8          307        130   3504         12      70      1
##  2    15         8          350        165   3693         11.5    70      1
##  3    18         8          318        150   3436         11      70      1
##  4    16         8          304        150   3433         12      70      1
##  5    17         8          302        140   3449         10.5    70      1
##  6    15         8          429        198   4341         10      70      1
##  7    14         8          454        220   4354          9      70      1
##  8    14         8          440        215   4312          8.5    70      1
##  9    14         8          455        225   4425         10      70      1
## 10    15         8          390        190   3850          8.5    70      1
## # … with 382 more rows, and 1 more variable: name &lt;fct&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/auto_plot-1.png" width="672" /></p>
<p>The relationship does not appear to be strictly linear:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/auto_plot_lm-1.png" width="672" /></p>
<p>Perhaps by adding quadratic terms to the linear regression we could improve overall model fit. To evaluate the model, we will split the data into a training set and validation set,<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> estimate a series of higher-order models, and calculate a test statistic summarizing the accuracy of the estimated <code>mpg</code>. To calculate the accuracy of the model, we will use <a href="/notes/model-accuracy/#quality-of-fit">mean squared error</a> (MSE), defined as</p>
<p><span class="math display">\[MSE = \frac{1}{N} \sum_{i = 1}^{N}{(y_i - \hat{f}(x_i))^2}\]</span></p>
<p>For this task, first we use <code>rsample::initial_split()</code> to create training and validation sets (using a 50/50 split), then estimate a linear regression model without any quadratic terms.</p>
<ul>
<li>I use <code>set.seed()</code> in the beginning - whenever you are writing a script that involves randomization (here, random subsetting of the data), always set the seed at the beginning of the script. This ensures the results can be reproduced precisely.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></li>
<li>I also use the <code>glm()</code> function rather than <code>lm()</code> - if you don’t change the <code>family</code> parameter, the results of <code>lm()</code> and <code>glm()</code> are exactly the same.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)

auto_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(<span class="dt">data =</span> Auto, <span class="dt">prop =</span> <span class="fl">0.5</span>)
auto_train &lt;-<span class="st"> </span><span class="kw">training</span>(auto_split)
auto_test &lt;-<span class="st"> </span><span class="kw">testing</span>(auto_split)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_lm &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> auto_train)
<span class="kw">summary</span>(auto_lm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = mpg ~ horsepower, data = auto_train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -13.7105   -3.4442   -0.5342    2.6256   15.1015  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 40.057910   1.054798   37.98   &lt;2e-16 ***
## horsepower  -0.157604   0.009402  -16.76   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 24.80151)
## 
##     Null deviance: 11780.6  on 195  degrees of freedom
## Residual deviance:  4811.5  on 194  degrees of freedom
## AIC: 1189.6
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>To estimate the MSE for a single partition (i.e. for a training or validation set):</p>
<ol style="list-style-type: decimal">
<li>Use <code>broom::augment()</code> to generate predicted values for the data set</li>
<li>Calculate the MSE from the fitted values using <code>rcfss::mse()</code>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></li>
</ol>
<p>For the training set, this would look like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(train_mse &lt;-<span class="st"> </span><span class="kw">augment</span>(auto_lm, <span class="dt">newdata =</span> auto_train) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(<span class="dt">truth =</span> mpg, <span class="dt">estimate =</span> .fitted))</code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 mse     standard        24.5</code></pre>
<p>For the validation set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(test_mse &lt;-<span class="st"> </span><span class="kw">augment</span>(auto_lm, <span class="dt">newdata =</span> auto_test) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(<span class="dt">truth =</span> mpg, <span class="dt">estimate =</span> .fitted))</code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 mse     standard        23.4</code></pre>
<p>For a strictly linear model, the MSE for the validation set is 23.38. How does this compare to a quadratic model? We can use the <code>poly()</code> function in conjunction with a <code>map()</code> iteration to estimate the MSE for a series of models with higher-order polynomial terms:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># visualize each model</span>
<span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;1&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">i =</span> <span class="dv">1</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;2&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">i =</span> <span class="dv">2</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;3&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">i =</span> <span class="dv">3</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;4&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">i =</span> <span class="dv">4</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;5&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">i =</span> <span class="dv">5</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Horsepower&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;MPG&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Highest-order</span><span class="ch">\n</span><span class="st">polynomial&quot;</span>)</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/mse-poly-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to estimate model using training set and generate fit statistics</span>
<span class="co"># using the test set</span>
poly_results &lt;-<span class="st"> </span><span class="cf">function</span>(train, test, i) {
  <span class="co"># Fit the model to the training set</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i, <span class="dt">raw =</span> <span class="ot">TRUE</span>), <span class="dt">data =</span> train)
  
  <span class="co"># `augment` will save the predictions with the test data set</span>
  res &lt;-<span class="st"> </span><span class="kw">augment</span>(mod, <span class="dt">newdata =</span> test) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mse</span>(<span class="dt">truth =</span> mpg, <span class="dt">estimate =</span> .fitted)
  
  <span class="co"># Return the test data set with the additional columns</span>
  res
}

<span class="co"># function to return MSE for a specific higher-order polynomial term</span>
poly_mse &lt;-<span class="st"> </span><span class="cf">function</span>(i, train, test){
  <span class="kw">poly_results</span>(train, test, i) <span class="op">%$%</span>
<span class="st">    </span><span class="kw">mean</span>(.estimate)
}

cv_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">terms =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">5</span>),
                 <span class="dt">mse_test =</span> <span class="kw">map_dbl</span>(terms, poly_mse, auto_train, auto_test))

<span class="kw">ggplot</span>(cv_mse, <span class="kw">aes</span>(terms, mse_test)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using validation set&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/mse-poly-2.png" width="672" /></p>
<p>Based on the MSE for the validation set, a polynomial model with a quadratic term (<span class="math inline">\(\text{horsepower}^2\)</span>) produces a lower average error than the standard model. A higher order term such as a fifth-order polynomial leads to an even larger reduction, though increases the complexity of interpreting the model.</p>
</div>
<div id="classification" class="section level2">
<h2><span class="header-section-number">2.2</span> Classification</h2>
<p>Recall our efforts to <a href="/notes/logistic-regression/">predict passenger survival during the sinking of the Titanic</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(titanic)
titanic &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(titanic_train) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Survived =</span> <span class="kw">factor</span>(Survived))

titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="left">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">0</td>
<td align="right">3</td>
<td align="left">Moran, Mr. James</td>
<td align="left">male</td>
<td align="right">NA</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">330877</td>
<td align="right">8.4583</td>
<td align="left"></td>
<td align="left">Q</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman_x &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                           <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman_x)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9401  -0.7136  -0.5883   0.7626   2.2455  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.59380    0.31032   1.913  0.05569 . 
## Age          0.01970    0.01057   1.863  0.06240 . 
## Sexmale     -1.31775    0.40842  -3.226  0.00125 **
## Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 740.40  on 710  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 748.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We can use the same validation set approach to evaluate the model’s accuracy. For classification models, instead of using MSE we examine the <strong>error rate</strong>. That is, of all the predictions generated for the validation set, what percentage of predictions are incorrect? The goal is to minimize this value as much as possible (ideally, until we make no errors and our error rate is <span class="math inline">\(0\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to convert log-odds to probabilities</span>
logit2prob &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">exp</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># split the data into training and validation sets</span>
titanic_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(<span class="dt">data =</span> titanic, <span class="dt">prop =</span> <span class="fl">0.5</span>)

<span class="co"># fit model to training data</span>
train_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> <span class="kw">training</span>(titanic_split),
                   <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(train_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = training(titanic_split))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1511  -0.7346  -0.5386   0.7339   2.2216  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.17464    0.41877   0.417 0.676659    
## Age          0.03570    0.01525   2.342 0.019198 *  
## Sexmale     -0.59608    0.56604  -1.053 0.292313    
## Age:Sexmale -0.06833    0.01994  -3.426 0.000612 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 478.37  on 353  degrees of freedom
## Residual deviance: 361.88  on 350  degrees of freedom
##   (92 observations deleted due to missingness)
## AIC: 369.88
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate predictions using validation set</span>
x_test_accuracy &lt;-<span class="st"> </span><span class="kw">augment</span>(train_model, <span class="dt">newdata =</span> <span class="kw">testing</span>(titanic_split)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.prob =</span> <span class="kw">logit2prob</span>(.fitted),
         <span class="dt">.pred =</span> <span class="kw">factor</span>(<span class="kw">round</span>(.prob)))

<span class="co"># calculate test accuracy rate</span>
<span class="kw">accuracy</span>(x_test_accuracy, <span class="dt">truth =</span> Survived, <span class="dt">estimate =</span> .pred)</code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.783</code></pre>
<p>This interactive model generates an error rate of 21.7%. We could compare this error rate to alternative classification models, either other logistic regression models (using different formulas) or a tree-based method.</p>
</div>
<div id="drawbacks-to-validation-sets" class="section level2">
<h2><span class="header-section-number">2.3</span> Drawbacks to validation sets</h2>
<p>There are two main problems with validation sets:</p>
<ol style="list-style-type: decimal">
<li><p>Validation estimates of the test error rates can be highly variable depending on which observations are sampled into the training and validation sets. See what happens if we repeat the sampling, estimation, and validation procedure for the <code>Auto</code> data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_variable &lt;-<span class="st"> </span><span class="cf">function</span>(Auto){
  auto_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(Auto, <span class="dt">prop =</span> <span class="fl">0.5</span>)
  auto_train &lt;-<span class="st"> </span><span class="kw">training</span>(auto_split)
  auto_test &lt;-<span class="st"> </span><span class="kw">testing</span>(auto_split)

  cv_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">terms =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">5</span>),
                       <span class="dt">mse_test =</span> <span class="kw">map_dbl</span>(terms, poly_mse, auto_train, auto_test))

  <span class="kw">return</span>(cv_mse)
}

<span class="kw">rerun</span>(<span class="dv">10</span>, <span class="kw">mse_variable</span>(Auto)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, mse_test, <span class="dt">color =</span> id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Variability of MSE estimates&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using the validation set approach&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/auto_variable_mse-1.png" width="672" /></p>
<p>Depending on the specific training/validation split, our MSE varies by up to 5.</p></li>
<li><p>If you don’t have a large data set, you’ll have to dramatically shrink the size of your training set. Most statistical learning methods perform better with more observations - if you don’t have enough data in the training set, you might overestimate the error rate in the test set.</p></li>
</ol>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level1">
<h1><span class="header-section-number">3</span> Leave-one-out cross-validation</h1>
<p>An alternative method is <strong>leave-one-out cross validation</strong> (LOOCV). Like with the validation set approach, you split the data into two parts. However the difference is that you only remove one observation for the validation set, and keep all remaining observations in the training set. The statistical learning method is fit on the <span class="math inline">\(N-1\)</span> training set. You then use the held-out observation to calculate the <span class="math inline">\(MSE = (Y_1 - \hat{Y}_1)^2\)</span> which should be an unbiased estimator of the test error. Because this MSE is highly dependent on which observation is held out, <strong>we repeat this process for every single observation in the data set</strong>. Mathematically, this looks like:</p>
<p><span class="math display">\[CV_{(N)} = \frac{1}{N} \sum_{i = 1}^{N}{MSE_i}\]</span></p>
<p>This method produces estimates of the error rate that are approximately unbiased and are non-varying for a given dataset, unlike the validation set approach where the MSE estimate is highly dependent on the sampling process for training/validation sets. However it can have high variance because the <span class="math inline">\(N\)</span> “training sets” are so similar to one another. LOOCV is also highly flexible and works with any kind of predictive modeling.</p>
<p>Of course the downside is that this method is computationally difficult. You have to estimate <span class="math inline">\(N\)</span> different models - if you have a large <span class="math inline">\(N\)</span> or each individual model takes a long time to compute, you may be stuck waiting a long time for the computer to finish its calculations.</p>
<div id="loocv-in-linear-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> LOOCV in linear regression</h2>
<p>We can use the <code>loo_cv()</code> function in the <code>rsample</code> library to compute the LOOCV of any linear or logistic regression model. It takes a single argument: the data frame being cross-validated. For the <code>Auto</code> dataset, this looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv_data &lt;-<span class="st"> </span><span class="kw">loo_cv</span>(Auto)
loocv_data</code></pre></div>
<pre><code>## # Leave-one-out cross-validation 
## # A tibble: 392 x 2
##    splits          id        
##    &lt;list&gt;          &lt;chr&gt;     
##  1 &lt;split [391/1]&gt; Resample1 
##  2 &lt;split [391/1]&gt; Resample2 
##  3 &lt;split [391/1]&gt; Resample3 
##  4 &lt;split [391/1]&gt; Resample4 
##  5 &lt;split [391/1]&gt; Resample5 
##  6 &lt;split [391/1]&gt; Resample6 
##  7 &lt;split [391/1]&gt; Resample7 
##  8 &lt;split [391/1]&gt; Resample8 
##  9 &lt;split [391/1]&gt; Resample9 
## 10 &lt;split [391/1]&gt; Resample10
## # … with 382 more rows</code></pre>
<p>Each element of <code>loocv_data$splits</code> is an object of class <code>rsplit</code>. This is essentially an efficient container for storing both the <strong>analysis</strong> data (i.e. the training data set) and the <strong>assessment</strong> data (i.e. the validation data set). If we print the contents of a single <code>rsplit</code> object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">first_resample &lt;-<span class="st"> </span>loocv_data<span class="op">$</span>splits[[<span class="dv">1</span>]]
first_resample</code></pre></div>
<pre><code>## &lt;391/1/392&gt;</code></pre>
<p>This tells us there are 391 observations in the analysis set, 1 observation in the assessment set, and the original data set contained 392 observations. To extract the analysis/assessment sets, use <code>analysis()</code> or <code>assessment()</code> respectively:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">training</span>(first_resample)</code></pre></div>
<pre><code>## # A tibble: 391 x 9
##      mpg cylinders displacement horsepower weight acceleration  year origin
##    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1    18         8          307        130   3504         12      70      1
##  2    15         8          350        165   3693         11.5    70      1
##  3    18         8          318        150   3436         11      70      1
##  4    16         8          304        150   3433         12      70      1
##  5    17         8          302        140   3449         10.5    70      1
##  6    15         8          429        198   4341         10      70      1
##  7    14         8          454        220   4354          9      70      1
##  8    14         8          440        215   4312          8.5    70      1
##  9    14         8          455        225   4425         10      70      1
## 10    15         8          390        190   3850          8.5    70      1
## # … with 381 more rows, and 1 more variable: name &lt;fct&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">assessment</span>(first_resample)</code></pre></div>
<pre><code>## # A tibble: 1 x 9
##     mpg cylinders displacement horsepower weight acceleration  year origin
##   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1    25         4          113         95   2228           14    71      3
## # … with 1 more variable: name &lt;fct&gt;</code></pre>
<p>Given this new <code>loocv_data</code> data frame, we write a function that will, for each resample:</p>
<ol style="list-style-type: decimal">
<li>Obtain the analysis data set (i.e. the <span class="math inline">\(N-1\)</span> training set)</li>
<li>Fit a linear regression model</li>
<li>Predict the validation data (also known as the <strong>assessment</strong> data, the <span class="math inline">\(1\)</span> validation set) using the <code>broom</code> package</li>
<li>Determine the MSE for each sample</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">holdout_results &lt;-<span class="st"> </span><span class="cf">function</span>(splits) {
  <span class="co"># Fit the model to the N-1</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> <span class="kw">analysis</span>(splits))
  
  <span class="co"># `augment` will save the predictions with the holdout data set</span>
  res &lt;-<span class="st"> </span><span class="kw">augment</span>(mod, <span class="dt">newdata =</span> <span class="kw">assessment</span>(splits)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="co"># calculate the metric</span>
<span class="st">    </span><span class="kw">mse</span>(<span class="dt">truth =</span> mpg, <span class="dt">estimate =</span> .fitted)
  
  <span class="co"># Return the metrics</span>
  res
}</code></pre></div>
<p>This function works for a single resample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">holdout_results</span>(loocv_data<span class="op">$</span>splits[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 mse     standard     0.00355</code></pre>
<p>To compute the MSE for each heldout observation (i.e. estimate the test MSE for each of the <span class="math inline">\(N\)</span> observations), we use the <code>map()</code> function from the <code>purrr</code> package to estimate the model for each training set, then calculate the MSE for each observation in each validation set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv_data_poly1 &lt;-<span class="st"> </span>loocv_data <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">results =</span> <span class="kw">map</span>(splits, holdout_results)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(results) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread</span>(.metric, .estimate)
loocv_data_poly1</code></pre></div>
<pre><code>## # A tibble: 392 x 4
##    splits          id         .estimator      mse
##    &lt;list&gt;          &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;
##  1 &lt;split [391/1]&gt; Resample1  standard    0.00355
##  2 &lt;split [391/1]&gt; Resample2  standard    1.25   
##  3 &lt;split [391/1]&gt; Resample3  standard   19.6    
##  4 &lt;split [391/1]&gt; Resample4  standard    2.42   
##  5 &lt;split [391/1]&gt; Resample5  standard   16.7    
##  6 &lt;split [391/1]&gt; Resample6  standard   97.0    
##  7 &lt;split [391/1]&gt; Resample7  standard   57.7    
##  8 &lt;split [391/1]&gt; Resample8  standard    1.77   
##  9 &lt;split [391/1]&gt; Resample9  standard   15.3    
## 10 &lt;split [391/1]&gt; Resample10 standard   24.2    
## # … with 382 more rows</code></pre>
<p>Now we can compute the overall LOOCV MSE for the data set by calculating the mean of the <code>mse</code> column:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv_data_poly1 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mse =</span> <span class="kw">mean</span>(mse))</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##     mse
##   &lt;dbl&gt;
## 1  24.2</code></pre>
<p>We can also use this method to compare the optimal number of polynomial terms as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># modified function to estimate model with varying highest order polynomial</span>
holdout_results &lt;-<span class="st"> </span><span class="cf">function</span>(splits, i) {
  <span class="co"># Fit the model to the N-1</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i, <span class="dt">raw =</span> <span class="ot">TRUE</span>), <span class="dt">data =</span> <span class="kw">analysis</span>(splits))
  
  <span class="co"># `augment` will save the predictions with the holdout data set</span>
  res &lt;-<span class="st"> </span><span class="kw">augment</span>(mod, <span class="dt">newdata =</span> <span class="kw">assessment</span>(splits)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="co"># calculate the metric</span>
<span class="st">    </span><span class="kw">mse</span>(<span class="dt">truth =</span> mpg, <span class="dt">estimate =</span> .fitted)
  
  <span class="co"># Return the assessment data set with the additional columns</span>
  res
}

<span class="co"># function to return MSE for a specific higher-order polynomial term</span>
poly_mse &lt;-<span class="st"> </span><span class="cf">function</span>(i, loocv_data){
  loocv_mod &lt;-<span class="st"> </span>loocv_data <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">results =</span> <span class="kw">map</span>(splits, holdout_results, i)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">unnest</span>(results) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread</span>(.metric, .estimate)
  
  <span class="kw">mean</span>(loocv_mod<span class="op">$</span>mse)
}

cv_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">terms =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">5</span>),
                 <span class="dt">mse_loocv =</span> <span class="kw">map_dbl</span>(terms, poly_mse, loocv_data))
cv_mse</code></pre></div>
<pre><code>## # A tibble: 5 x 2
##   terms mse_loocv
##   &lt;int&gt;     &lt;dbl&gt;
## 1     1      24.2
## 2     2      19.2
## 3     3      19.3
## 4     4      19.4
## 5     5      19.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(cv_mse, <span class="kw">aes</span>(terms, mse_loocv)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using LOOCV&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/loocv_poly-1.png" width="672" /></p>
<p>And arrive at a similar conclusion. There may be a very marginal advantage to adding a fifth-order polynomial, but not substantial enough for the additional complexity over a mere second-order polynomial.</p>
</div>
<div id="loocv-in-classification" class="section level2">
<h2><span class="header-section-number">3.2</span> LOOCV in classification</h2>
<p>Let’s verify the error rate of our interactive terms model for the Titanic data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to generate assessment statistics for titanic model</span>
holdout_results &lt;-<span class="st"> </span><span class="cf">function</span>(splits) {
  <span class="co"># Fit the model to the N-1</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> <span class="kw">analysis</span>(splits),
             <span class="dt">family =</span> binomial)
  
  <span class="co"># `augment` will save the predictions with the holdout data set</span>
  res &lt;-<span class="st"> </span><span class="kw">augment</span>(mod, <span class="dt">newdata =</span> <span class="kw">assessment</span>(splits)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">.prob =</span> <span class="kw">logit2prob</span>(.fitted),
           <span class="dt">.pred =</span> <span class="kw">round</span>(.prob))
  
  <span class="co"># Return the assessment data set with the additional columns</span>
  res
}

titanic_loocv &lt;-<span class="st"> </span><span class="kw">loo_cv</span>(titanic) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">results =</span> <span class="kw">map</span>(splits, holdout_results)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(results) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.pred =</span> <span class="kw">factor</span>(.pred)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">accuracy</span>(<span class="dt">truth =</span> Survived, <span class="dt">estimate =</span> .pred)

<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(titanic_loocv<span class="op">$</span>.estimate, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.219888</code></pre>
<p>In a classification problem, the LOOCV tells us the average error rate based on our predictions. So here, it tells us that the interactive <code>Age * Sex</code> model has a 22% error rate. This is similar to the validation set result (21.7%).</p>
</div>
</div>
<div id="k-fold-cross-validation" class="section level1">
<h1><span class="header-section-number">4</span> <span class="math inline">\(K\)</span>-fold cross-validation</h1>
<p>A less computationally-intensive approach to cross validation is <strong><span class="math inline">\(K\)</span>-fold cross-validation</strong>. Rather than dividing the data into <span class="math inline">\(N\)</span> groups, one divides the observations into <span class="math inline">\(K\)</span> groups, or <strong>folds</strong>, of approximately equal size. The first fold is treated as the validation set, and the model is estimated on the remaining <span class="math inline">\(K-1\)</span> folds. This process is repeated <span class="math inline">\(K\)</span> times, with each fold serving as the validation set precisely once. The <span class="math inline">\(K\)</span>-fold CV estimate is calculated by averaging the MSE values for each fold:</p>
<p><span class="math display">\[CV_{(K)} = \frac{1}{K} \sum_{i = 1}^{K}{MSE_i}\]</span></p>
<p>As you may have noticed, LOOCV is a special case of <span class="math inline">\(K\)</span>-fold cross-validation where <span class="math inline">\(K = N\)</span>. More typically researchers will use <span class="math inline">\(K=5\)</span> or <span class="math inline">\(K=10\)</span> depending on the size of the data set and the complexity of the statistical model.</p>
<div id="k-fold-cv-in-linear-regression" class="section level2">
<h2><span class="header-section-number">4.1</span> <span class="math inline">\(K\)</span>-fold CV in linear regression</h2>
<p>Let’s go back to the <code>Auto</code> data set. Instead of LOOCV, let’s use 10-fold CV to compare the different polynomial models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># modified function to estimate model with varying highest order polynomial</span>
holdout_results &lt;-<span class="st"> </span><span class="cf">function</span>(splits, i) {
  <span class="co"># Fit the model to the training set</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, i, <span class="dt">raw =</span> <span class="ot">TRUE</span>), <span class="dt">data =</span> <span class="kw">analysis</span>(splits))
  
  <span class="co"># Save the heldout observations</span>
  holdout &lt;-<span class="st"> </span><span class="kw">assessment</span>(splits)
  
  <span class="co"># `augment` will save the predictions with the holdout data set</span>
  res &lt;-<span class="st"> </span><span class="kw">augment</span>(mod, <span class="dt">newdata =</span> holdout) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="co"># calculate the metric</span>
<span class="st">    </span><span class="kw">mse</span>(<span class="dt">truth =</span> mpg, <span class="dt">estimate =</span> .fitted)
  
  <span class="co"># Return the assessment data set with the additional columns</span>
  res
}

<span class="co"># function to return MSE for a specific higher-order polynomial term</span>
poly_mse &lt;-<span class="st"> </span><span class="cf">function</span>(i, vfold_data){
  vfold_mod &lt;-<span class="st"> </span>vfold_data <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">results =</span> <span class="kw">map</span>(splits, holdout_results, i)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">unnest</span>(results) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread</span>(.metric, .estimate)
  
  <span class="kw">mean</span>(vfold_mod<span class="op">$</span>mse)
}

<span class="co"># split Auto into 10 folds</span>
auto_cv10 &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(<span class="dt">data =</span> Auto, <span class="dt">v =</span> <span class="dv">10</span>)

cv_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">terms =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">5</span>),
                     <span class="dt">mse_vfold =</span> <span class="kw">map_dbl</span>(terms, poly_mse, auto_cv10))
cv_mse</code></pre></div>
<pre><code>## # A tibble: 5 x 2
##   terms mse_vfold
##   &lt;int&gt;     &lt;dbl&gt;
## 1     1      24.2
## 2     2      19.3
## 3     3      19.4
## 4     4      19.6
## 5     5      19.3</code></pre>
<p>How do these results compare to the LOOCV values?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_loocv &lt;-<span class="st"> </span><span class="kw">loo_cv</span>(Auto)

<span class="kw">tibble</span>(<span class="dt">terms =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">5</span>),
       <span class="st">`</span><span class="dt">10-fold</span><span class="st">`</span> =<span class="st"> </span><span class="kw">map_dbl</span>(terms, poly_mse, auto_cv10),
       <span class="dt">LOOCV =</span> <span class="kw">map_dbl</span>(terms, poly_mse, auto_loocv)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(method, MSE, <span class="op">-</span>terms) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> method)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;MSE estimates&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;CV Method&quot;</span>)</code></pre></div>
<p><img src="/notes/cross-validation_files/figure-html/10_fold_auto_loocv-1.png" width="672" /></p>
<p>Pretty much the same results.</p>
</div>
<div id="computational-speed-of-loocv-vs.-k-fold-cv" class="section level2">
<h2><span class="header-section-number">4.2</span> Computational speed of LOOCV vs. <span class="math inline">\(K\)</span>-fold CV</h2>
<div id="loocv" class="section level3">
<h3><span class="header-section-number">4.2.1</span> LOOCV</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tictoc)

<span class="kw">tic</span>()
<span class="kw">poly_mse</span>(<span class="dt">vfold_data =</span> auto_loocv, <span class="dt">i =</span> <span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 19.24821</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">toc</span>()</code></pre></div>
<pre><code>## 5.684 sec elapsed</code></pre>
</div>
<div id="fold-cv" class="section level3">
<h3><span class="header-section-number">4.2.2</span> 10-fold CV</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tic</span>()
<span class="kw">poly_mse</span>(<span class="dt">vfold_data =</span> auto_cv10, <span class="dt">i =</span> <span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 19.26807</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">toc</span>()</code></pre></div>
<pre><code>## 0.345 sec elapsed</code></pre>
<p>On my machine, 10-fold CV was about 40 times faster than LOOCV. Again, estimating <span class="math inline">\(K=10\)</span> models is going to be much easier than estimating <span class="math inline">\(K=392\)</span> models.</p>
</div>
</div>
<div id="k-fold-cv-in-logistic-regression" class="section level2">
<h2><span class="header-section-number">4.3</span> <span class="math inline">\(K\)</span>-fold CV in logistic regression</h2>
<p>You’ve gotten the idea by now, but let’s do it one more time on our interactive Titanic model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to generate assessment statistics for titanic model</span>
holdout_results &lt;-<span class="st"> </span><span class="cf">function</span>(splits) {
  <span class="co"># Fit the model to the training set</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> <span class="kw">analysis</span>(splits),
             <span class="dt">family =</span> binomial)
  
  <span class="co"># `augment` will save the predictions with the holdout data set</span>
  res &lt;-<span class="st"> </span><span class="kw">augment</span>(mod, <span class="dt">newdata =</span> <span class="kw">assessment</span>(splits)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">.prob =</span> <span class="kw">logit2prob</span>(.fitted),
           <span class="dt">.pred =</span> <span class="kw">round</span>(.prob))

  <span class="co"># Return the assessment data set with the additional columns</span>
  res
}

titanic_cv10 &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(<span class="dt">data =</span> titanic, <span class="dt">v =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">results =</span> <span class="kw">map</span>(splits, holdout_results)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(results) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.pred =</span> <span class="kw">factor</span>(.pred)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">accuracy</span>(<span class="dt">truth =</span> Survived, <span class="dt">estimate =</span> .pred)

<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(titanic_cv10<span class="op">$</span>.estimate, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.2200643</code></pre>
<p>Not a large difference from the LOOCV approach, but it take much less time to compute.</p>
</div>
</div>
<div id="appropriate-value-for-k" class="section level1">
<h1><span class="header-section-number">5</span> Appropriate value for <span class="math inline">\(K\)</span></h1>
<p>Ignoring the computational efficiency concerns, why not always estimate cross-validation with <span class="math inline">\(K=N\)</span>? Or more generally, what is the optimal value for <span class="math inline">\(K\)</span>? <strong>It depends.</strong> Well that is not very helpful.</p>
<p>With more explanation, it depends on how we wish to handle the bias-variance tradeoff. LOOCV is a low-bias, high-variance method. That is, it provides unbiased estimates of the test error since each training set contains <span class="math inline">\(N-1\)</span> observations. This is almost as many observations as contained in the full data set. <span class="math inline">\(K\)</span>-fold CV for <span class="math inline">\(K=5\)</span> or <span class="math inline">\(10\)</span> leads to an intermediate amount of bias, since each training set contains <span class="math inline">\(\frac{(K-1)N}{K}\)</span> observations. This is fewer than LOOCV, but more than a standard validation set approach with just a single split into training and validation sets.</p>
<p>The amount of bias is also driven by the size of the training set. The larger the training set, the less bias we should expect in our results because the model was fit to more data points. Consider a hypothetical learning curve for a classifier:</p>
<p><img src="/notes/cross-validation_files/figure-html/hypo-class-1.png" width="672" /></p>
<p>As the training set size increases, <span class="math inline">\(1 - \text{Expected test error}\)</span> increases as well with diminishing returns. With 200 observations, <span class="math inline">\(5\)</span>-fold cross-validation would use training sets of 160 observations, which is fairly similar to the full set. However with a dataset of 50, <span class="math inline">\(5\)</span>-fold cross-validation would use training sets of 40 observations which has more error. Therefore the larger the data set, the fewer folds you can implement.</p>
<p>If all we care about is bias, we should prefer LOOCV. However, recall the contributors to a model’s error:</p>
<p><span class="math display">\[\text{Error} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}\]</span></p>
<p>We also should be concerned with the <strong>variance</strong> of the model. LOOCV has a higher variance than <span class="math inline">\(K\)</span>-fold with <span class="math inline">\(K &lt; N\)</span>. When we perform LOOCV, we are averaging the outputs of <span class="math inline">\(N\)</span> fitted models which are trained on nearly entirely identical sets of observations. The results will be highly correlated with one another. In contrast, <span class="math inline">\(K\)</span>-fold CV with <span class="math inline">\(K &lt; N\)</span> averages the output of <span class="math inline">\(K\)</span> fitted models that are less correlated with one another, since the data sets are not as identical. Since the mean of many highly correlated quantities has higher variance than the mean of quantities with less correlation, the test error estimate from LOOCV has higher variance than the test error estimate from <span class="math inline">\(K\)</span>-fold CV.</p>
<p>Given these considerations, a typical approach uses <span class="math inline">\(K=5\)</span> or <span class="math inline">\(K=10\)</span>. Empirical research (see <span class="citation">Breiman and Spector (<a href="#ref-breiman1992submodel">1992</a>)</span>, <span class="citation">Kohavi and others (<a href="#ref-kohavi1995study">1995</a>)</span>) shows that cross-validation with these number of folds suffers neither excessively high bias nor excessively high variance.</p>
</div>
<div id="variations-on-cross-validation" class="section level1">
<h1><span class="header-section-number">6</span> Variations on cross-validation</h1>
<p>To ensure each set is approximately similar to one another in every important aspect, we use random sampling without replacement to partition the data set. Alternative approaches include:</p>
<ul>
<li>Stratified cross-validation - splitting the data into folds based on criteria such as ensuring each fold has the same proportion of observations with a given categorical value (such as the response class value). While random sampling should lead to approximately equal class balances, stratified sampling will ensure an equal balance across folds.</li>
<li>Repeated cross-validation - this is where <span class="math inline">\(K\)</span>-fold CV is repeated <span class="math inline">\(N\)</span> times, where for each <span class="math inline">\(N\)</span> the data sample is shuffled prior to each repetition. This ensures a different split of the sample.</li>
<li><p>Cross-validation with time series data - <span class="citation">Bergmeir and Benítez (<a href="#ref-bergmeir2012use">2012</a>)</span> evaluate multiple forms of cross-validation methods for time series models. For some types of models, standard cross-validation techniques can be employed without bias. In other situations, a standard approach is to partition the data temporally. For instance, if you have 10 years of observations for a given unit you may reserve the first 8 years for the training set and the last 2 years for the validation set. Other forms of cross-validation use <strong>rolling validation sets</strong> whereby one generates a series of validation sets, each containing a single observation. The training set consists only of observations that occurred prior to the observation that forms the validation set.</p>
<div class="figure">
<img src="https://robjhyndman.com/files/cv1-1.png" alt="From Cross-validation for time series" />
<p class="caption">From <a href="https://robjhyndman.com/hyndsight/tscv/">Cross-validation for time series</a></p>
</div></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">7</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-01-28                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package       * version    date       lib
##  assertthat      0.2.0      2017-04-11 [2]
##  backports       1.1.3      2018-12-14 [2]
##  base64enc       0.1-3      2015-07-28 [2]
##  bayesplot       1.6.0      2018-08-02 [2]
##  bindr           0.1.1      2018-03-13 [2]
##  bindrcpp      * 0.2.2      2018-03-29 [1]
##  blogdown        0.9.4      2018-11-26 [1]
##  bookdown        0.9        2018-12-21 [1]
##  broom         * 0.5.1      2018-12-05 [2]
##  callr           3.1.1      2018-12-21 [2]
##  cellranger      1.1.0      2016-07-27 [2]
##  class           7.3-15     2019-01-01 [2]
##  cli             1.0.1      2018-09-25 [1]
##  codetools       0.2-16     2018-12-24 [2]
##  colorspace      1.3-2      2016-12-14 [2]
##  colourpicker    1.0        2017-09-27 [2]
##  crayon          1.3.4      2017-09-16 [2]
##  crosstalk       1.0.0      2016-12-21 [2]
##  desc            1.2.0      2018-05-01 [2]
##  devtools        2.0.1      2018-10-26 [1]
##  dials         * 0.0.2      2018-12-09 [1]
##  digest          0.6.18     2018-10-10 [1]
##  dplyr         * 0.7.8      2018-11-10 [1]
##  DT              0.5        2018-11-05 [2]
##  dygraphs        1.1.1.6    2018-07-11 [2]
##  evaluate        0.12       2018-10-09 [2]
##  forcats       * 0.3.0      2018-02-19 [2]
##  fs              1.2.6      2018-08-23 [1]
##  generics        0.0.2      2018-11-29 [1]
##  ggplot2       * 3.1.0      2018-10-25 [1]
##  ggridges        0.5.1      2018-09-27 [2]
##  glue            1.3.0      2018-07-17 [2]
##  gower           0.1.2      2017-02-23 [2]
##  gridExtra       2.3        2017-09-09 [2]
##  gtable          0.2.0      2016-02-26 [2]
##  gtools          3.8.1      2018-06-26 [2]
##  haven           2.0.0      2018-11-22 [2]
##  here          * 0.1        2017-05-28 [2]
##  hms             0.4.2      2018-03-10 [2]
##  htmltools       0.3.6      2017-04-28 [1]
##  htmlwidgets     1.3        2018-09-30 [2]
##  httpuv          1.4.5.1    2018-12-18 [2]
##  httr            1.4.0      2018-12-11 [2]
##  igraph          1.2.2      2018-07-27 [2]
##  infer         * 0.4.0      2018-11-15 [1]
##  inline          0.3.15     2018-05-18 [2]
##  ipred           0.9-8      2018-11-05 [1]
##  ISLR          * 1.2        2017-10-20 [2]
##  janeaustenr     0.1.5      2017-06-10 [2]
##  jsonlite        1.6        2018-12-07 [2]
##  knitr           1.21       2018-12-10 [2]
##  labeling        0.3        2014-08-23 [2]
##  later           0.7.5      2018-09-18 [2]
##  lattice         0.20-38    2018-11-04 [2]
##  lava            1.6.4      2018-11-25 [2]
##  lazyeval        0.2.1      2017-10-29 [2]
##  lme4            1.1-19     2018-11-10 [2]
##  loo             2.0.0      2018-04-11 [2]
##  lubridate       1.7.4      2018-04-11 [2]
##  magrittr      * 1.5        2014-11-22 [2]
##  markdown        0.9        2018-12-07 [2]
##  MASS            7.3-51.1   2018-11-01 [2]
##  Matrix          1.2-15     2018-11-01 [2]
##  matrixStats     0.54.0     2018-07-23 [2]
##  memoise         1.1.0      2017-04-21 [2]
##  mime            0.6        2018-10-05 [1]
##  miniUI          0.1.1.1    2018-05-18 [2]
##  minqa           1.2.4      2014-10-09 [2]
##  modelr          0.1.2      2018-05-11 [2]
##  munsell         0.5.0      2018-06-12 [2]
##  nlme            3.1-137    2018-04-07 [2]
##  nloptr          1.2.1      2018-10-03 [2]
##  nnet            7.3-12     2016-02-02 [2]
##  parsnip       * 0.0.1      2018-11-12 [1]
##  pillar          1.3.1      2018-12-15 [2]
##  pkgbuild        1.0.2      2018-10-16 [1]
##  pkgconfig       2.0.2      2018-08-16 [2]
##  pkgload         1.0.2      2018-10-29 [1]
##  plyr            1.8.4      2016-06-08 [2]
##  prettyunits     1.0.2      2015-07-13 [2]
##  pROC            1.13.0     2018-09-24 [1]
##  processx        3.2.1      2018-12-05 [2]
##  prodlim         2018.04.18 2018-04-18 [2]
##  profvis       * 0.3.5      2018-02-22 [2]
##  promises        1.0.1      2018-04-13 [2]
##  ps              1.3.0      2018-12-21 [2]
##  purrr         * 0.2.5      2018-05-29 [2]
##  R6              2.3.0      2018-10-04 [1]
##  rcfss         * 0.1.5      2019-01-24 [1]
##  Rcpp            1.0.0      2018-11-07 [1]
##  readr         * 1.3.1      2018-12-21 [2]
##  readxl          1.2.0      2018-12-19 [2]
##  recipes       * 0.1.4      2018-11-19 [1]
##  remotes         2.0.2      2018-10-30 [1]
##  reshape2        1.4.3      2017-12-11 [2]
##  rlang           0.3.0.1    2018-10-25 [1]
##  rmarkdown       1.11       2018-12-08 [2]
##  rpart           4.1-13     2018-02-23 [1]
##  rprojroot       1.3-2      2018-01-03 [2]
##  rsample       * 0.0.3      2018-11-20 [1]
##  rsconnect       0.8.12     2018-12-05 [2]
##  rstan           2.18.2     2018-11-07 [2]
##  rstanarm        2.18.2     2018-11-10 [2]
##  rstantools      1.5.1      2018-08-22 [2]
##  rstudioapi      0.8        2018-10-02 [1]
##  rvest           0.3.2      2016-06-17 [2]
##  scales        * 1.0.0      2018-08-09 [1]
##  sessioninfo     1.1.1      2018-11-05 [1]
##  shiny           1.2.0      2018-11-02 [2]
##  shinyjs         1.0        2018-01-08 [2]
##  shinystan       2.5.0      2018-05-01 [2]
##  shinythemes     1.1.2      2018-11-06 [2]
##  SnowballC       0.5.1      2014-08-09 [2]
##  StanHeaders     2.18.0-1   2018-12-13 [2]
##  stringi         1.2.4      2018-07-20 [2]
##  stringr       * 1.3.1      2018-05-10 [2]
##  survival        2.43-3     2018-11-26 [2]
##  testthat        2.0.1      2018-10-13 [2]
##  threejs         0.3.1      2017-08-13 [2]
##  tibble        * 2.0.0      2019-01-04 [2]
##  tictoc        * 1.0        2014-06-17 [1]
##  tidymodels    * 0.0.2      2018-11-27 [1]
##  tidyposterior   0.0.2      2018-11-15 [1]
##  tidypredict     0.2.1      2018-12-20 [1]
##  tidyr         * 0.8.2      2018-10-28 [2]
##  tidyselect      0.2.5      2018-10-11 [1]
##  tidytext        0.2.0      2018-10-17 [1]
##  tidyverse     * 1.2.1      2017-11-14 [2]
##  timeDate        3043.102   2018-02-21 [2]
##  titanic       * 0.1.0      2015-08-31 [2]
##  tokenizers      0.2.1      2018-03-29 [2]
##  usethis         1.4.0      2018-08-14 [1]
##  withr           2.1.2      2018-03-15 [2]
##  xfun            0.4        2018-10-23 [1]
##  xml2            1.2.0      2018-01-24 [2]
##  xtable          1.8-3      2018-08-29 [2]
##  xts             0.11-2     2018-11-05 [2]
##  yaml            2.2.0      2018-07-25 [2]
##  yardstick     * 0.0.2      2018-11-05 [1]
##  zoo             1.8-4      2018-09-19 [2]
##  source                           
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  Github (rstudio/blogdown@b2e1ed4)
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.1)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.1)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.1)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.1)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  local                            
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.2)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.1)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
##  CRAN (R 3.5.0)                   
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">8</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-bergmeir2012use">
<p>Bergmeir, Christoph, and José M Benítez. 2012. “On the Use of Cross-Validation for Time Series Predictor Evaluation.” <em>Information Sciences</em> 191. Elsevier: 192–213.</p>
</div>
<div id="ref-breiman1992submodel">
<p>Breiman, Leo, and Philip Spector. 1992. “Submodel Selection and Evaluation in Regression. the X-Random Case.” <em>International Statistical Review/Revue Internationale de Statistique</em>. JSTOR, 291–319.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
<div id="ref-kohavi1995study">
<p>Kohavi, Ron, and others. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” In <em>Ijcai</em>, 14:1137–45. 2. Montreal, Canada.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For educational purposes, here we will omit the test set. In a real-world situation, we would first partition out a test set of data.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The actual value you use is irrelevant. Just be sure to set it in the script, otherwise R will randomly pick one each time you start a new session.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The default <code>family</code> for <code>glm()</code> is <code>gaussian()</code>, or the <strong>Gaussian</strong> distribution. You probably know it by its other name, the <a href="https://en.wikipedia.org/wiki/Normal_distribution"><strong>Normal</strong> distribution</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Note that most other metric functions can be found in the <a href="https://tidymodels.github.io/yardstick/"><code>yardstick</code></a> package. However <code>yardstick</code> only implements root mean squared error (RMSE), so I wrote a function in <code>rcfss</code> to calculate MSE. Otherwise you could just calculate RMSE and square the resulting value.<a href="#fnref4">↩</a></p></li>
</ol>
</div>
