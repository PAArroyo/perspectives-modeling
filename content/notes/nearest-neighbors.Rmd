---
title: Nearest Neighbors
date: 2019-01-23T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Classification
    weight: 4
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(broom)
library(rsample)
library(knitr)
library(patchwork)
library(FNN)
library(kknn)
library(titanic)
library(rcfss)
library(pROC)
library(here)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}

# Naive non-parametric regression

Suppose we have detailed information wages and education. We don't have data for the entire population, but we do have observations for one million employed Americans:

```{r np-data}
n <- 1e06
wage <- tibble(educ = rpois(n, lambda = 12),
               age = rpois(n, lambda = 40),
               prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

ggplot(wage, aes(wage)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Histogram of simulated income data",
       subtitle = "Binwidth = 5",
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

If we want to estimate the income for an individual given their education level $(0, 1, 2, \dots, 25)$, we could estimate the conditional distribution of income for each of these values:

$$\mu = \E(\text{Income}|\text{Education}) = f(\text{Education})$$

For each level of education, the conditional (or expected) income would be the mean or median of all individuals in the sample with the same level of education.

```{r np-wage-cond, dependson = "np-data"}
wage %>%
  group_by(educ) %>%
  summarize(mean = mean(wage),
            sd = sd(wage)) %>%
  ggplot(aes(educ, mean, ymin = mean - sd, ymax = mean + sd)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "Conditional income, by education level",
       subtitle = "Plus/minus SD",
       x = "Education level",
       y = "Income, in thousands of dollars")

wage %>%
  filter(educ == 12) %>%
  ggplot(aes(wage)) +
  geom_density() +
  geom_vline(xintercept = mean(wage$wage[wage$educ == 12]), linetype = 2) +
  labs(title = "Conditional distribution of income for education = 12",
       subtitle = str_c("Mean income = ", formatC(mean(wage$wage[wage$educ == 12]), digits = 3)),
       x = "Income, in thousands of dollars",
       y = "Frequency count")
```

Imagine instead that we we have $X$ and $Y$, two continuous variables from a sample of a population, and we want to understand the relationship between the variables. Specifically, we want to use our knowledge of $X$ to predict $Y$. Therefore what we want to know is the mean value of $Y$ as a function of $X$ in the population of individuals from whom the sample was drawn:

$$\mu = \E(Y|x) = f(x)$$

Unfortunately because $X$ is continuous, it is unlikely that we would draw precisely the same values of $X$ for more than a single observation. Therefore we cannot directly calculate the conditional distribution of $Y$ given $X$, and therefore cannot calculate conditional means. Instead, we can divide $X$ into many narrow intervals (or **bins**), just like we would for a histogram. Within each bin we can estimate the conditional distribution of $Y$ and estimate the conditional mean of $Y$ with great precision.

If we have fewer observations, then we have to settle for fewer bins and less precision in our estimates. Here we use data on the average income of 102 different occupations in Canada and their relationship to occupational prestige (measured continuously):

```{r prestige}
# get data
prestige <- here("static", "data", "prestige.csv") %>%
  read_csv()
```

```{r prestige-5bins, dependson="prestige"}
# bin into 5 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 6)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
               as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  enframe(name = NULL)

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 5",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

The $X$-axis is carved into 5 bins with roughly 20 observations in each bin. The line is a **naive nonparametric regression line** that is calculated by connecting the points defined by the conditional variable means $\bar{Y}$ and the explanatory variable means $\bar{X}$ in the five intervals.

Just like ordinary least squares regression (OLS), this regression line also suffers from **bias** and **variance**. If the actual relationship between prestige and income is non-linear **within a bin**, then our estimate of the conditional mean $\bar{Y}$ will be biased towards a linear relationship. We can minimize bias by making the bins as numerous and narrow as possible:

```{r prestige-50bins, dependson="prestige"}
# bin into 50 and get means
prestige_bin <- prestige %>%
  mutate(bin = cut_number(income, 51)) %>%
  group_by(bin) %>%
  summarize(prestige = mean(prestige),
            income = mean(income))

# get cutpoints
labs <- levels(prestige_bin$bin)
cutpoints <- c(as.numeric( sub("\\((.+),.*", "\\1", labs) ),
               as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", labs) )) %>%
  unique %>%
  sort %>%
  .[2:(length(.)-1)] %>%
  enframe(name = NULL)

ggplot(prestige, aes(income, prestige)) +
  geom_point(shape = 1) +
  geom_line(data = prestige_bin) +
  geom_point(data = prestige_bin) +
  geom_vline(data = cutpoints, aes(xintercept = value), linetype = 2, alpha = .25) +
  labs(title = "Naive nonparametric regression",
       subtitle = "Bins = 50",
       x = "Average income (in dollars)",
       y = "Occupational prestige")
```

But now we have introduced overfitting into the nonparametric regression estimates. In addition, we substantially increased our variance of the estimated conditional sample means $\bar{Y}$. If we were to draw a new sample, the estimated conditional sample means $\bar{Y}$ could be widely different from the original model and our resulting estimates of the conditional sample means will be highly variable.

Naive nonparametric regression is a consistent estimator of the population regression curve as the sample size increases. As $n \rightarrow \infty$, we can shrink the size of the individual intervals and still have sizeable numbers of observations in each interval. In the limit, we have an infinite number of intervals and infinite number of observations in each interval, so the naive nonparametric regression line and the population regression line are identical.

As a practical consideration, if our sample size $n$ is truly large, then naive nonparametric regression could be a good estimation procedure. However as we introduce multiple explanatory variables into the model, the problem starts to blow up. Assume we have three discrete explanatory variables each with 10 possible values:

$$X_1 \in \{1, 2, \dots ,10 \}$$
$$X_2 \in \{1, 2, \dots ,10 \}$$
$$X_3 \in \{1, 2, \dots ,10 \}$$

There are then $10^3 = 1000$ possible combinations of the explanatory variables and $1000$ conditional expectations of $Y$ given $X$:

$$\mu = \E(Y|x_1, x_2, x_3) = f(x_1, x_2, x_3)$$

In order to accurate estimate conditional expectations for each category, we would need substantial numbers of observations **for every combination of $X$**. This would require a sample size far greater than most social scientists have the resources to collect.

Let's return to our simulated wage data. Our dataset contains information on education, age, and job prestige:

```{r wage-sim-describe}
ggplot(wage, aes(educ)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Education",
       y = "Frequency count")

ggplot(wage, aes(age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Age",
       y = "Frequency count")

ggplot(wage, aes(prestige)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of simulated wage data",
       x = "Job prestige",
       y = "Frequency count")
```

Can we estimate naive nonparametric regression on this dataset with $N = 1,000,000$?

```{r wage-sim-np}
wage_np <- wage %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                            wage_sd = NA,
                                            n = 0))

# number of unique combos 
wage_unique <- nrow(wage_np)

# n for each unique combo
ggplot(wage_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

Even on a dataset with $1,000,000$ observations, for the vast majority of the potential combinations of variables we have zero observations from which to generate expected values. What if we instead drew $10,000,000$ observations from the same data generating process?

```{r wage-sim-np-ten}
n <- 1e07
wage10 <- tibble(educ = rpois(n, lambda = 12),
                 age = rpois(n, lambda = 40),
                 prestige = rpois(n, lambda = 3)) %>%
  mutate(educ = ifelse(educ > 25, 25, educ),
         wage = 10 + 2 * educ + .5 * age + 5 * prestige + rnorm(n, 0, 3))

wage10_np <- wage10 %>%
  group_by(educ, age, prestige) %>%
  summarize(wage_mean = mean(wage),
            wage_sd = sd(wage),
            n = n()) %>%
  ungroup %>%
  complete(educ, age, prestige, fill = list(wage_mean = NA,
                                            wage_sd = NA,
                                            n = 0))

# number of unique combos 
wage10_unique <- nrow(wage10_np)

# n for each unique combo
ggplot(wage10_np, aes(n)) +
  geom_density() +
  labs(title = "Naive nonparametric regression of simulated wage data",
       x = "Number of observations for each unique combination",
       y = "Density")
```

Unless your dataset is extremely large or you have a small handful of variables with a low number of unique values, naive nonparametric estimation will not be effective.

# $K$-nearest neighbors regression

An alternative, but related, method is called **$K$-nearest neighbors regression** (KNN regression). Rather than binning the data into discrete and fixed intervals, KNN regression uses a moving average to generate the regression line. Given a value for $K$ and a prediction point $x_0$, KNN regression identifies the $K$ training observations nearest to the prediction point $x_0$, represented by $N_0$ and estimates $f(x_0)$ as the average of all the training responses in $N_0$:

$$\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in N_0} y_i$$

With $K=1$, the resulting KNN regression line will fit the training observations extraordinarily well.

```{r prestige-knn-1}
prestige_knn1 <- knn.reg(select(prestige, income), y = prestige$prestige,
                         test = select(prestige, income), k = 1)

prestige %>%
  mutate(pred = prestige_knn1$pred) %>%
  ggplot(aes(income, prestige)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "1-nearest neighbor regression",
       x = "Income (in dollars)",
       y = "Occupational prestige")
```

Perhaps a bit too well. Compare this to $K=9$:

```{r prestige-knn-9}
prestige_knn9 <- knn.reg(select(prestige, income), y = prestige$prestige,
                         test = select(prestige, income), k = 9)

prestige %>%
  mutate(pred = prestige_knn9$pred) %>%
  ggplot(aes(income, prestige)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "9-nearest neighbor regression",
       x = "Income (in dollars)",
       y = "Occupational prestige")
```

This regression line averages over the nine nearest observations; while still a step function, it is smoother than $K=1$. Small values for $K$ provide low bias estimates of the training observations but high variance. Large values for $K$ provide low variance but higher bias by masking some of the structure of $f(X)$.

## Linear regression vs. $K$-nearest neighbors

Parametric methods such as linear regression are superior to non-parametric methods such as KNN regression when the parametric approach accurately assumes the true functional form of $f$.

```{r np-p-line}
sim <- tibble(x = runif(100, -1,1),
              y = 2 + x + rnorm(100, 0, .2))

sim_knn9 <- knn.reg(select(sim, x), y = sim$y,
                    test = select(sim, x), k = 9)

sim %>%
  mutate(pred = sim_knn9$pred) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  geom_abline(aes(color = "True"), intercept = 2, slope = 1) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(color = "Method")
```

Here we simulate data from a linear relationship:

$$f(x) = 2 + x + \epsilon_i$$

The black line represents the true model, the blue line represents the linear regression model, and the red line represents the 9-nearest neighbor regression line. As we can see, the linear model does a great job approximating the true model because we have defined the relationship to be linear and that is what OLS attempts to estimate. The KNN line is too jumpy and contours too closely to the training data to capture the true relationship.

```{r np-p-line2}
# estimate test MSE for LM and KNN models
sim_test <- tibble(x = runif(100, -1,1),
                   y = 2 + x + rnorm(100, 0, .2))
mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- tibble(k = 1:10,
                  knn = map(k, ~ knn.reg(select(sim, x), y = sim$y,
                                         test = select(sim_test, x), k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

As $k$ increases, KNN regression does a better job approximating the linear relationship. Here we can see as $k$ increases, the test MSE (based on a separate draw of observations from the data generating process) shrinks and approaches the test MSE based on the linear model (the dashed line).

Of course as non-linearity in the true relationship increases, KNN will perform better relative to a parametric model which assumes linearity:

$$f(x) = 2 + x + x^2 + x^3 + \epsilon_i$$

```{r np-p-cubic}
x_cube <- function(x) {
  2 + x + x^2 + x^3
}

sim <- tibble(x = runif(100, -1,1),
              y = x_cube(x) + rnorm(100, 0, .2))

sim_knn9 <- knn.reg(select(sim, x), y = sim$y,
                    test = select(sim, x), k = 9)

sim %>%
  mutate(pred = sim_knn9$pred) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  stat_function(aes(color = "True"), fun = x_cube) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(color = "Method")

# estimate test MSE for LM and KNN models
sim_test <- tibble(x = runif(100, -1,1),
                   y = x_cube(x) + rnorm(100, 0, .2))

mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- tibble(k = 1:10,
                  knn = map(k, ~ knn.reg(select(sim, x), y = sim$y,
                                         test = select(sim_test, x), k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

While KNN performs better in the presence of a non-linear relationship, it also performs worse as the number of predictors $p$ increases. Here we use the same data generating process as before:

$$f(x) = 2 + x + x^2 + x^3 + \epsilon_i$$

But this time generate additional **noise parameters**, or variables that are not actually included in $f(x)$ but are included in $\hat{f}(x)$. Linear regression is more robust to the addition of such parameters and the test MSE increases more slowly as a result. Compare this to the change in test MSE for KNN regression:

```{r knn-nonrobust}
sim_nr <- tibble(x1 = runif(100, -1,1),
                 y = x_cube(x1) + rnorm(100, 0, .2),
                 x2 = rnorm(100, 0, 1),
                 x3 = rnorm(100, 0, 1),
                 x4 = rnorm(100, 0, 1),
                 x5 = rnorm(100, 0, 1),
                 x6 = rnorm(100, 0, 1))
sim_nr_test <- tibble(x1 = runif(100, -1,1),
                      y = x_cube(x1) + rnorm(100, 0, .2),
                      x2 = rnorm(100, 0, 1),
                      x3 = rnorm(100, 0, 1),
                      x4 = rnorm(100, 0, 1),
                      x5 = rnorm(100, 0, 1),
                      x6 = rnorm(100, 0, 1))

sim_pred_knn <- expand.grid(p = 1:6,
                            k = 1:10) %>%
  as_tibble %>%
  mutate(lm = map(p, ~ lm(formula(str_c("y ~ ", str_c("x", seq.int(.), collapse = " + "))),
                          data = sim_nr)),
         mse_lm = map_dbl(lm, ~ mse(., sim_nr_test)),
         knn = map2(p, k, ~ knn.reg(select_(sim_nr, .dots = str_c("x", seq.int(.x))),
                                    y = sim_nr$y,
                                    test = select_(sim_nr_test, .dots = str_c("x", seq.int(.x))),
                                    k = .y)),
         mse_knn = map_dbl(knn, ~ mean((sim_nr_test$y - .$pred)^2)))

ggplot(sim_pred_knn, aes(k, mse_knn)) +
  facet_grid(. ~ p, labeller = labeller(p = label_both)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = mse_lm), linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

We should be able to anticipate this problem. It is the same pitfall of naive nonparametric regression: as the number of predictors increases, the number of dimensions also increases. Spreading 100 observations over $p=6$ dimensions results in the problem that for many observations, there are no nearby neighbors. The closest neighbor may be extraordinarily far away in $p$-dimensional space, so the prediction resulting from averaging across these neighbors is poor.

## Weighted $K$-nearest neighbors

One alternative to this conundrum is to use a weighting function to weight our KNN estimates based on the **distance** of the nearby neighbors. For example, the `kknn` package weights KNN estimates based on the **Minkowski distance** between points:

$$\text{Distance}(x_i, y_i) = \left( \sum_{i = 1}^n |x_i - y_i| ^p \right)^\frac{1}{p}$$

where $p$ is the order parameter (not the number of predictors). $p=1$ results in **Manhattan distance**, while $p=2$ is known as **Euclidean distance**.

```{r knn-weight}
sim <- tibble(x = runif(100, -1,1),
              y = x_cube(x) + rnorm(100, 0, .2))

sim_wknn <- kknn(y ~ x, train = sim, test = sim, k = 5)

sim %>%
  mutate(pred = sim_wknn[["fitted.values"]]) %>%
  ggplot(aes(x, y)) +
  geom_point(shape = 1) +
  stat_function(aes(color = "True"), fun = x_cube) +
  geom_smooth(aes(color = "LM"), method = "lm", se = FALSE) +
  geom_step(aes(y = pred, color = "KNN")) +
  labs(title = "5-nearest neighbor regression",
       subtitle = "Euclidean distance weighting",
       color = "Method")

# estimate test MSE for LM and KNN models
sim_test <- tibble(x = runif(100, -1,1),
                   y = x_cube(x) + rnorm(100, 0, .2))

mse_lm <- lm(y ~ x, data = sim) %>%
  mse(sim_test)

mse_knn <- tibble(k = 1:10,
                  knn = map(k, ~ kknn(y ~ x, train = sim, test = sim_test, k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$fitted.values)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

Let's compare the robustness of KNN vs. weighted KNN using the previous example with random noise parameters.

```{r wknn-nonrobust}
sim_pred_wknn <- sim_pred_knn %>%
  mutate(wknn = map2(p, k, ~ kknn(formula(str_c("y ~ ",
                                                str_c("x", seq.int(.x), collapse = " + "))),
                                  train = sim_nr, test = sim_nr_test, k = .y)),
         mse_wknn = map_dbl(wknn, ~ mean((sim_nr_test$y - .$fitted.values)^2)))
sim_pred_lm <- sim_pred_wknn %>%
  select(p, k, mse_lm) %>%
  distinct

sim_pred_wknn %>%
  select(p, k, contains("mse"), -mse_lm) %>%
  gather(method, mse, contains("mse")) %>%
  mutate(method = str_replace(method, "mse_", "")) %>%
  mutate(method = factor(method, levels = c("knn", "wknn"),
                         labels = c("KNN", "Weighted KNN"))) %>%
  ggplot(aes(k, mse, color = method)) +
  facet_grid(. ~ p, labeller = labeller(p = label_both)) +
  geom_line() +
  geom_point() +
  geom_hline(data = sim_pred_lm, aes(yintercept = mse_lm), linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN",
       subtitle = "Traditional and weighted KNN",
       x = "K",
       y = "Test mean squared error",
       method = NULL) +
  expand_limits(y = 0) +
  theme(legend.position = "bottom")
```

## Estimating KNN on simulated wage data

```{r wage-sim-knn}
# split into train/test set
wage_split <- initial_split(wage, prop = 0.5)
wage_train <- training(wage_split)
wage_test <- testing(wage_split)

# estimate test MSE for LM and KNN models
mse_lm <- lm(wage ~ educ + age + prestige, data = wage_train) %>%
  mse(wage_test)

mse_knn <- tibble(k = c(1:10, seq(20, 100, by = 10)),
                  knn = map(k, ~ knn.reg(select(wage_train, -wage), y = wage_train$wage,
                                         test = select(wage_test, -wage), k = .)),
                  mse = map_dbl(knn, ~ mean((wage_test$wage - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "KNN on simulated wage data",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

## KNN on Biden

```{r biden-knn}
biden <- here("static", "data", "biden.csv") %>%
  read_csv()

# split into train/test set
biden_split <- initial_split(biden, p = 0.7)
biden_train <- training(biden_split)
biden_test <- testing(biden_split)

# estimate test MSE for LM and KNN models
mse_lm <- lm(biden ~ ., data = biden_train) %>%
  mse(biden_test)

mse_knn <- tibble(k = c(1:10, seq(20, 100, by = 10)),
                  knn = map(k, ~ knn.reg(select(biden_train, -biden), y = biden_train$biden,
                                         test = select(biden_test, -biden), k = .)),
                  mse = map_dbl(knn, ~ mean((biden_test$biden - .$pred)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "KNN for Biden",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

```{r biden-wknn, warning = FALSE}
# estimate test MSE for LM and WKNN models
mse_lm <- lm(biden ~ ., data = biden_train) %>%
  mse(biden_test)

mse_knn <- tibble(k = c(1:10, seq(20, 100, by = 10)),
                  knn = map(k, ~ kknn(biden ~ .,
                                      train = biden_train, test = biden_test, k = .)),
                  mse = map_dbl(knn, ~ mean((sim_test$y - .$fitted.values)^2)))

ggplot(mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "Weighted KNN for Biden",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)
```

# Bayes decision rule

For classification problems, the test error rate is minimized by a simple classifier that assigns each observation to the most likely class given its predictor values:

$$\Pr(Y = j | X = x_0)$$

where $x_0$ is the test observation and each possible class is represented by $J$. This is a **conditional probability** that $Y = j$, given the observed predictor vector$x_0$. This classifier is known as the **Bayes classifier**. If the response variable is binary (i.e. two classes), the Bayes classifier corresponds to predicting class one if $\Pr(Y = 1 | X = x_0) > 0.5$, and class two otherwise.

In the simulated example below, the blue lines indicate the **Bayes decision boundary**. The Bayes classifier's prediction is determined by this boundary: observations on the blue side would be assigned to the blue class, and observations on the red side are assigned to the red class.

```{r bayes-class}
bayes_rule <- function(x1, x2) {
  x1 + x1^2 + x2 + x2^2
}

bayes_grid <- expand.grid(x1 = seq(-1, 1, by = .05),
                          x2 = seq(-1, 1, by = .05)) %>%
  as_tibble %>%
  mutate(logodds = bayes_rule(x1, x2),
         y = logodds > .5,
         prob = logit2prob(logodds))

bayes_bound <- bind_rows(mutate(bayes_grid,
                                prob = prob,
                                cls = TRUE,
                                prob_cls = ifelse(y == cls, 1, 0)),
                         mutate(bayes_grid,
                                prob = prob,
                                cls = FALSE,
                                prob_cls = ifelse(y == cls, 1, 0)))

sim_bayes <- tibble(x1 = runif(200, -1, 1),
                    x2 = runif(200, -1, 1),
                    logodds = bayes_rule(x1, x2) + rnorm(200, 0, .5),
                    y = logodds > .5,
                    y_actual = bayes_rule(x1, x2) > .5)
sim_bayes_err <- mean(sim_bayes$y != sim_bayes$y_actual)

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_point(size = .5, alpha = .5) +
  geom_contour(aes(z = prob_cls, group = cls), bins = 1) +
  geom_point(data = sim_bayes) +
  theme(legend.position = "none")
```

The Bayes classifer produces the lowest possible test error rate, called the **Bayes error rule**, because it will always assign observations based on the maximum conditional probability:

$$1 - \E \left( \max_j \Pr(Y = j | X) \right)$$

where the expectation averages the probability over all possible values of $X$. In this simulation the Bayes error rate is $`r sim_bayes_err`$. Because in the true population the classes overlap somewhat, the Bayes classifier cannot generate an error rate of zero.

# $K$-nearest neighbors classification

Unfortunately we do not know the conditional distribution of $Y$ given $X$ in real-world data, so we cannot compute the Bayes classifier. Instead we try to produce **estimates** of the conditional distribution of $Y$ given $X$, and then classify a given observation to the class with the highest estimated probability. Logistic regression and other types of GLMs, tree-based methods, and SVMs all operate on this basic principle (or related ones - in the case of SVMs the decision is based on the test observation's location relative to the separating hyperplane).

However regression-based methods such as GLMs make assumptions about the functional form of $f$. For a purely non-parametric approach, we could use **$K$-nearest neighbors** (KNN) classification. Similar to KNN regression, given a positive integer $K$ and a test observation $x_0$, the KNN classifier identifies the $K$ nearest training observations to $x_0$, again represented by $N_0$. The conditional probability for class $j$ is the fraction of points in $N_0$ whose response values equal $j$:

$\Pr(Y = j| X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)$$

where $I(y_i = j)$ is an indicator function. Finally KNN applies Bayes rule and classifies the test observation $x_0$ to the class with the largest probability. Here is the KNN classifier applied to the example above with $K=1$:

```{r knn-class1, dependson="bayes-class"}
knn1 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 1, prob = TRUE)
prob1 <- attr(knn1, "prob")

bayes_bound1 <- bind_rows(mutate(bayes_grid,
                                 prob = attr(knn1, "prob"),
                                 y = as.logical(knn1),
                                 cls = TRUE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)),
                          mutate(bayes_grid,
                                 prob = attr(knn1, "prob"),
                                 y = as.logical(knn1),
                                 cls = FALSE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound1, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==1),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

All this classifier does is look to the single closest neighbor to make a prediction. Compared to the known decision boundary, this strongly overfits the training data. Compare this to $K=5$:

```{r knn-class5, dependson="bayes-class"}
knn5 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                   cl = sim_bayes$y, k = 5, prob = TRUE)
prob5 <- attr(knn5, "prob")

bayes_bound5 <- bind_rows(mutate(bayes_grid,
                                 prob = attr(knn5, "prob"),
                                 y = as.logical(knn5),
                                 cls = TRUE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)),
                          mutate(bayes_grid,
                                 prob = attr(knn5, "prob"),
                                 y = as.logical(knn5),
                                 cls = FALSE,
                                 prob_cls = ifelse(y == cls,
                                                   1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound5, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==5),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

The resulting decision boundary is still choppy, but not as much as before.

```{r knn-class10, dependson="bayes-class"}
knn10 <- class::knn(select(sim_bayes, x1, x2), test = select(bayes_grid, x1, x2),
                    cl = sim_bayes$y, k = 10, prob = TRUE)
prob10 <- attr(knn10, "prob")

bayes_bound10 <- bind_rows(mutate(bayes_grid,
                                  prob = attr(knn10, "prob"),
                                  y = as.logical(knn5),
                                  cls = TRUE,
                                  prob_cls = ifelse(y == cls,
                                                    1, 0)),
                           mutate(bayes_grid,
                                  prob = attr(knn10, "prob"),
                                  y = as.logical(knn5),
                                  cls = FALSE,
                                  prob_cls = ifelse(y == cls,
                                                    1, 0)))

ggplot(bayes_bound, aes(x1, x2, color = y)) +
  geom_contour(aes(z = prob_cls, group = cls, linetype = "True boundary"), bins = 1) +
  geom_contour(data = bayes_bound10, aes(z = prob_cls, group = cls, linetype = "KNN"), bins = 1) +
  geom_point(data = sim_bayes) +
  scale_color_discrete(guide = FALSE) +
  labs(title = "K nearest neighbor classifier",
       subtitle = expression(K==10),
       linetype = NULL) +
  theme(legend.position = "bottom")
```

As with KNN regression, we can calculate the test error rate across different values for $k$ to determine an optimal value:

```{r knn-class-compare, dependson="bayes-class"}
# estimate test MSE for KNN models
sim_test <- tibble(x1 = runif(5000, -1, 1),
                   x2 = runif(5000, -1, 1),
                   logodds = bayes_rule(x1, x2) + rnorm(5000, 0, .5),
                   y = logodds > .5)

mse_knn <- tibble(k = 1:100,
                  knn_train = map(k, ~ class::knn(select(sim_bayes, x1, x2),
                                                  test = select(sim_bayes, x1, x2),
                                                  cl = sim_bayes$y, k = .)),
                  knn_test = map(k, ~ class::knn(select(sim_bayes, x1, x2),
                                                 test = select(sim_test, x1, x2),
                                                 cl = sim_bayes$y, k = .)),
                  mse_train = map_dbl(knn_train, ~ mean(sim_bayes$y != as.logical(.))),
                  mse_test = map_dbl(knn_test, ~ mean(sim_test$y != as.logical(.))))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = sim_bayes_err, linetype = 2) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0)
```

And of course we could adapt this to use LOOCV or $k$-fold CV rather than the validation set approach.

## Applying KNN to Titanic

```{r titanic-data}
titanic <- titanic::titanic_train %>%
  as_tibble %>%
  select(-Name, -Ticket, -Cabin, -PassengerId, -Embarked) %>%
  mutate(Female = ifelse(Sex == "female", 1, 0)) %>%
  select(-Sex) %>%
  na.omit

titanic_split <- resample_partition(titanic, p = c("test" = .3, "train" = .7))
titanic_train <- as_tibble(titanic_split$train)
titanic_test <- as_tibble(titanic_split$test)
```

```{r titanic-logit}
titanic_logit <- glm(Survived ~ ., data = titanic_train, family = binomial)
titanic_logit_mse <- mse.glm(titanic_logit, titanic_test)
```

```{r titanic-knn-compare, dependson="bayes-class", warning = FALSE}
# estimate test MSE for KNN models
mse_knn <- tibble(k = 1:100,
                  knn_train = map(k, ~ class::knn(select(titanic_train, -Survived),
                                                  test = select(titanic_train, -Survived),
                                                  cl = titanic_train$Survived, k = .)),
                  knn_test = map(k, ~ class::knn(select(titanic_train, -Survived),
                                                 test = select(titanic_test, -Survived),
                                                 cl = titanic_train$Survived, k = .)),
                  mse_train = map_dbl(knn_train, ~ mean(titanic_test$Survived != .)),
                  mse_test = map_dbl(knn_test, ~ mean(titanic_test$Survived != .)))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = titanic_logit_mse, linetype = 2) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0)
```

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}
* @james2013introduction
* @friedman2001elements
