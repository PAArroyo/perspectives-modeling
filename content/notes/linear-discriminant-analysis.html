---
title: Linear discriminant analysis
date: 2019-01-23T13:30:00-06:00 # Schedule page publish date.
  
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
 notes:
  parent: Classification
  weight: 2
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#why-discriminant-analysis"><span class="toc-section-number">1</span> Why discriminant analysis?</a></li>
<li><a href="#running-example"><span class="toc-section-number">2</span> Running example</a></li>
<li><a href="#linear-discriminant-analysis"><span class="toc-section-number">3</span> Linear discriminant analysis</a><ul>
<li><a href="#estimate-and-understand-model"><span class="toc-section-number">3.1</span> Estimate and understand model</a></li>
<li><a href="#make-predictions"><span class="toc-section-number">3.2</span> Make Predictions</a></li>
</ul></li>
<li><a href="#quadratic-discriminant-analysis"><span class="toc-section-number">4</span> Quadratic discriminant analysis</a><ul>
<li><a href="#estimate-and-understand-model-1"><span class="toc-section-number">4.1</span> Estimate and understand model</a></li>
<li><a href="#make-predictions-1"><span class="toc-section-number">4.2</span> Make predictions</a></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">5</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">6</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(rsample)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">3</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<p><a href="/notes/logistic-regression/">Logistic regression</a> models <span class="math inline">\(\Pr (Y = k | X = x)\)</span> using the logistic function, for the case of two response classes. Another way of stating this is that we model the conditional distribution of the response <span class="math inline">\(Y\)</span> given the predictors <span class="math inline">\(X\)</span>. This method can generalize to more than two response classes (e.g. ordinal/multinomial logistic regression), but more commonly researchers employ <strong>discriminant analysis</strong>. In this alternative approach, we model the distribution of the predictors <span class="math inline">\(X\)</span> separately in each of the response classes, and then use Bayes’ theorem to flip these around into estimates for the probability of the response class given the value of <span class="math inline">\(X\)</span>. Here we cover two variations, <strong>linear discriminant analysis</strong> (LDA) and <strong>quadratic discriminant analysis</strong> (QDA).</p>
<div id="why-discriminant-analysis" class="section level1">
<h1><span class="header-section-number">1</span> Why discriminant analysis?</h1>
<p>So why do we need another classification method beyond logistic regression? There are several reasons:</p>
<ul>
<li>When the classes of the reponse variable <span class="math inline">\(Y\)</span> (i.e. <em>default = “Yes”, default = “No”</em>) are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. LDA &amp; QDA do not suffer from this problem.</li>
<li>If <span class="math inline">\(n\)</span> is small and the distribution of the predictors <span class="math inline">\(X\)</span> is approximately normal in each of the classes, the LDA &amp; QDA models are again more stable than the logistic regression model.</li>
<li>LDA &amp; QDA are often preferred over logistic regression when we have more than two non-ordinal response classes (i.e.: <em>stroke</em>, <em>drug overdose</em>, and <em>epileptic seizure</em>).</li>
</ul>
<p>However, its important to note that LDA &amp; QDA have assumptions that are often more restrictive then logistic regression:</p>
<ul>
<li>Both LDA and QDA assume the the predictor variables <span class="math inline">\(X\)</span> are drawn from a multivariate Gaussian (aka <em>normal</em>) distribution.</li>
<li>LDA assumes equality of covariances among the predictor variables <span class="math inline">\(X\)</span> across each all levels of <span class="math inline">\(Y\)</span>. This assumption is relaxed with the QDA model.</li>
<li>LDA and QDA require the number of predictor variables <span class="math inline">\(p\)</span> to be less then the sample size <span class="math inline">\(n\)</span>. Furthermore, its important to keep in mind that performance will severely decline as <span class="math inline">\(p\)</span> approaches <span class="math inline">\(n\)</span>. A simple rule of thumb is to use LDA &amp; QDA on data sets where <span class="math display">\[n \geq 5 \times p\]</span>.</li>
</ul>
<p>Also, when considering between LDA &amp; QDA its important to know that LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the predictor variable share a common variance across each <span class="math inline">\(Y\)</span> response class is badly off, then LDA can suffer from high bias. Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix is clearly untenable.</p>
</div>
<div id="running-example" class="section level1">
<h1><span class="header-section-number">2</span> Running example</h1>
<p>These notes use the <code>Default</code> data provided by the <code>ISLR</code> package. This is a simulated data set containing information on ten thousand customers such as whether the customer defaulted, is a student, the average balance carried by the customer and the income of the customer.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(default &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(ISLR<span class="op">::</span>Default))</code></pre></div>
<pre><code>## # A tibble: 10,000 x 4
##    default student balance income
##    &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;
##  1 No      No         730. 44362.
##  2 No      Yes        817. 12106.
##  3 No      No        1074. 31767.
##  4 No      No         529. 35704.
##  5 No      No         786. 38463.
##  6 No      Yes        920.  7492.
##  7 No      No         826. 24905.
##  8 No      Yes        809. 17600.
##  9 No      No        1161. 37469.
## 10 No      No           0  29275.
## # … with 9,990 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># split into training/test set</span>
split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(default, <span class="dt">prop =</span> .<span class="dv">6</span>)
train &lt;-<span class="st"> </span><span class="kw">training</span>(split)
test &lt;-<span class="st"> </span><span class="kw">testing</span>(split)</code></pre></div>
</div>
<div id="linear-discriminant-analysis" class="section level1">
<h1><span class="header-section-number">3</span> Linear discriminant analysis</h1>
<p>LDA computes “discriminant scores” for each observation to classify what response variable class it is in (i.e. default or not default). These scores are obtained by finding linear combinations of the independent variables. For a single predictor variable <span class="math inline">\(X = x\)</span> the LDA classifier is estimated as</p>
<p><span class="math display">\[\hat\delta_k(x) = x \times \frac{\hat\mu_k}{\hat\sigma^2} - \frac{\hat\mu_k^2}{2\hat\sigma^2} + \log(\hat\pi_k)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat\delta_k(x)\)</span> is the estimated discriminant score that the observation will fall in the <em>k</em>th class within the response variable (i.e. <em>default</em> or <em>not default</em>) based on the value of the predictor variable <em>x</em></li>
<li><span class="math inline">\(\hat\mu_k\)</span> is the average of all the training observations from the <em>k</em>th class</li>
<li><span class="math inline">\(\hat\sigma^2\)</span> is the weighted average of the sample variances for each of the <em>K</em> classes</li>
<li><span class="math inline">\(\hat\pi_k\)</span> is the prior probability that an observation belongs to the <em>k</em>th class (not to be confused with the mathematical constant <span class="math inline">\(\pi \approx 3.14159\)</span>)</li>
</ul>
<p>This classifier assumes the predictor is drawn from a Gaussian or normal distribution with parameters <span class="math inline">\(\mu, \sigma^2\)</span> (i.e. mean and variance). It is <strong>linear</strong> because <span class="math inline">\(\hat\delta_k(x)\)</span> is a linear function of <span class="math inline">\(x\)</span>. It assigns an observation to the <span class="math inline">\(k\)</span>th class of <span class="math inline">\(Y_k\)</span> for which discriminant score <span class="math inline">\(\hat\delta_k(x)\)</span> is largest. For example, lets assume there are two classes (<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>) for the response variable <span class="math inline">\(Y\)</span>. Based on the predictor variable(s), LDA is going to compute the probability distribution of being classified as class <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. The linear decision boundary between the probability distributions is represented by the dashed line. Discriminant scores to the left of the dashed line will be classified as <span class="math inline">\(A\)</span> and scores to the right will be classified as <span class="math inline">\(B\)</span>.</p>
<p><img src="/notes/linear-discriminant-analysis_files/figure-html/decision-bound-1.png" width="672" /></p>
<p>When dealing with more than one predictor variable, the LDA classifier assumes that the observations in the <span class="math inline">\(k\)</span>th class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(\mu_k, \mathbf{\Sigma})\)</span>, where <span class="math inline">\(\mu_k\)</span> is a class-specific mean vector, and <span class="math inline">\(\mathbf{\Sigma}\)</span> is a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes. Incorporating this into the LDA classifier results in</p>
<p><span class="math display">\[\hat\delta_k(x) = x^T\mathbf{Σ}^{-1}\hat\mu_k - \frac{1}{2}\hat\mu_k^T\mathbf{Σ}^{-1} - \hat\mu_k + \log(\hat\pi_k)\]</span></p>
<p>where an observation will be assigned to class <span class="math inline">\(k\)</span> where the discriminant score <span class="math inline">\(\hat\delta_k(x)\)</span> is largest.</p>
<div id="estimate-and-understand-model" class="section level2">
<h2><span class="header-section-number">3.1</span> Estimate and understand model</h2>
<p>In R, we fit a LDA model using the <code>lda</code> function, which is part of the <code>MASS</code> library.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Notice that the syntax for the <code>lda</code> is identical to that of <code>lm</code> and to that of <code>glm</code> except for the absence of the family option.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(lda_m1 &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">lda</span>(default <span class="op">~</span><span class="st"> </span>balance <span class="op">+</span><span class="st"> </span>student, <span class="dt">data =</span> train))</code></pre></div>
<pre><code>## Call:
## lda(default ~ balance + student, data = train)
## 
## Prior probabilities of groups:
##     No    Yes 
## 0.9668 0.0332 
## 
## Group means:
##     balance studentYes
## No      807      0.292
## Yes    1774      0.392
## 
## Coefficients of linear discriminants:
##                 LD1
## balance     0.00226
## studentYes -0.21770</code></pre>
<p>The LDA output indicates that our prior probabilities are <span class="math inline">\(\hat\pi_1 = 0.967\)</span> and <span class="math inline">\(\hat\pi_2 = 0.033\)</span>; in other words, 96.7% of the training observations are customers who did not default and 3.32% represent those that defaulted. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of <span class="math inline">\(\mu_k\)</span>. These suggest that customers that tend to default have, on average, a credit card balance of $1773.522 and are more likely to be students then non-defaulters (29.202% of non-defaulters are students whereas 39.196% of defaulters are).</p>
<p>The <strong>coefficients of linear discriminants</strong> output provides the linear combination of <code>balance</code> and <code>student = Yes</code> that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of <span class="math inline">\(X = x\)</span>. If</p>
<p><span class="math display">\[0.002 \times \text{balance} + -0.218 \times \text{student}\]</span></p>
<p>is large, then the LDA classifier will predict that the customer will default, and if it is small, then the LDA classifier will predict the customer will not default.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(lda_m1)</code></pre></div>
<p><img src="/notes/linear-discriminant-analysis_files/figure-html/default-lda-plot-1.png" width="672" /></p>
</div>
<div id="make-predictions" class="section level2">
<h2><span class="header-section-number">3.2</span> Make Predictions</h2>
<p>We can use <code>predict()</code> for LDA to generate predicted values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
 <span class="dt">balance =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1000</span>, <span class="dv">2000</span>), <span class="dv">2</span>), 
    <span class="dt">student =</span> <span class="kw">c</span>(<span class="st">&quot;No&quot;</span>, <span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;Yes&quot;</span>))
 )</code></pre></div>
<pre><code>## # A tibble: 4 x 2
##   balance student
##     &lt;dbl&gt; &lt;chr&gt;  
## 1    1000 No     
## 2    2000 No     
## 3    1000 Yes    
## 4    2000 Yes</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(df_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_m1, df))</code></pre></div>
<pre><code>## $class
## [1] No  Yes No  No 
## Levels: No Yes
## 
## $posterior
##      No     Yes
## 1 0.990 0.00971
## 2 0.439 0.56096
## 3 0.994 0.00609
## 4 0.556 0.44401
## 
## $x
##     LD1
## 1 0.428
## 2 2.684
## 3 0.210
## 4 2.466</code></pre>
<p>We see that <code>predict()</code> returns a list with three elements. The first element, <code>class</code>, contains LDA’s predictions about the customer defaulting. Here we see that the second observation (non-student with balance of $2,000) is the only one that is predicted to default. The second element, <code>posterior</code>, is a matrix that contains the posterior probability that the corresponding observations will or will not default. Here we see that the only observation to have a posterior probability of defaulting greater than 50% is observation 2, which is why the LDA model predicted this observation will default. However, we also see that observation 4 has a 42% probability of defaulting. Right now the model is predicting that this observation will not default because this probability is less than 50%; however, we will see shortly how we can make adjustments to our posterior probability thresholds. Finally, <code>x</code> contains the linear discriminant values, described earlier.</p>
<p>As previously mentioned the default setting is to use a 50% threshold for the posterior probabilities. We can recreate the predictions contained in the <code>class</code> element above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># number of non-defaulters</span>
<span class="kw">sum</span>(df_pred<span class="op">$</span>posterior[, <span class="dv">1</span>] <span class="op">&gt;=</span><span class="st"> </span>.<span class="dv">5</span>)</code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># number of defaulters</span>
<span class="kw">sum</span>(df_pred<span class="op">$</span>posterior[, <span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>)</code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>If we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. For instance, suppose that a credit card company is extremely risk-adverse and wants to assume that a customer with 40% or greater probability is a high-risk customer. We can easily assess the number of high-risk customers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># number of high-risk customers with 40% probability of defaulting</span>
<span class="kw">sum</span>(df_pred<span class="op">$</span>posterior[, <span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">4</span>)</code></pre></div>
<pre><code>## [1] 2</code></pre>
</div>
</div>
<div id="quadratic-discriminant-analysis" class="section level1">
<h1><span class="header-section-number">4</span> Quadratic discriminant analysis</h1>
<p>As previously mentioned, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution and the covariance of the predictor variables are common across all <span class="math inline">\(k\)</span> levels of the response variable <span class="math inline">\(Y\)</span>. <strong>Quadratic discriminant analysis</strong> (QDA) provides an alternative approach. Like LDA, the QDA classifier assumes that the observations from each class of <span class="math inline">\(Y\)</span> are drawn from a Gaussian distribution. However, unlike LDA, QDA assumes that each class has its own covariance matrix. In other words, the predictor variables are not assumed to have common variance across each of the <span class="math inline">\(k\)</span> levels in <span class="math inline">\(Y\)</span>. Mathematically, it assumes that an observation from the <span class="math inline">\(k\)</span>th class is of the form <span class="math inline">\(X ∼ N(\mu_k, \mathbf{\Sigma}_k)\)</span>, where <span class="math inline">\(\mathbf{\Sigma}_k\)</span> is a covariance matrix for the <span class="math inline">\(k\)</span>th class. Under this assumption, the classifier assigns an observation to the class for which</p>
<p><span class="math display">\[\hat\delta_k(x) = -\frac{1}{2}x^T\mathbf{Σ}^{-1}_kx+x^T\mathbf{Σ}^{-1}_k\hat\mu_k - \frac{1}{2}\hat\mu_k^T\mathbf{Σ}^{-1}_k\hat\mu_k - \frac{1}{2}\log\big|\mathbf{Σ}_k\big|+\log(\hat\pi_k)\]</span></p>
<p>is largest. Why is this important? In trying to classify observations into response classes, LDA provides linear decision boundaries that are based on the assumption that the observations vary consistently across all classes. However, QDA is able to capture differing covariances and provide non-linear classification decision boundaries.</p>
<div id="estimate-and-understand-model-1" class="section level2">
<h2><span class="header-section-number">4.1</span> Estimate and understand model</h2>
<p>Similar to <code>lda()</code>, we can use the <code>MASS</code> library to fit a QDA model. Here we use the <code>qda()</code> function. The output is very similar to the <code>lda()</code> output. It contains the prior probabilities and the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(qda.m1 &lt;-<span class="st"> </span><span class="kw">qda</span>(default <span class="op">~</span><span class="st"> </span>balance <span class="op">+</span><span class="st"> </span>student, <span class="dt">data =</span> train))</code></pre></div>
<pre><code>## Call:
## qda(default ~ balance + student, data = train)
## 
## Prior probabilities of groups:
##     No    Yes 
## 0.9668 0.0332 
## 
## Group means:
##     balance studentYes
## No      807      0.292
## Yes    1774      0.392</code></pre>
</div>
<div id="make-predictions-1" class="section level2">
<h2><span class="header-section-number">4.2</span> Make predictions</h2>
<p>The <code>predict()</code> function works in exactly the same fashion as for LDA except it does not return the linear discriminant values. In comparing this simple prediction example to that seen in the LDA section we see minor changes in the posterior probabilities. Most notably, the posterior probability that observation 4 will default increased by nearly 8% points.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(qda.m1, df)</code></pre></div>
<pre><code>## $class
## [1] No  Yes No  Yes
## Levels: No Yes
## 
## $posterior
##      No     Yes
## 1 0.997 0.00309
## 2 0.407 0.59321
## 3 0.998 0.00189
## 4 0.495 0.50477</code></pre>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">5</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-01-22                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package      * version    date       lib
##  assertthat     0.2.0      2017-04-11 [2]
##  backports      1.1.3      2018-12-14 [2]
##  bindr          0.1.1      2018-03-13 [2]
##  bindrcpp     * 0.2.2      2018-03-29 [1]
##  blogdown       0.9.4      2018-11-26 [1]
##  bookdown       0.9        2018-12-21 [1]
##  broom        * 0.5.1      2018-12-05 [2]
##  callr          3.1.1      2018-12-21 [2]
##  caret        * 6.0-81     2018-11-20 [1]
##  cellranger     1.1.0      2016-07-27 [2]
##  class          7.3-15     2019-01-01 [2]
##  cli            1.0.1      2018-09-25 [1]
##  codetools      0.2-16     2018-12-24 [2]
##  colorspace     1.3-2      2016-12-14 [2]
##  crayon         1.3.4      2017-09-16 [2]
##  data.table     1.11.8     2018-09-30 [2]
##  desc           1.2.0      2018-05-01 [2]
##  devtools       2.0.1      2018-10-26 [1]
##  digest         0.6.18     2018-10-10 [1]
##  dplyr        * 0.7.8      2018-11-10 [1]
##  evaluate       0.12       2018-10-09 [2]
##  fansi          0.4.0      2018-10-05 [2]
##  forcats      * 0.3.0      2018-02-19 [2]
##  foreach        1.4.4      2017-12-12 [2]
##  fs             1.2.6      2018-08-23 [1]
##  generics       0.0.2      2018-11-29 [1]
##  ggplot2      * 3.1.0      2018-10-25 [1]
##  glue           1.3.0      2018-07-17 [2]
##  gower          0.1.2      2017-02-23 [2]
##  gtable         0.2.0      2016-02-26 [2]
##  haven          2.0.0      2018-11-22 [2]
##  here           0.1        2017-05-28 [2]
##  hms            0.4.2      2018-03-10 [2]
##  htmltools      0.3.6      2017-04-28 [1]
##  httr           1.4.0      2018-12-11 [2]
##  ipred          0.9-8      2018-11-05 [1]
##  iterators      1.0.10     2018-07-13 [2]
##  jsonlite       1.6        2018-12-07 [2]
##  knitr        * 1.21       2018-12-10 [2]
##  labeling       0.3        2014-08-23 [2]
##  lattice      * 0.20-38    2018-11-04 [2]
##  lava           1.6.4      2018-11-25 [2]
##  lazyeval       0.2.1      2017-10-29 [2]
##  lubridate      1.7.4      2018-04-11 [2]
##  magrittr       1.5        2014-11-22 [2]
##  MASS         * 7.3-51.1   2018-11-01 [2]
##  Matrix         1.2-15     2018-11-01 [2]
##  memoise        1.1.0      2017-04-21 [2]
##  ModelMetrics   1.2.2      2018-11-03 [2]
##  modelr       * 0.1.2      2018-05-11 [2]
##  munsell        0.5.0      2018-06-12 [2]
##  nlme           3.1-137    2018-04-07 [2]
##  nnet           7.3-12     2016-02-02 [2]
##  patchwork    * 0.0.1      2018-09-06 [1]
##  pillar         1.3.1      2018-12-15 [2]
##  pkgbuild       1.0.2      2018-10-16 [1]
##  pkgconfig      2.0.2      2018-08-16 [2]
##  pkgload        1.0.2      2018-10-29 [1]
##  plyr           1.8.4      2016-06-08 [2]
##  prettyunits    1.0.2      2015-07-13 [2]
##  processx       3.2.1      2018-12-05 [2]
##  prodlim        2018.04.18 2018-04-18 [2]
##  ps             1.3.0      2018-12-21 [2]
##  purrr        * 0.2.5      2018-05-29 [2]
##  R6             2.3.0      2018-10-04 [1]
##  RColorBrewer   1.1-2      2014-12-07 [2]
##  Rcpp           1.0.0      2018-11-07 [1]
##  readr        * 1.3.1      2018-12-21 [2]
##  readxl         1.2.0      2018-12-19 [2]
##  recipes        0.1.4      2018-11-19 [1]
##  remotes        2.0.2      2018-10-30 [1]
##  reshape2       1.4.3      2017-12-11 [2]
##  rlang          0.3.0.1    2018-10-25 [1]
##  rmarkdown      1.11       2018-12-08 [2]
##  rpart          4.1-13     2018-02-23 [1]
##  rprojroot      1.3-2      2018-01-03 [2]
##  rsample      * 0.0.3      2018-11-20 [1]
##  rstudioapi     0.8        2018-10-02 [1]
##  rvest          0.3.2      2016-06-17 [2]
##  scales         1.0.0      2018-08-09 [1]
##  sessioninfo    1.1.1      2018-11-05 [1]
##  stringi        1.2.4      2018-07-20 [2]
##  stringr      * 1.3.1      2018-05-10 [2]
##  survival       2.43-3     2018-11-26 [2]
##  testthat       2.0.1      2018-10-13 [2]
##  tibble       * 2.0.0      2019-01-04 [2]
##  tidyr        * 0.8.2      2018-10-28 [2]
##  tidyselect     0.2.5      2018-10-11 [1]
##  tidyverse    * 1.2.1      2017-11-14 [2]
##  timeDate       3043.102   2018-02-21 [2]
##  titanic      * 0.1.0      2015-08-31 [2]
##  usethis        1.4.0      2018-08-14 [1]
##  utf8           1.1.4      2018-05-24 [2]
##  withr          2.1.2      2018-03-15 [2]
##  xfun           0.4        2018-10-23 [1]
##  xml2           1.2.0      2018-01-24 [2]
##  yaml           2.2.0      2018-07-25 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  Github (rstudio/blogdown@b2e1ed4)   
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> References</h1>
<ul>
<li><a href="http://uc-r.github.io/discriminant_analysis">Linear &amp; Quadratic Discrimination</a></li>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note I do not explicitly load <code>MASS</code> via <code>library(MASS)</code> because doing so would cause a conflict with <code>dplyr::select()</code>.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
