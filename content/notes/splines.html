---
title: Regression splines
date: 2019-02-18T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Moving beyond linearity
    weight: 2
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#basis-functions"><span class="toc-section-number">1</span> Basis functions</a></li>
<li><a href="#regression-splines"><span class="toc-section-number">2</span> Regression splines</a><ul>
<li><a href="#piecewise-polynomials"><span class="toc-section-number">2.1</span> Piecewise polynomials</a></li>
<li><a href="#constraints-and-splines"><span class="toc-section-number">2.2</span> Constraints and splines</a></li>
<li><a href="#spline-basis-representation"><span class="toc-section-number">2.3</span> Spline basis representation</a></li>
<li><a href="#basis-splines-vs.natural-splines"><span class="toc-section-number">2.4</span> Basis splines vs. natural splines</a></li>
<li><a href="#choosing-the-number-and-location-of-knots"><span class="toc-section-number">2.5</span> Choosing the number and location of knots</a></li>
<li><a href="#comparison-to-polynomial-regression"><span class="toc-section-number">2.6</span> Comparison to polynomial regression</a></li>
</ul></li>
<li><a href="#smoothing-splines"><span class="toc-section-number">3</span> Smoothing splines</a><ul>
<li><a href="#choosing-the-smoothing-parameter-lambda"><span class="toc-section-number">3.1</span> Choosing the smoothing parameter <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li><a href="#multivariate-adaptive-regression-splines-mars"><span class="toc-section-number">4</span> Multivariate adaptive regression splines (MARS)</a><ul>
<li><a href="#basis-functions-1"><span class="toc-section-number">4.1</span> Basis functions</a></li>
<li><a href="#model-construction"><span class="toc-section-number">4.2</span> Model construction</a></li>
<li><a href="#application-to-ames-housing-data"><span class="toc-section-number">4.3</span> Application to Ames housing data</a></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">5</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">6</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(titanic)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(splines)
<span class="kw">library</span>(ISLR)
<span class="kw">library</span>(lattice)
<span class="kw">library</span>(gam)
<span class="kw">library</span>(here)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(margins)
<span class="kw">library</span>(earth)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<div id="basis-functions" class="section level1">
<h1><span class="header-section-number">1</span> Basis functions</h1>
<p><strong>Basis functions</strong> are a family of functions or transformations applied to a variable <span class="math inline">\(X\)</span>: <span class="math inline">\(b_1(X), b_2(X), \ldots, b_K(X)\)</span>. Instead of fitting the linear model to <span class="math inline">\(X\)</span>, we fit the model:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \ldots + \beta_K b_K(x_i) + \epsilon_i\]</span></p>
<p>The functional form of the basis function is determined in advanced and is fixed and known. <a href="/notes/global-methods/">Polynomial and step functions</a> are specific types of basis functions. For polynomial regression, <span class="math inline">\(b_j(x_i) = x_i^j\)</span> where <span class="math inline">\(j\)</span> is the polynomial degree. Since this linear model is just a transformation of the predictors <span class="math inline">\(X_i\)</span>, we can use least squares to estimate the model and apply all the standard statistical inferential techniques to evaluate and interpret the model.</p>
</div>
<div id="regression-splines" class="section level1">
<h1><span class="header-section-number">2</span> Regression splines</h1>
<p><strong>Regression splines</strong> extend polynomial transformations and piecewise constant regression by fitting separate polynomial functions over different regions of <span class="math inline">\(X\)</span>.</p>
<div id="piecewise-polynomials" class="section level2">
<h2><span class="header-section-number">2.1</span> Piecewise polynomials</h2>
<p><strong>Piecewise polynomial regression</strong> fits separate low-degree polynomials for different regions of <span class="math inline">\(X\)</span>. For instance, a cubic piecewise polynomial regression model takes the form:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i\]</span></p>
<p>where the <span class="math inline">\(\beta\)</span>s are now vectors with different values for different parts of the range of <span class="math inline">\(X\)</span>. The points where the parameters change are called <strong>knots</strong>.</p>
<p>The special case of a piecewise cubic polynomial with 0 knots is just an ordinary cubic polynomial:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i\]</span></p>
<p>Likewise, the special case of a piecewise constant polynomial (i.e. polynomial with degree <span class="math inline">\(0\)</span>) is piecewise constant regression.</p>
<p>A piecewise cubic polynomial with a single knot at point <span class="math inline">\(c\)</span> takes the form:</p>
<p><span class="math display">\[y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp; \text{if } x_i &lt; c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp; \text{if } x_i \geq c
   \end{cases}\]</span></p>
<p>In essence, we fit two separate polynomial functions to the data. Each function is estimated using least squares applied to the individual functions and subsets of data. The more knots <span class="math inline">\(K\)</span> used, the more flexible the piecewise polynomial.</p>
<p>While the example here uses a cubic (third-order) polynomial, we don’t have to use it. We could select any higher-order degree for the polynomial.</p>
<p>Of course this leads to a somewhat odd and discontinuous function:</p>
<p><img src="/notes/splines_files/figure-html/sim-piecewise-1.png" width="672" /></p>
</div>
<div id="constraints-and-splines" class="section level2">
<h2><span class="header-section-number">2.2</span> Constraints and splines</h2>
<p>Instead of a discontinuous piecewise function, we’d rather have something continuous. That is, we impose the <strong>constraint</strong> that the fitted curve must be continuous.</p>
<p><img src="/notes/splines_files/figure-html/sim-spline-1.png" width="672" /></p>
<p>But notice that this constraint is insufficient. Now the function is continuous, but still looks unnatural because of the V-shaped join. We should add two additional constraints: not only should the fitted curve be continuous, but the first and second <strong>derivatives</strong> should also be continuous at the knot. This will generate a fitted curve that is continuous and <strong>smooth</strong>.</p>
<p><img src="/notes/splines_files/figure-html/sim-spline-smooth-1.png" width="672" /></p>
<p>By increasing the number of knots, we will increase the flexibility of the resulting spline:</p>
<p><img src="/notes/splines_files/figure-html/sim-spline-smooth-5-1.png" width="672" /></p>
</div>
<div id="spline-basis-representation" class="section level2">
<h2><span class="header-section-number">2.3</span> Spline basis representation</h2>
<p>Regression splines can be represented as basis functions, which identifies how they can be estimated using OLS and still fit it under the constraint that it and its derivatives can be continuous. A cubic spline with <span class="math inline">\(K\)</span> knots can be modeled as</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 b_1 (x_i) + \beta_3 b_3 (x_i) + \cdots + \beta_{K + 3} b_{K + 3} (x_i) + \epsilon_i\]</span></p>
<p>for an appropriate choice of basis functions <span class="math inline">\(b_1, b_2, \ldots, b_{K+3}\)</span><span class="math inline">\(.^[\)</span>K$ knots with a polynomial degree of <span class="math inline">\(3\)</span>.]</p>
<p>The basis function for a cubic spline is to start with a basis for a cubic polynomial and then add one <strong>truncated power basis</strong> function per knot. A truncated power basis function is defined as:</p>
<p><span class="math display">\[
h(x, \zeta) = (x - \zeta)_+^3 = 
\begin{cases} 
  (x - \zeta)^3 &amp; \text{if } x &gt; \zeta \\
  0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\zeta\)</span> is the knot. Adding a term of the form <span class="math inline">\(\beta_4 h(x, \zeta)\)</span> to this model will lead to a discontinuity in only the third derivative at <span class="math inline">\(\zeta\)</span>. The function remains continuous, with continuous first and second derivatives, at each of the knots.</p>
<p>So fitting a cubic spline to a data set with <span class="math inline">\(K\)</span> knots requires least squares with an intercept and <span class="math inline">\(3 + K\)</span> predictors</p>
<p><span class="math display">\[X, X^2, X^3, h(X, \zeta_1), h(X, \zeta_2), \ldots, h(X, \zeta_K)\]</span></p>
<p>where <span class="math inline">\(\zeta_1, \ldots, \zeta_K\)</span> are the knots. This amounts to estimating a total of <span class="math inline">\(K + 4\)</span> regression coefficients, and so fitting a cubic spline with <span class="math inline">\(K\)</span> knots uses <span class="math inline">\(K+4\)</span> degrees of freedom.</p>
<p>Notice how the model matrix is constructed when constructing a cubic spline with <span class="math inline">\(K = 4\)</span> knots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_piece <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(<span class="dt">splines =</span> <span class="kw">bs</span>(sim_piece<span class="op">$</span>x, <span class="dt">df =</span> <span class="dv">7</span>) <span class="op">%&gt;%</span>
<span class="st">              </span>as_tibble)</code></pre></div>
<pre><code>## # A tibble: 100 x 8
##         x    `1`   `2`     `3`    `4`   `5`        `6`   `7`
##     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;
##  1 1.14   0.499  0.416 0.0489  0      0     0          0    
##  2 6.22   0      0     0.0203  0.505  0.446 0.0288     0    
##  3 6.09   0      0     0.0293  0.539  0.412 0.0199     0    
##  4 6.23   0      0     0.0196  0.502  0.449 0.0297     0    
##  5 8.61   0      0     0       0.0253 0.271 0.568      0.136
##  6 6.40   0      0     0.0112  0.454  0.490 0.0450     0    
##  7 0.0950 0      0     0       0      0     0          0    
##  8 2.33   0.0326 0.528 0.428   0.0113 0     0          0    
##  9 6.66   0      0     0.00360 0.377  0.542 0.0773     0    
## 10 5.14   0      0     0.183   0.663  0.155 0.00000183 0    
## # … with 90 more rows</code></pre>
</div>
<div id="basis-splines-vs.natural-splines" class="section level2">
<h2><span class="header-section-number">2.4</span> Basis splines vs. natural splines</h2>
<p>One drawback to the splines as currently presented (known as <strong>basis splines</strong>) is that they have high variance at the outer range of the predictors. Consider a basis spline with 3 knots applied to the <a href="/notes/global-methods/#age-and-voting">voting and age logistic regression model</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mh &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;mental_health.csv&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>na.omit</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   vote96 = col_double(),
##   mhealth_sum = col_double(),
##   age = col_double(),
##   educ = col_double(),
##   black = col_double(),
##   female = col_double(),
##   married = col_double(),
##   inc10 = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Basis spline with 3 knots&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4282572 0.6315037 0.2250108
## 2  22.875 0.4302940 0.5164704 0.3441176
## 3  25.750 0.4627694 0.5435835 0.3819553
## 4  28.625 0.5123251 0.5778767 0.4467736
## 5  31.500 0.5643390 0.6248809 0.5037972
## 6  34.375 0.6046836 0.6687555 0.5406117
## 7  37.250 0.6321075 0.6856957 0.5785192
## 8  40.125 0.6572544 0.7098796 0.6046293
## 9  43.000 0.6899492 0.7485517 0.6313467
## 10 45.875 0.7318233 0.7852485 0.6783982
## 11 48.750 0.7748341 0.8202808 0.7293873
## 12 51.625 0.8122626 0.8565635 0.7679616
## 13 54.500 0.8402300 0.8873578 0.7931022
## 14 57.375 0.8570720 0.9048012 0.8093428
## 15 60.250 0.8638145 0.9093047 0.8183243
## 16 63.125 0.8626314 0.9058251 0.8194377
## 17 66.000 0.8545801 0.8984857 0.8106745
## 18 68.875 0.8399964 0.8899250 0.7900678
## 19 71.750 0.8188883 0.8797559 0.7580207
## 20 74.625 0.7912946 0.8650531 0.7175361</code></pre>
<p><img src="/notes/splines_files/figure-html/vote-age-basis-spline-1.png" width="672" /></p>
<p>Notice the 95% confidence interval balloons at both low and high values for age. To control for this, we impose <strong>boundary constraints</strong>: the function is required to be linear at the boundary (the region where <span class="math inline">\(X\)</span> is smaller than the smallest knot, or larger than the largest knot). This type of spline is known as a <strong>natural spline</strong>. This leads to more stable estimates at the boundaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate basis spline model</span>
vote_age_basis &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4282572 0.6315037 0.2250108
## 2  22.875 0.4302940 0.5164704 0.3441176
## 3  25.750 0.4627694 0.5435835 0.3819553
## 4  28.625 0.5123251 0.5778767 0.4467736
## 5  31.500 0.5643390 0.6248809 0.5037972
## 6  34.375 0.6046836 0.6687555 0.5406117
## 7  37.250 0.6321075 0.6856957 0.5785192
## 8  40.125 0.6572544 0.7098796 0.6046293
## 9  43.000 0.6899492 0.7485517 0.6313467
## 10 45.875 0.7318233 0.7852485 0.6783982
## 11 48.750 0.7748341 0.8202808 0.7293873
## 12 51.625 0.8122626 0.8565635 0.7679616
## 13 54.500 0.8402300 0.8873578 0.7931022
## 14 57.375 0.8570720 0.9048012 0.8093428
## 15 60.250 0.8638145 0.9093047 0.8183243
## 16 63.125 0.8626314 0.9058251 0.8194377
## 17 66.000 0.8545801 0.8984857 0.8106745
## 18 68.875 0.8399964 0.8899250 0.7900678
## 19 71.750 0.8188883 0.8797559 0.7580207
## 20 74.625 0.7912946 0.8650531 0.7175361</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vote_age_natural &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4104826 0.5694648 0.2515004
## 2  22.875 0.4360929 0.5241522 0.3480335
## 3  25.750 0.4671766 0.5368465 0.3975066
## 4  28.625 0.5088239 0.5837505 0.4338973
## 5  31.500 0.5624939 0.6227874 0.5022005
## 6  34.375 0.6105011 0.6791545 0.5418478
## 7  37.250 0.6345767 0.6965424 0.5726110
## 8  40.125 0.6485917 0.7086990 0.5884845
## 9  43.000 0.6785472 0.7470780 0.6100163
## 10 45.875 0.7301420 0.7856302 0.6746538
## 11 48.750 0.7836706 0.8343419 0.7329993
## 12 51.625 0.8239502 0.8782916 0.7696088
## 13 54.500 0.8470984 0.8976731 0.7965238
## 14 57.375 0.8581347 0.9018701 0.8143992
## 15 60.250 0.8606929 0.9014754 0.8199105
## 16 63.125 0.8568879 0.9017039 0.8120720
## 17 66.000 0.8480399 0.8996926 0.7963872
## 18 68.875 0.8344507 0.8913064 0.7775950
## 19 71.750 0.8158132 0.8755124 0.7561140
## 20 74.625 0.7917365 0.8538166 0.7296564</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">bind_rows</span>(
  <span class="dt">Basis =</span> vote_age_basis,
  <span class="dt">Natural =</span> vote_age_natural,
  <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals, <span class="dt">color =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper, <span class="dt">color =</span> model), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower, <span class="dt">color =</span> model), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Spline&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/splines_files/figure-html/vote-age-natural-spline-1.png" width="672" /></p>
</div>
<div id="choosing-the-number-and-location-of-knots" class="section level2">
<h2><span class="header-section-number">2.5</span> Choosing the number and location of knots</h2>
<p>Typically knots are placed in a uniform fashion; that is, the cutpoints <span class="math inline">\(c\)</span> are determined by first identifying the number of knots <span class="math inline">\(K\)</span> for the model, then partitioning <span class="math inline">\(X\)</span> into uniform quantiles. So if we fit a cubic regression spline with 5 knots on the voting and age logistic regression, it would divide <code>age</code> into 5 equal quantiles<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> and estimate the cubic spline function for each quantile:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">8</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">attr</span>(<span class="kw">bs</span>(mh<span class="op">$</span>age, <span class="dt">df =</span> <span class="dv">8</span>), <span class="st">&quot;knots&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Natural spline with 5 knots&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4365875 0.6152524 0.2579225
## 2  22.875 0.4328343 0.5205456 0.3451230
## 3  25.750 0.4498344 0.5346405 0.3650284
## 4  28.625 0.5067269 0.5790485 0.4344054
## 5  31.500 0.5821232 0.6651290 0.4991173
## 6  34.375 0.6231946 0.6923669 0.5540224
## 7  37.250 0.6237312 0.7086129 0.5388495
## 8  40.125 0.6228755 0.6946885 0.5510625
## 9  43.000 0.6680380 0.7513065 0.5847694
## 10 45.875 0.7529138 0.8133569 0.6924707
## 11 48.750 0.8127571 0.8773629 0.7481513
## 12 51.625 0.8315533 0.8859030 0.7772036
## 13 54.500 0.8316128 0.8860993 0.7771264
## 14 57.375 0.8309324 0.8956626 0.7662023
## 15 60.250 0.8363073 0.8975423 0.7750724
## 16 63.125 0.8434146 0.8953149 0.7915143
## 17 66.000 0.8478102 0.8979973 0.7976231
## 18 68.875 0.8453973 0.9022059 0.7885887
## 19 71.750 0.8334613 0.8969400 0.7699826
## 20 74.625 0.8114817 0.8792868 0.7436766</code></pre>
<p><img src="/notes/splines_files/figure-html/vote-spline-1.png" width="672" /></p>
<p>But this still leaves unaddressed the matter of how many knots should we use? Or how many degrees should each polynomial be? While theory should still be our guide, we can also use <a href="/notes/cross-validation/">cross-validation</a> to determine the optimal number of knots and/or polynomial degrees. For our voting and age model, we can estimate <span class="math inline">\(10\)</span>-fold CV MSE for varying numbers of knots for a cubic natural spline:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to simplify things</span>
vote_spline &lt;-<span class="st"> </span><span class="cf">function</span>(splits, <span class="dt">df =</span> <span class="ot">NULL</span>){
  <span class="co"># estimate the model on each fold</span>
  model &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> df),
                <span class="dt">data =</span> <span class="kw">analysis</span>(splits))
  
  model_acc &lt;-<span class="st"> </span><span class="kw">augment</span>(model, <span class="dt">newdata =</span> <span class="kw">assessment</span>(splits)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">accuracy</span>(<span class="dt">truth =</span> <span class="kw">factor</span>(vote96), <span class="dt">estimate =</span> <span class="kw">factor</span>(<span class="kw">round</span>(.fitted)))
  
  <span class="kw">mean</span>(model_acc<span class="op">$</span>.estimate)
}

tune_over_knots &lt;-<span class="st"> </span><span class="cf">function</span>(splits, knots){
  <span class="kw">vote_spline</span>(splits, <span class="dt">df =</span> knots <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)
}

<span class="co"># estimate CV error for knots in 0:25</span>
results &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(mh, <span class="dt">v =</span> <span class="dv">10</span>)

<span class="kw">expand</span>(results, id, <span class="dt">knots =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(results) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">acc =</span> <span class="kw">map2_dbl</span>(splits, knots, tune_over_knots)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(knots) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">acc =</span> <span class="kw">mean</span>(acc)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">err =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>acc) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(knots, err)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>percent) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Optimal number of knots for natural cubic spline regression&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Knots&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;10-fold CV error&quot;</span>)</code></pre></div>
<pre><code>## Joining, by = &quot;id&quot;</code></pre>
<p><img src="/notes/splines_files/figure-html/vote-cv-1.png" width="672" /></p>
<p>These results (weakly) suggest the optimal number of knots is around 7. The resulting model produced by these parameters is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">10</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">attr</span>(<span class="kw">bs</span>(mh<span class="op">$</span>age, <span class="dt">df =</span> <span class="dv">10</span>), <span class="st">&quot;knots&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Natural spline with 9 knots&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4282471 0.6252063 0.2312880
## 2  22.875 0.4348659 0.5276855 0.3420463
## 3  25.750 0.4556856 0.5549905 0.3563806
## 4  28.625 0.5031728 0.5913505 0.4149951
## 5  31.500 0.5759378 0.6593652 0.4925103
## 6  34.375 0.6289635 0.7183163 0.5396107
## 7  37.250 0.6243092 0.7120114 0.5366070
## 8  40.125 0.6185019 0.6979372 0.5390666
## 9  43.000 0.6683266 0.7593541 0.5772991
## 10 45.875 0.7578417 0.8313389 0.6843446
## 11 48.750 0.8104157 0.8759382 0.7448932
## 12 51.625 0.8263481 0.8944110 0.7582853
## 13 54.500 0.8319732 0.9015882 0.7623582
## 14 57.375 0.8351908 0.8945150 0.7758667
## 15 60.250 0.8384688 0.9091522 0.7677855
## 16 63.125 0.8428727 0.9123006 0.7734447
## 17 66.000 0.8456804 0.9030068 0.7883540
## 18 68.875 0.8435757 0.8995645 0.7875869
## 19 71.750 0.8330172 0.8994600 0.7665745
## 20 74.625 0.8119896 0.8872808 0.7366983</code></pre>
<p><img src="/notes/splines_files/figure-html/vote-optimal-mod-1.png" width="672" /></p>
</div>
<div id="comparison-to-polynomial-regression" class="section level2">
<h2><span class="header-section-number">2.6</span> Comparison to polynomial regression</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate natural spline model with df = 15</span>
vote_age_spline &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">15</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.3793679 0.5937175 0.1650183
## 2  22.875 0.4671325 0.5806376 0.3536274
## 3  25.750 0.4581774 0.5564811 0.3598738
## 4  28.625 0.4840827 0.5847188 0.3834465
## 5  31.500 0.6029202 0.7061861 0.4996543
## 6  34.375 0.6179708 0.7159191 0.5200224
## 7  37.250 0.6235426 0.7238391 0.5232461
## 8  40.125 0.6247015 0.7286857 0.5207172
## 9  43.000 0.6674434 0.7672512 0.5676356
## 10 45.875 0.7508906 0.8340875 0.6676938
## 11 48.750 0.8144245 0.8898146 0.7390344
## 12 51.625 0.8351965 0.9218317 0.7485614
## 13 54.500 0.8314213 0.9177248 0.7451178
## 14 57.375 0.8246783 0.9038665 0.7454902
## 15 60.250 0.8285899 0.9252102 0.7319696
## 16 63.125 0.8456132 0.9235091 0.7677174
## 17 66.000 0.8595835 0.9339104 0.7852565
## 18 68.875 0.8560419 0.9333724 0.7787114
## 19 71.750 0.8343533 0.9014878 0.7672188
## 20 74.625 0.8024240 0.8888855 0.7159626</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vote_age_poly &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(age, <span class="dt">degree =</span> <span class="dv">15</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper       lower
## 1  20.000 0.3147316 0.6468989 -0.01743579
## 2  22.875 0.4481369 0.5904278  0.30584594
## 3  25.750 0.4493254 0.5489748  0.34967596
## 4  28.625 0.5029676 0.5920328  0.41390234
## 5  31.500 0.5772570 0.6624795  0.49203446
## 6  34.375 0.6270813 0.6999641  0.55419857
## 7  37.250 0.6262224 0.7007168  0.55172801
## 8  40.125 0.6197892 0.6946550  0.54492341
## 9  43.000 0.6669163 0.7433337  0.59049880
## 10 45.875 0.7529868 0.8194943  0.68647929
## 11 48.750 0.8153374 0.8780881  0.75258669
## 12 51.625 0.8336618 0.8975367  0.76978695
## 13 54.500 0.8274747 0.8973390  0.75761035
## 14 57.375 0.8261138 0.9013465  0.75088105
## 15 60.250 0.8390544 0.9160919  0.76201684
## 16 63.125 0.8480082 0.9289159  0.76710054
## 17 66.000 0.8413788 0.9238493  0.75890838
## 18 68.875 0.8362665 0.9250883  0.74744477
## 19 71.750 0.8524834 0.9369094  0.76805735
## 20 74.625 0.8572613 0.9520232  0.76249934</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">bind_rows</span>(
  <span class="st">`</span><span class="dt">Natural cubic spline</span><span class="st">`</span> =<span class="st"> </span>vote_age_spline,
  <span class="st">`</span><span class="dt">Polynomial</span><span class="st">`</span> =<span class="st"> </span>vote_age_poly,
  <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals, <span class="dt">color =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="co"># geom_line(aes(y = upper, color = model), linetype = 2) +</span>
<span class="st">  </span><span class="co"># geom_line(aes(y = lower, color = model), linetype = 2) +</span>
<span class="st">  </span><span class="co"># geom_hline(yintercept = 0, linetype = 1) +</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/splines_files/figure-html/vote-spline-poly-1.png" width="672" /></p>
<p>Splines are generally superior to polynomial regression because their flexibility is defined primarily in terms of the number of knots in the model, rather than the degree of the highest polynomial term (e.g. <span class="math inline">\(X^{15}\)</span>), whereas polynomial regression models must use high degree polynomials to achieve similar flexibility. This leads to more overfitting and wilder behavior, especially in the boundary regions. In the comparison above, we fit a natural cubic spline with 15 degrees of freedom (aka 12 knots) and compare it to a degree-15 polynomial regression. Notice the undesirable behavior of the polynomial regression model at the boundaries relative to the natural cubic spline.</p>
</div>
</div>
<div id="smoothing-splines" class="section level1">
<h1><span class="header-section-number">3</span> Smoothing splines</h1>
<p><strong>Smoothing splines</strong> take a different approach to producing a spline. The goal is to fit a function <span class="math inline">\(g(x)\)</span> to the observed data well; that is,</p>
<p><span class="math display">\[\min \left\{ \text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2 \right\}\]</span></p>
<p>With no constraints, we could generate a function that interpolates all of the <span class="math inline">\(y_i\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_spline_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="dt">min =</span> <span class="op">-</span><span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">1</span>),
  <span class="dt">y =</span> <span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">15</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">rowSums</span>() <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)
)

<span class="kw">ggplot</span>(sim_spline_data, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y))</code></pre></div>
<p><img src="/notes/splines_files/figure-html/sim-perfect-fit-1.png" width="672" /></p>
<p>Clearly such a function would overfit the data. Instead, we want a function <span class="math inline">\(g\)</span> that makes <span class="math inline">\(\text{RSS}\)</span> small, but that is also smooth.</p>
<p>The approach of a smoothing spline is to find the function that minimizes</p>
<p><span class="math display">\[\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g&#39;&#39;(t)^2 dt\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a non-negative tuning parameter. The function that <span class="math inline">\(g\)</span> minimizes is the smoothing spline.</p>
<p>This is the same “Loss + Penalty” form that we used for <a href="/syllabus/selection-regulation/">ridge/lasso/elastic net regression</a>. The second derivative of the function <span class="math inline">\(g(t)\)</span> is a measure of the function’s <strong>roughness</strong>: it is large in absolute value if <span class="math inline">\(g(t)\)</span> is wiggly, and it is close to zero otherwise. By integrating over <span class="math inline">\(g&#39;&#39;(t)^2\)</span>, we capture the total change in <span class="math inline">\(g&#39;(t)\)</span>.</p>
<p>Including this as a weighted penalty forces <span class="math inline">\(g(x)\)</span> to impose smooth constraints. Otherwise the function is not minimized appropriately. As <span class="math inline">\(\lambda\)</span> increases, <span class="math inline">\(g\)</span> will become more smooth. When <span class="math inline">\(\lambda = 0\)</span>, <span class="math inline">\(g(x)\)</span> is a line that perfectly interpolates the original data points. When <span class="math inline">\(\lambda \rightarrow \infty\)</span>, <span class="math inline">\(g\)</span> will be perfectly smooth - it will just be a straight line that passes closely to the training points. In fact, it will be the linear least squares fit line.</p>
<p>What does this function look like?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># smooth spline data points</span>
sim_smooth &lt;-<span class="st"> </span><span class="kw">smooth.spline</span>(sim_spline_data<span class="op">$</span>x, sim_spline_data<span class="op">$</span>y, <span class="dt">df =</span> <span class="dv">6</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>predict <span class="op">%&gt;%</span>
<span class="st">  </span>as_tibble

<span class="kw">ggplot</span>(sim_spline_data, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Cubic spline&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Natural spline&quot;</span>),
              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(x, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> sim_smooth, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Smoothing spline&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y),
       <span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/splines_files/figure-html/sim-spline-compare-1.png" width="672" /></p>
<p>A smoothing spline is strongly related to a natural cubic spline, with a couple notable exceptions. First, <span class="math inline">\(g(x)\)</span> is a natural cubic spline with knots at <span class="math inline">\(x_1, \ldots, x_n\)</span>. That is, there is a knot at every observed value of <span class="math inline">\(X\)</span>. Second, it is a <strong>shrunken</strong> version where the value of <span class="math inline">\(\lambda\)</span> controls the level of shrinkage.</p>
<div id="choosing-the-smoothing-parameter-lambda" class="section level2">
<h2><span class="header-section-number">3.1</span> Choosing the smoothing parameter <span class="math inline">\(\lambda\)</span></h2>
<p>One concern with smoothing splines is the number of degrees of freedom. With <span class="math inline">\(K = n\)</span> knots, we seem to need <span class="math inline">\(n + 4\)</span> degrees of freedom. For a linear regression model, that is not possible.</p>
<p>However, since the tuning parameter <span class="math inline">\(\lambda\)</span> dictates the roughness of the smoothing spline, it also controls the <strong>effective degrees of freedom</strong>. As <span class="math inline">\(\lambda\)</span> increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span>, the effective degrees of freedom decreases from <span class="math inline">\(n\)</span> to <span class="math inline">\(2\)</span>.</p>
<p>Degrees of freedom refer to the number of free parameters in a model, such as the number of coefficients fit in a regression model. Since a smoothing spline has <span class="math inline">\(n\)</span> parameters but each parameter is constrained by the penalty component of the loss function, they don’t carry the same value as a normal parameter. <span class="math inline">\(df_\lambda\)</span> is a measure of the flexibility of the smoothing spline, based on effectively how many degrees of freedom the model consumes. We can write this as</p>
<p><span class="math display">\[\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda \mathbf{y}\]</span></p>
<p>where <span class="math inline">\(\hat{\mathbf{g}}_\lambda\)</span> is the solution to</p>
<p><span class="math display">\[\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g&#39;&#39;(t)^2 dt\]</span></p>
<p>for a particular choice of <span class="math inline">\(\lambda\)</span>. This vector of values can be written as an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{S}_\lambda\)</span> times the response vector <span class="math inline">\(\mathbf{y}\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> The effective degrees of freedom is defined as</p>
<p><span class="math display">\[df_\lambda = \sum_{i=1}^n \{\mathbf{S}_\lambda \}_{ii}\]</span></p>
<p>We do not have to tune the number or location of knots for a smoothing spline - they exist at each <span class="math inline">\(x_1, \ldots, x_n\)</span>. We do need to choose a value for <span class="math inline">\(\lambda\)</span>. We can use cross-validation to determine which value of <span class="math inline">\(\lambda\)</span> generates the smallest test RSS/MSE. However, this can be computationally intensive. Instead, we can use an analytical short-cut for <a href="/notes/cross-validation/#leave-one-out-cross-validation">LOOCV</a>. This short-cut allows us to estimate a single model using the formula</p>
<p><span class="math display">\[\text{RSS}_{cv}(\lambda) = \sum_{i=1}^n (y_i - \hat{g}_\lambda^{(-i)} (x_i))^2 = \sum_{i=1}^n \left[ \frac{y_i - \hat{g}_\lambda (x_i)}{1 - \{ \mathbf{S}_\lambda \}_{ii}} \right]^2\]</span></p>
<ul>
<li><span class="math inline">\(\hat{g}_\lambda^{(-i)} (x_i)\)</span> - fitted value for a smoothing spline evaluated at <span class="math inline">\(x_i\)</span>, fit using all the training observations except the <span class="math inline">\(i\)</span>th observation</li>
<li><span class="math inline">\(\hat{g}_\lambda (x_i)\)</span> - fitted value for a smoothing spline evaluated at <span class="math inline">\(x_i\)</span>, fit using all the training observations</li>
</ul>
<p>To compute the LOOCV fits, we only need to use <span class="math inline">\(\hat{g}_\lambda\)</span>, which is the original model. In truth, most functions use the same approach for LOOCV for all least squares regression model.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>Compare the difference between effective degrees of freedom:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">list</span>(
  <span class="st">`</span><span class="dt">50</span><span class="st">`</span> =<span class="st"> </span><span class="kw">smooth.spline</span>(sim_spline_data<span class="op">$</span>x, sim_spline_data<span class="op">$</span>y, <span class="dt">df =</span> <span class="dv">50</span>),
  <span class="st">`</span><span class="dt">20</span><span class="st">`</span> =<span class="st"> </span><span class="kw">smooth.spline</span>(sim_spline_data<span class="op">$</span>x, sim_spline_data<span class="op">$</span>y, <span class="dt">df =</span> <span class="dv">20</span>),
  <span class="st">`</span><span class="dt">2</span><span class="st">`</span> =<span class="st"> </span><span class="kw">smooth.spline</span>(sim_spline_data<span class="op">$</span>x, sim_spline_data<span class="op">$</span>y, <span class="dt">df =</span> <span class="dv">2</span>),
  <span class="st">`</span><span class="dt">11</span><span class="st">`</span> =<span class="st"> </span><span class="kw">smooth.spline</span>(sim_spline_data<span class="op">$</span>x, sim_spline_data<span class="op">$</span>y)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">map_df</span>(predict, <span class="dt">.id =</span> <span class="st">&quot;df&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">df =</span> <span class="kw">factor</span>(df, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">11</span>, <span class="dv">20</span>, <span class="dv">50</span>),
                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;2&quot;</span>, <span class="st">&quot;11 (CV)&quot;</span>, <span class="st">&quot;20&quot;</span>, <span class="st">&quot;50&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sim_spline_data, <span class="dt">alpha =</span> .<span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">color =</span> df)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y),
       <span class="dt">color =</span> <span class="st">&quot;Effective degrees of freedom&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/splines_files/figure-html/sim-smooth-spline-compare-1.png" width="672" /></p>
</div>
</div>
<div id="multivariate-adaptive-regression-splines-mars" class="section level1">
<h1><span class="header-section-number">4</span> Multivariate adaptive regression splines (MARS)</h1>
<p>MARS is an adaptive procedure for regression which generalizes many of the earlier approaches we saw such as <a href="http://uc-r.github.io/model_selection#stepwise">stepwise regression</a> and <a href="/notes/global-methods/">polynomial regression and step functions</a>. It works well for high-dimensional data (i.e. lots of predictors) and can be performed efficiently with cross-validation techniques.</p>
<div id="basis-functions-1" class="section level2">
<h2><span class="header-section-number">4.1</span> Basis functions</h2>
<p>MARS uses expansions in piecewise linear basis functions of the form <span class="math inline">\((x - t)_+\)</span> and <span class="math inline">\((t - x)_+\)</span>. The <span class="math inline">\(+\)</span> means positive part, so</p>
<p><span class="math display">\[
(x - t)_+ = \begin{cases}
x - t, &amp; \text{if } x &gt; t, \\
0, &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
(t - x)_+ = \begin{cases}
t - x, &amp; \text{if } x &lt; t, \\
0, &amp; \text{otherwise}
\end{cases}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(
  <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;xt&quot;</span>), <span class="dt">fun =</span> <span class="cf">function</span>(x) <span class="kw">ifelse</span>(x <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>, x <span class="op">-</span><span class="st"> </span>.<span class="dv">5</span>, <span class="dv">0</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;tx&quot;</span>), <span class="dt">fun =</span> <span class="cf">function</span>(x) <span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span>.<span class="dv">5</span>, .<span class="dv">5</span> <span class="op">-</span><span class="st"> </span>x, <span class="dv">0</span>),
                <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>,
                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="kw">expression</span>((t <span class="op">-</span><span class="st"> </span>x)[<span class="st">&quot;+&quot;</span>]), <span class="kw">expression</span>((x <span class="op">-</span><span class="st"> </span>t)[<span class="st">&quot;+&quot;</span>]))) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;MARS basis functions&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(t <span class="op">==</span><span class="st"> </span><span class="fl">0.5</span>),
       <span class="dt">x =</span> <span class="kw">expression</span>(x),
       <span class="dt">y =</span> <span class="st">&quot;Basis function&quot;</span>,
       <span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/splines_files/figure-html/mars-hinge-1.png" width="672" /></p>
<p>Each function is piecewise linear (also known as a linear spline), with a knot at the value <span class="math inline">\(t\)</span>. This distinguishes MARS from traditional splines where the piecewise function is a cubic spline and constrained to be smooth at the knots.</p>
<p>We call these two functions a <strong>reflected pair</strong>. The idea is to form reflected pairs for each input <span class="math inline">\(X_j\)</span> with knots at each observed value <span class="math inline">\(x_{ij}\)</span> of that input. The collection of basis functions is therefore</p>
<p><span class="math display">\[C = \{ (X_j - t)_+, (t - X_j)_+ \}\]</span></p>
<p>for</p>
<p><span class="math display">\[
\begin{align}
t &amp;\in \{ x_{1j}, x_{2j}, \ldots, x_{Nj} \} \\
j &amp;= 1, 2, \ldots, p
\end{align}
\]</span></p>
<p>If all the input values are distinct, then there are <span class="math inline">\(2Np\)</span> basis functions altogether.</p>
</div>
<div id="model-construction" class="section level2">
<h2><span class="header-section-number">4.2</span> Model construction</h2>
<p>MARS uses a forward stepwise linear regression approach, but instead of using the original inputs we use functions from the set <span class="math inline">\(C\)</span> and their products. So the model takes on the form</p>
<p><span class="math display">\[f(X) = \beta_0 + \sum_{m=1}^M \beta_m h_m (X)\]</span></p>
<p>where each <span class="math inline">\(h_m (X)\)</span> is a function in <span class="math inline">\(C\)</span> or a product of two or more functions. Given a choice for <span class="math inline">\(h_m\)</span>, the coefficients are estimated using standard linear regrewssion. We start with only the constant function <span class="math inline">\(h_0 (X) = 1\)</span> in the model, and all functions in <span class="math inline">\(C\)</span> are candidate functions. At each stage, we consider as a new basis function pair all products of a function <span class="math inline">\(h_m\)</span> in the model set <span class="math inline">\(M\)</span> with one of the reflected pairs in <span class="math inline">\(C\)</span>. We add to the model <span class="math inline">\(M\)</span> the term of the form</p>
<p><span class="math display">\[\hat{\beta}_{M+1} h_{\mathcal{l}}(X) \times (X_j - t)_+ + \hat{\beta}_{M + 2} h_{\mathcal{l}}(X) \times (t - X_j)_+, h_{\mathcal{l}}\in M\]</span></p>
<p>that produces the largest decrease in training error, where <span class="math inline">\(\hat{\beta}_{M+1}, \hat{\beta}_{M+2}\)</span> are coefficients estimated by least squares. The winning products are added to the model and the process continues until the model set <span class="math inline">\(M\)</span> contains some preset maximum number of terms.</p>
<p>At this point, we have a large model that overfits the training data, so we use a backward deletion process to remove terms that cause the smallest increase in residual squared error. This produces an estimated best model <span class="math inline">\(\hat{f}_\lambda\)</span> of each size (number of terms) <span class="math inline">\(\lambda\)</span>. We use a generalized form of cross-validation to determine the optimal value for <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="application-to-ames-housing-data" class="section level2">
<h2><span class="header-section-number">4.3</span> Application to Ames housing data</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(AmesHousing<span class="op">::</span><span class="kw">make_ames</span>(), <span class="dt">prop =</span> .<span class="dv">7</span>, <span class="dt">strata =</span> <span class="st">&quot;Sale_Price&quot;</span>)
ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)
ames_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(ames_split)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ames_base &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ames_train, <span class="kw">aes</span>(Year_Built, Sale_Price)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> .<span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">{
  ames_base <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;(A) Linear regression&quot;</span>)
  } <span class="op">+</span><span class="st"> </span>{
    ames_base <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="st">&quot;(B) Degree-2 polynomial regression&quot;</span>)
  } <span class="op">+</span><span class="st"> </span>{
    ames_base <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">3</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="st">&quot;(C) Degree-3 polynomial regression&quot;</span>)
  } <span class="op">+</span><span class="st"> </span>{
    ames_base <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">cut_interval</span>(x, <span class="dv">3</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="st">&quot;(D) Step function regression&quot;</span>)
  }</code></pre></div>
<p><img src="/notes/splines_files/figure-html/ames-poly-step-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">year_built_k1 &lt;-<span class="st"> </span><span class="kw">earth</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>Year_Built,  
  <span class="dt">minspan =</span> <span class="op">-</span><span class="dv">1</span>,
  <span class="dt">data =</span> ames_train   
) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction

year_built_k2 &lt;-<span class="st"> </span><span class="kw">earth</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>Year_Built,  
  <span class="dt">minspan =</span> <span class="op">-</span><span class="dv">2</span>,
  <span class="dt">data =</span> ames_train   
) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction

year_built_k3 &lt;-<span class="st"> </span><span class="kw">earth</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>Year_Built,  
  <span class="dt">minspan =</span> <span class="op">-</span><span class="dv">3</span>,
  <span class="dt">data =</span> ames_train   
) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction

year_built_k4 &lt;-<span class="st"> </span><span class="kw">earth</span>(
  Sale_Price <span class="op">~</span><span class="st"> </span>Year_Built,  
  <span class="dt">minspan =</span> <span class="op">-</span><span class="dv">4</span>,
  <span class="dt">data =</span> ames_train   
) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction

{
  ames_base <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> year_built_k1, <span class="kw">aes</span>(<span class="dt">y =</span> fitted), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;(A) One knot&quot;</span>)
} <span class="op">+</span><span class="st"> </span>{
  ames_base <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> year_built_k2, <span class="kw">aes</span>(<span class="dt">y =</span> fitted), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;(B) Two knots&quot;</span>)
} <span class="op">+</span><span class="st"> </span>{
  ames_base <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> year_built_k3, <span class="kw">aes</span>(<span class="dt">y =</span> fitted), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;(C) Three knots&quot;</span>)
} <span class="op">+</span><span class="st"> </span>{
  ames_base <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> year_built_k4, <span class="kw">aes</span>(<span class="dt">y =</span> fitted), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;(4) Four knots&quot;</span>)
}</code></pre></div>
<p><img src="/notes/splines_files/figure-html/ames-mars-knots-1.png" width="672" /></p>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">5</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-02-18                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package       * version    date       lib
##  AmesHousing     0.0.3      2017-12-17 [1]
##  assertthat      0.2.0      2017-04-11 [2]
##  backports       1.1.3      2018-12-14 [2]
##  base64enc       0.1-3      2015-07-28 [2]
##  bayesplot       1.6.0      2018-08-02 [2]
##  bindr           0.1.1      2018-03-13 [2]
##  bindrcpp      * 0.2.2      2018-03-29 [1]
##  blogdown        0.10       2019-01-09 [1]
##  bookdown        0.9        2018-12-21 [1]
##  broom         * 0.5.1      2018-12-05 [2]
##  callr           3.1.1      2018-12-21 [2]
##  cellranger      1.1.0      2016-07-27 [2]
##  class           7.3-15     2019-01-01 [2]
##  cli             1.0.1      2018-09-25 [1]
##  codetools       0.2-16     2018-12-24 [2]
##  colorspace      1.4-0      2019-01-13 [2]
##  colourpicker    1.0        2017-09-27 [2]
##  crayon          1.3.4      2017-09-16 [2]
##  crosstalk       1.0.0      2016-12-21 [2]
##  data.table      1.12.0     2019-01-13 [2]
##  desc            1.2.0      2018-05-01 [2]
##  devtools        2.0.1      2018-10-26 [1]
##  dials         * 0.0.2      2018-12-09 [1]
##  digest          0.6.18     2018-10-10 [1]
##  dplyr         * 0.7.8      2018-11-10 [1]
##  DT              0.5        2018-11-05 [2]
##  dygraphs        1.1.1.6    2018-07-11 [2]
##  earth         * 4.7.0      2019-01-03 [1]
##  evaluate        0.12       2018-10-09 [2]
##  forcats       * 0.3.0      2018-02-19 [2]
##  foreach       * 1.4.4      2017-12-12 [2]
##  fs              1.2.6      2018-08-23 [1]
##  gam           * 1.16       2018-07-20 [2]
##  generics        0.0.2      2018-11-29 [1]
##  ggplot2       * 3.1.0      2018-10-25 [1]
##  ggridges        0.5.1      2018-09-27 [2]
##  glue            1.3.0      2018-07-17 [2]
##  gower           0.1.2      2017-02-23 [2]
##  gridExtra       2.3        2017-09-09 [2]
##  gtable          0.2.0      2016-02-26 [2]
##  gtools          3.8.1      2018-06-26 [2]
##  haven           2.0.0      2018-11-22 [2]
##  here          * 0.1        2017-05-28 [2]
##  hms             0.4.2      2018-03-10 [2]
##  htmltools       0.3.6      2017-04-28 [1]
##  htmlwidgets     1.3        2018-09-30 [2]
##  httpuv          1.4.5.1    2018-12-18 [2]
##  httr            1.4.0      2018-12-11 [2]
##  igraph          1.2.2      2018-07-27 [2]
##  infer         * 0.4.0      2018-11-15 [1]
##  inline          0.3.15     2018-05-18 [2]
##  ipred           0.9-8      2018-11-05 [1]
##  ISLR          * 1.2        2017-10-20 [2]
##  iterators       1.0.10     2018-07-13 [2]
##  janeaustenr     0.1.5      2017-06-10 [2]
##  jsonlite        1.6        2018-12-07 [2]
##  knitr         * 1.21       2018-12-10 [2]
##  labeling        0.3        2014-08-23 [2]
##  later           0.7.5      2018-09-18 [2]
##  lattice       * 0.20-38    2018-11-04 [2]
##  lava            1.6.4      2018-11-25 [2]
##  lazyeval        0.2.1      2017-10-29 [2]
##  lme4            1.1-19     2018-11-10 [2]
##  loo             2.0.0      2018-04-11 [2]
##  lubridate       1.7.4      2018-04-11 [2]
##  magrittr        1.5        2014-11-22 [2]
##  margins       * 0.3.23     2018-05-22 [2]
##  markdown        0.9        2018-12-07 [2]
##  MASS            7.3-51.1   2018-11-01 [2]
##  Matrix          1.2-15     2018-11-01 [2]
##  matrixStats     0.54.0     2018-07-23 [2]
##  memoise         1.1.0      2017-04-21 [2]
##  mime            0.6        2018-10-05 [1]
##  miniUI          0.1.1.1    2018-05-18 [2]
##  minqa           1.2.4      2014-10-09 [2]
##  modelr          0.1.2      2018-05-11 [2]
##  munsell         0.5.0      2018-06-12 [2]
##  nlme            3.1-137    2018-04-07 [2]
##  nloptr          1.2.1      2018-10-03 [2]
##  nnet            7.3-12     2016-02-02 [2]
##  parsnip       * 0.0.1      2018-11-12 [1]
##  patchwork     * 0.0.1      2018-09-06 [1]
##  pillar          1.3.1      2018-12-15 [2]
##  pkgbuild        1.0.2      2018-10-16 [1]
##  pkgconfig       2.0.2      2018-08-16 [2]
##  pkgload         1.0.2      2018-10-29 [1]
##  plotmo        * 3.5.2      2019-01-02 [1]
##  plotrix       * 3.7-4      2018-10-03 [2]
##  plyr            1.8.4      2016-06-08 [2]
##  prediction      0.3.6.1    2018-12-04 [2]
##  prettyunits     1.0.2      2015-07-13 [2]
##  pROC            1.13.0     2018-09-24 [1]
##  processx        3.2.1      2018-12-05 [2]
##  prodlim         2018.04.18 2018-04-18 [2]
##  promises        1.0.1      2018-04-13 [2]
##  ps              1.3.0      2018-12-21 [2]
##  purrr         * 0.3.0      2019-01-27 [2]
##  R6              2.3.0      2018-10-04 [1]
##  rcfss         * 0.1.5      2019-01-24 [1]
##  Rcpp            1.0.0      2018-11-07 [1]
##  readr         * 1.3.1      2018-12-21 [2]
##  readxl          1.2.0      2018-12-19 [2]
##  recipes       * 0.1.4      2018-11-19 [1]
##  remotes         2.0.2      2018-10-30 [1]
##  reshape2        1.4.3      2017-12-11 [2]
##  rlang           0.3.1      2019-01-08 [1]
##  rmarkdown       1.11       2018-12-08 [2]
##  rpart           4.1-13     2018-02-23 [1]
##  rprojroot       1.3-2      2018-01-03 [2]
##  rsample       * 0.0.4      2019-01-07 [1]
##  rsconnect       0.8.13     2019-01-10 [2]
##  rstan           2.18.2     2018-11-07 [2]
##  rstanarm        2.18.2     2018-11-10 [2]
##  rstantools      1.5.1      2018-08-22 [2]
##  rstudioapi      0.9.0      2019-01-09 [1]
##  rvest           0.3.2      2016-06-17 [2]
##  scales        * 1.0.0      2018-08-09 [1]
##  sessioninfo     1.1.1      2018-11-05 [1]
##  shiny           1.2.0      2018-11-02 [2]
##  shinyjs         1.0        2018-01-08 [2]
##  shinystan       2.5.0      2018-05-01 [2]
##  shinythemes     1.1.2      2018-11-06 [2]
##  SnowballC       0.6.0      2019-01-15 [2]
##  StanHeaders     2.18.0-1   2018-12-13 [2]
##  stringi         1.2.4      2018-07-20 [2]
##  stringr       * 1.3.1      2018-05-10 [2]
##  survival        2.43-3     2018-11-26 [2]
##  TeachingDemos * 2.10       2016-02-12 [2]
##  testthat        2.0.1      2018-10-13 [2]
##  threejs         0.3.1      2017-08-13 [2]
##  tibble        * 2.0.1      2019-01-12 [2]
##  tidymodels    * 0.0.2      2018-11-27 [1]
##  tidyposterior   0.0.2      2018-11-15 [1]
##  tidypredict     0.3.0      2019-01-10 [1]
##  tidyr         * 0.8.2.9000 2019-02-11 [1]
##  tidyselect      0.2.5      2018-10-11 [1]
##  tidytext        0.2.0      2018-10-17 [1]
##  tidyverse     * 1.2.1      2017-11-14 [2]
##  timeDate        3043.102   2018-02-21 [2]
##  titanic       * 0.1.0      2015-08-31 [2]
##  tokenizers      0.2.1      2018-03-29 [2]
##  usethis         1.4.0      2018-08-14 [1]
##  withr           2.1.2      2018-03-15 [2]
##  xfun            0.4        2018-10-23 [1]
##  xml2            1.2.0      2018-01-24 [2]
##  xtable          1.8-3      2018-08-29 [2]
##  xts             0.11-2     2018-11-05 [2]
##  yaml            2.2.0      2018-07-25 [2]
##  yardstick     * 0.0.2      2018-11-05 [1]
##  zoo             1.8-4      2018-09-19 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  local                               
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (tidyverse/tidyr@0b27690)    
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That is, each interval contains the same number of observations rather than each interval having an equal width.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>See ESL ch 5.4.1 for the technical definition of <span class="math inline">\(\mathbf{S}_\lambda\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This trick does not work for GLMs or related modeling strategies.<a href="#fnref3">↩</a></p></li>
</ol>
</div>
