---
title: Latent Dirchlet allocation
date: 2019-03-06T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Dimension reduction
    weight: 2
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(tidymodels)
library(patchwork)
library(here)
library(tidytext)
library(tm)
library(topicmodels)
library(rjson)
library(furrr)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}

# Topic modeling

Text documents can also be modeled and explored **thematically**. For instance, [David Blei](http://delivery.acm.org/10.1145/2140000/2133826/p77-blei.pdf) proposes searching through the complete history of the New York Times. Broad themes may relate to the individual sections in the paper (foreign policy, national affairs, sports) but there might be specific themes within or across these sections (Chinese foreign policy, the conflict in the Middle East, the U.S.'s relationship with Russia). If the documents are grouped by these themes, we could track the evolution of the NYT's reporting on these issues over time, or examine how discussion of different themes intersects.

In order to do this, we would need detailed information on the theme of every article. Hand-coding this corpus would be exceedingly time-consuming, not to mention would requiring knowing the thematic structure of the documents before one even begins coding. For the vast majority of corpa, this is not a feasible approach.

Instead, we can use **probabilistic topic models**, statistical algorithms that analyze words in original text documents to uncover the thematic structure of the both the corpus and individual documents themselves. They do not require any hand coding or labeling of the documents prior to analysis - instead, the algorithms emerge from the analysis of the text.

# Latent Dirichlet allocation

LDA assumes that each document in a corpus contains a mix of topics that are found throughout the entire corpus. The topic structure is hidden - we can only observe the documents and words, not the topics themselves. Because the structure is hidden (also known as **latent**), this method seeks to infer the topic structure given the known words and documents.

## Food and animals

Suppose you have the following set of sentences:

1. I ate a banana and spinach smoothie for breakfast.
1. I like to eat broccoli and bananas.
1. Chinchillas and kittens are cute.
1. My sister adopted a kitten yesterday.
1. Look at this cute hamster munching on a piece of broccoli.

Latent Dirichlet allocation is a way of automatically discovering **topics** that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like

* Sentences 1 and 2: 100% Topic A
* Sentences 3 and 4: 100% Topic B
* Sentence 5: 60% Topic A, 40% Topic B

* Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, ...
* Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, ...

You could infer that topic A is a topic about **food**, and topic B is a topic about **cute animals**. But LDA does not explicitly identify topics in this manner. All it can do is tell you the probability that specific words are associated with the topic.

## An LDA document structure

LDA represents documents as mixtures of topics that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you

* Decide on the number of words $N$ the document will have
* Choose a topic mixture for the document (according to a [Dirichlet probability distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) over a fixed set of $K$ topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals.
* Generate each word in the document by:
    * First picking a topic (according to the distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).
    * Then using the topic to generate the word itself (according to the topic's multinomial distribution). For instance, the food topic might output the word "broccoli" with 30% probability, "bananas" with 15% probability, and so on.

Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.

### Food and animals

How could we have generated the sentences in the previous example? When generating a document $D$:

* Decide that $D$ will be 1/2 about food and 1/2 about cute animals.
* Pick 5 to be the number of words in $D$.
* Pick the first word to come from the food topic, which then gives you the word "broccoli".
* Pick the second word to come from the cute animals topic, which gives you "panda".
* Pick the third word to come from the cute animals topic, giving you "adorable".
* Pick the fourth word to come from the food topic, giving you "cherries".
* Pick the fifth word to come from the food topic, giving you "eating".

So the document generated under the LDA model will be "broccoli panda adorable cherries eating" (remember that LDA uses a bag-of-words model).

## Learning topic structure through LDA

Now suppose you have a set of documents. You've chosen some fixed number of $K$ topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this? One way (known as collapsed [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)) is the following:

* Go through each document, and randomly assign each word in the document to one of the $K$ topics
* Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics. But because it's random, this is not a very accurate structure.
* To improve on them, for each document $d$:
    * Go through each word $w$ in $d$
        * And for each topic $t$, compute two things:
            1. The proportion of words in document $d$ that are currently assigned to topic $t$ - $p(t | d)$
            1. The proportion of assignments to topic $t$ over all documents that come from this word $w$ - $p(w | t)$
        * Reassign $w$ a new topic, where you choose topic $t$ with probability $p(t|d) \times p(w|t)$ - this is the probability that topic $t$ generated word $w$
        * In other words, in this step, we're assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.
* After repeating the previous step a large number of times (really large number of times, like a minimum of 10,000), you'll eventually reach a roughly steady state where your assignments are pretty good
* You can use these assignments to estimate two things:
    1. The topic mixtures of each document (by counting the proportion of words assigned to each topic within that document)
    1. The words associated to each topic (by counting the proportion of words assigned to each topic overall)
    
Frequently when using LDA, you don't actually know the underlying topic structure of the documents. **Generally that is why you are using LDA to analyze the text in the first place**. LDA is still useful in these instances, but we have to perform additional tests and analysis to confirm that the topic structure uncovered by LDA is a good structure.

# `USCongress`

## Get documents

```{r get-docs}
# get USCongress data
data(USCongress, package = "RTextTools")

# topic labels
major_topics <- tibble(
  major = c(1:10, 12:21, 99),
  label = c("Macroeconomics", "Civil rights, minority issues, civil liberties",
            "Health", "Agriculture", "Labor and employment", "Education", "Environment",
            "Energy", "Immigration", "Transportation", "Law, crime, family issues",
            "Social welfare", "Community development and housing issues",
            "Banking, finance, and domestic commerce", "Defense",
            "Space, technology, and communications", "Foreign trade",
            "International affairs and foreign aid", "Government operations",
            "Public lands and water management", "Other, miscellaneous")
)

(congress <- as_tibble(USCongress) %>%
    mutate(text = as.character(text)) %>%
    left_join(major_topics))
```

## Convert to tidy text data frame

```{r convert-tidytext, dependson = "get-docs"}
(congress_tokens <- congress %>%
   unnest_tokens(output = word, input = text) %>%
   # remove numbers
   filter(!str_detect(word, "^[0-9]*$")) %>%
   # remove stop words
   anti_join(stop_words) %>%
   # stem the words
   mutate(word = SnowballC::wordStem(word)))
```

## Convert to document-term matrix

```{r dtm, dependson = "convert-tidytext"}
# remove terms with low tf-idf for future LDA model
(congress_dtm <- congress_tokens %>%
  count(major, word) %>%
  bind_tf_idf(term = word, document = major, n = n) %>%
  group_by(major) %>%
  top_n(40, wt = tf_idf) %>%
  ungroup %>%
  count(word) %>%
  select(-n) %>%
  left_join(congress_tokens) %>%
  # get count of each token in each document
  count(ID, word) %>%
  # create a document-term matrix with all features and tf weighting
  cast_dtm(document = ID, term = word, value = n))
```

## See overall structure

```{r bind-tf-idf, dependson = "convert-tidytext"}
(congress_tfidf <- congress_tokens %>%
   count(label, word) %>%
   bind_tf_idf(term = word, document = label, n = n))
```

```{r plot-tf-idf, dependson = "bind-tf-idf"}
# sort the data frame and convert word to a factor column
plot_congress <- congress_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

# graph the top 10 tokens for 4 categories
plot_congress %>%
  filter(label %in% c("Macroeconomics",
                      "Civil rights, minority issues, civil liberties",
                      "Health", "Education")) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ label, scales = "free") +
  coord_flip()
```

## Build a 20 topic LDA model

```{r congress-20-topic, dependson = "dtm"}
congress_lda <- LDA(congress_dtm, k = 20, control = list(seed = 1234))
congress_lda
```

## Compare LDA to supervised structure

```{r congress-20-topn, fig.asp = 2, dependson = "congress-20-topic"}
congress_lda_td <- tidy(congress_lda)

top_terms <- congress_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip()
```

Do we see unsupervised topics that match the supervised classification scheme?

## Document classification

```{r uscongress-gamma, dependson = "congress-20-topic"}
congress_gamma <- tidy(congress_lda, matrix = "gamma")
congress_gamma
```

```{r congress-model-compare, dependson = "uscongress-gamma"}
congress_tokens %>%
  count(label, word) %>%
  bind_tf_idf(term = word, document = label, n = n) %>%
  group_by(label) %>%
  top_n(40, wt = tf_idf) %>%
  ungroup %>%
  count(word) %>%
  select(-n) %>%
  left_join(congress_tokens) %>%
  distinct(ID) %>%
  left_join(congress) %>%
  mutate(document = as.character(row_number())) %>%
  filter(label %in% c("Macroeconomics",
                      "Civil rights, minority issues, civil liberties",
                      "Health", "Education")) %>%
  left_join(congress_gamma) %>%
  na.omit %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ label) +
  labs(x = "LDA topic",
       y = expression(gamma))
```

Do the policy agendas topic codes map onto the 20-topic LDA model? Not well. No clear and distinct topics for this subset at least. Substantive implications?

# Associated Press articles

The `topicmodels` package includes a document-term matrix of a sample of articles published by the Associated Press in 1992. Let's load them into R and estimate a series of LDA models.

```{r associated_press}
data("AssociatedPress", package = "topicmodels")

# tidy and remove stop words
ap_td <- tidy(AssociatedPress)
```

```{r ap_stopwords, dependson = "associated_press"}
ap_dtm <- ap_td %>%
  anti_join(stop_words, by = c(term = "word")) %>%
  cast_dtm(document, term, count)
ap_dtm
```

## Selecting $k$

Remember that for LDA, you need to specify in advance the number of topics in the underlying topic structure.

### $k=4$

Let's estimate an LDA model for the Associated Press articles, setting $k=4$.

```{r ap_topic_4, dependson = "associated_press"}
ap_lda <- LDA(ap_dtm, k = 4, control = list(seed = 1234))
ap_lda
```

What do the top terms for each of these topics look like?

```{r ap_4_topn, dependson = "ap_topic_4"}
ap_lda_td <- tidy(ap_lda)

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  coord_flip()
```

Fair enough. The four topics generally look to describe:

### $k=12$

What happens if we set $k=12$? How do our results change?

```{r ap_topic_12, dependson = "associated_press"}
ap_lda <- LDA(ap_dtm, k = 12, control = list(seed = 1234))
ap_lda
```

```{r ap_12_topn, dependson="ap_topic_12"}
ap_lda_td <- tidy(ap_lda)

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```

Hmm. Well, these topics appear to be more specific, yet not as easily decodeable. Alas, this is the problem with LDA. Several different values for $k$ may be plausible, but by increasing $k$ we sacrifice clarity. Is there any statistical measure which will help us determine the optimal number of topics?

## Perplexity

Well, sort of. Some aspects of LDA are driven by gut-thinking (or perhaps [truthiness](http://www.cc.com/video-clips/63ite2/the-colbert-report-the-word---truthiness)). However we can have some help. [**Perplexity**](https://en.wikipedia.org/wiki/Perplexity) is a statistical measure of how well a probability model predicts a sample. As applied to LDA, for a given value of $k$, you estimate the LDA model. Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents.

`topicmodels` includes the function `perplexity()` which calculates this value for a given model.

```{r ap_12_perplex, dependson="ap_topic_12"}
perplexity(ap_lda)
```

However, the statistic is somewhat meaningless on its own. The benefit of this statistic comes in comparing perplexity across different models with varying $k$s. The model with the lowest perplexity is generally considered the "best".

Let's estimate a series of LDA models on the Associated Press dataset. Here I make use of `purrr` and the `map()` functions to iteratively generate a series of LDA models for the AP corpus, using a different number of topics in each model.^[Note that LDA can quickly become CPU and memory intensive as you scale up the size of the corpus and number of topics. Replicating this analysis on your computer may take a long time (i.e. minutes or even hours). It is very possible you may not be able to replicate this analysis on your machine. If so, you need to reduce the amount of text, the number of models, or offload the analysis to the [Research Computing Center](https://rcc.uchicago.edu/).]

```{r ap_lda_compare, dependson="associated_press"}
n_topics <- c(2, 4, 10, 20, 50, 100)

if(file.exists(here("static", "data", "ap_lda_compare.Rdata"))){
  load(file = here("static", "data", "ap_lda_compare.Rdata"))
} else{
  plan(multiprocess)
  
  ap_lda_compare <- n_topics %>%
    future_map(LDA, x = ap_dtm, control = list(seed = 1234))
  save(ap_lda_compare, file = here("static", "data", "ap_lda_compare.Rdata"))
}
```

```{r ap_lda_compare_viz, dependson="ap_lda_compare"} 
tibble(k = n_topics,
           perplex = map_dbl(ap_lda_compare, perplexity)) %>%
  ggplot(aes(k, perplex)) +
  geom_point() +
  geom_line() +
  labs(title = "Evaluating LDA topic models",
       subtitle = "Optimal number of topics (smaller is better)",
       x = "Number of topics",
       y = "Perplexity")
```

It looks like the 100-topic model has the lowest perplexity score. What kind of topics does this generate? Let's look just at the first 12 topics produced by the model (`ggplot2` has difficulty rendering a graph for 100 separate facets):

```{r ap_100_topn, dependson="ap_lda_compare"}
ap_lda_td <- tidy(ap_lda_compare[[6]])

top_terms <- ap_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  filter(topic <= 12) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 3) +
  coord_flip()
```

We are getting even more specific topics now. The question becomes how would we present these results and use them in an informative way? Not to mention perplexity was still dropping at $k=100$ - would $k=200$ generate an even lower perplexity score?^[I tried to estimate this model, but my computer was taking too long.]

Again, this is where your intuition and domain knowledge as a researcher is important. You can use perplexity as one data point in your decision process, but a lot of the time it helps to simply look at the topics themselves and the highest probability words associated with each one to determine if the structure makes sense. If you have a known topic structure you can compare it to (such as the books example above), this can also be useful.

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* @james2013introduction
* @friedman2001elements
