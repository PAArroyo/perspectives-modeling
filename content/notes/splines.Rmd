---
title: Regression splines
date: 2019-02-18T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Moving beyond linearity
    weight: 2
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(tidymodels)
library(rcfss)
library(titanic)
library(knitr)
library(splines)
library(ISLR)
library(lattice)
library(gam)
library(here)
library(patchwork)
library(margins)
library(earth)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}

# Basis functions

**Basis functions** are a family of functions or transformations applied to a variable $X$: $b_1(X), b_2(X), \ldots, b_K(X)$. Instead of fitting the linear model to $X$, we fit the model:

$$y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \ldots + \beta_K b_K(x_i) + \epsilon_i$$

The functional form of the basis function is determined in advanced and is fixed and known. [Polynomial and step functions](/notes/global-methods/) are specific types of basis functions. For polynomial regression, $b_j(x_i) = x_i^j$ where $j$ is the polynomial degree. Since this linear model is just a transformation of the predictors $X_i$, we can use least squares to estimate the model and apply all the standard statistical inferential techniques to evaluate and interpret the model.

# Regression splines

**Regression splines** extend polynomial transformations and piecewise constant regression by fitting separate polynomial functions over different regions of $X$.

## Piecewise polynomials

**Piecewise polynomial regression** fits separate low-degree polynomials for different regions of $X$. For instance, a cubic piecewise polynomial regression model takes the form:

$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$

where the $\beta$s are now vectors with different values for different parts of the range of $X$. The points where the parameters change are called **knots**.

The special case of a piecewise cubic polynomial with 0 knots is just an ordinary cubic polynomial:

$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$$

Likewise, the special case of a piecewise constant polynomial (i.e. polynomial with degree $0$) is piecewise constant regression. 

A piecewise cubic polynomial with a single knot at point $c$ takes the form:

$$y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i & \text{if } x_i < c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i & \text{if } x_i \geq c
   \end{cases}$$
   
In essence, we fit two separate polynomial functions to the data. Each function is estimated using least squares applied to the individual functions and subsets of data. The more knots $K$ used, the more flexible the piecewise polynomial.

While the example here uses a cubic (third-order) polynomial, we don't have to use it. We could select any higher-order degree for the polynomial.

Of course this leads to a somewhat odd and discontinuous function:

```{r sim-piecewise, echo = FALSE}
# simulate data
sim_piece <- tibble(
  x = runif(100, 0, 10),
  y = ifelse(x < 5,
             .05 * x + .05 * x^2 + .05 * x^3 + rnorm(100, 0, 3),
             .1 * x + .1 * x^2 - .05 * x^3 + rnorm(100, 0, 3))
)

# estimate models
sim_piece_mod1 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece, subset = x < 5)
sim_piece_mod2 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece, subset = x >= 5)

# draw the plot
sim_piece_grid <- tibble(
  x = seq(0, 10, by = .001)
)

bind_rows(
  sim_piece_mod1 = augment(sim_piece_mod1, newdata = sim_piece_grid),
  sim_piece_mod2 = augment(sim_piece_mod2, newdata = sim_piece_grid),
  .id = "model"
) %>%
  filter((x < 5 & model == "sim_piece_mod1") |
           (x >=5 & model == "sim_piece_mod2")) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_piece) +
  geom_line(aes(y = .fitted, color = model), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  labs(title = "Piecewise cubic regression",
       x = "X",
       y = "Y")
```

## Constraints and splines

Instead of a discontinuous piecewise function, we'd rather have something continuous. That is, we impose the **constraint** that the fitted curve must be continuous.

```{r sim-spline, echo = FALSE}
###### very hackish implementation
# simulate data
sim_piece_cont <- sim_piece %>%
  mutate(y = ifelse(x < 5,
                    15 + .05 * x - .5 * x^2 - .05 * x^3,
                    .05 * x + .1 * x^2 - .05 * x^3))

# estimate models
sim_piece_cont_mod1 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece_cont, subset = x < 5)
sim_piece_cont_mod2 <- glm(y ~ poly(x, 3, raw = TRUE), data = sim_piece_cont, subset = x >= 5)

# draw the plot
bind_rows(
  sim_piece_cont_mod1 = augment(sim_piece_cont_mod1, newdata = sim_piece_grid),
  sim_piece_cont_mod2 = augment(sim_piece_cont_mod2, newdata = sim_piece_grid),
  .id = "model"
) %>%
  filter((x < 5 & model == "sim_piece_cont_mod1") |
           (x >=5 & model == "sim_piece_cont_mod2")) %>%
  ggplot() +
  geom_point(data = sim_piece %>%
               mutate(y = ifelse(x < 5,
                                 15 + .05 * x - .5 * x^2 - .05 * x^3 + rnorm(100, 0, 3),
                                 .05 * x + .1 * x^2 - .05 * x^3) + rnorm(100, 0, 3)), aes(x, y)) +
  geom_line(aes(x, .fitted, color = model), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  scale_color_brewer(type = "qual", guide = FALSE) +
  labs(title = "Continuous piecewise cubic regression",
       x = "X",
       y = "Y")
```

But notice that this constraint is insufficient. Now the function is continuous, but still looks unnatural because of the V-shaped join. We should add two additional constraints: not only should the fitted curve be continuous, but the first and second **derivatives** should also be continuous at the knot. This will generate a fitted curve that is continuous and **smooth**.

```{r sim-spline-smooth, echo = FALSE}
# estimate models
sim_piece_smooth <- glm(y ~ bs(x, knots = c(5)), data = sim_piece)

# draw the plot
augment(sim_piece_smooth, newdata = sim_piece) %>%
  ggplot(aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = .fitted), size = 1) +
  geom_vline(xintercept = 5, linetype = 2, color = "grey") +
  labs(title = "Cubic spline",
       x = "X",
       y = "Y")
```

By increasing the number of knots, we will increase the flexibility of the resulting spline:

```{r sim-spline-smooth-5, echo = FALSE}
tibble(
  terms = c(1, 5, 10),
  models = map(terms, ~ glm(y ~ bs(x, df = . + 3), data = sim_piece)),
  pred = map(models, augment, newdata = sim_piece)
) %>%
  unnest(pred) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_piece, alpha = .2) +
  geom_line(aes(y = .fitted, color = factor(terms))) +
  scale_color_brewer(type = "qual") +
  labs(title = "Cubic spline",
       x = "X",
       y = "Y",
       color = "Knots")
```

## Spline basis representation

Regression splines can be represented as basis functions, which identifies how they can be estimated using OLS and still fit it under the constraint that it and its derivatives can be continuous. A cubic spline with $K$ knots can be modeled as

$$y_i = \beta_0 + \beta_1 b_1 (x_i) + \beta_2 b_2 (x_i) + \cdots + \beta_{K + 3} b_{K + 3} (x_i) + \epsilon_i$$

for an appropriate choice of basis functions $b_1, b_2, \ldots, b_{K+3}$$.^[$K$ knots with a polynomial degree of $3$.]

The basis function for a cubic spline is to start with a basis for a cubic polynomial and then add one **truncated power basis** function per knot. A truncated power basis function is defined as:

$$
h(x, \zeta) = (x - \zeta)_+^3 = 
\begin{cases} 
  (x - \zeta)^3 & \text{if } x > \zeta \\
  0 & \text{otherwise}
\end{cases}
$$

where $\zeta$ is the knot. Adding a term of the form $\beta_4 h(x, \zeta)$ to this model will lead to a discontinuity in only the third derivative at $\zeta$. The function remains continuous, with continuous first and second derivatives, at each of the knots.

So fitting a cubic spline to a data set with $K$ knots requires least squares with an intercept and $3 + K$ predictors

$$X, X^2, X^3, h(X, \zeta_1), h(X, \zeta_2), \ldots, h(X, \zeta_K)$$

where $\zeta_1, \ldots, \zeta_K$ are the knots. This amounts to estimating a total of $K + 4$ regression coefficients, and so fitting a cubic spline with $K$ knots uses $K+4$ degrees of freedom.

Notice how the model matrix is constructed when constructing a cubic spline with $K = 4$ knots:

```{r basis-spline-output}
sim_piece %>%
  select(x) %>%
  bind_cols(splines = bs(sim_piece$x, df = 7) %>%
              as_tibble)
```

## Basis splines vs. natural splines

One drawback to the splines as currently presented (known as **basis splines**) is that they have high variance at the outer range of the predictors. Consider a basis spline with 3 knots applied to the [voting and age logistic regression model](/notes/global-methods/#age-and-voting):

```{r mhealth}
mh <- read_csv(here("static", "data", "mental_health.csv")) %>%
  na.omit
```

```{r vote-age-basis-spline, dependson = "mhealth"}
# estimate model
glm(vote96 ~ bs(age, df = 6), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  labs(title = "Predicted probability of voting",
       subtitle = "Basis spline with 3 knots",
       x = "Age",
       y = "Predicted probability of voting")
```

Notice the 95% confidence interval balloons at both low and high values for age. To control for this, we impose **boundary constraints**: the function is required to be linear at the boundary (the region where $X$ is smaller than the smallest knot, or larger than the largest knot). This type of spline is known as a **natural spline**. This leads to more stable estimates at the boundaries.

```{r vote-age-natural-spline, dependson = "mhealth"}
# estimate basis spline model
vote_age_basis <- glm(vote96 ~ bs(age, df = 6), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)
vote_age_natural <- glm(vote96 ~ ns(age, df = 6), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)

bind_rows(
  Basis = vote_age_basis,
  Natural = vote_age_natural,
  .id = "model"
) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals, color = model)) +
  geom_line(aes(y = upper, color = model), linetype = 2) +
  geom_line(aes(y = lower, color = model), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  scale_color_brewer(type = "qual") +
  labs(title = "Predicted probability of voting",
       x = "Age",
       y = "Predicted probability of voting",
       color = "Spline") +
  theme(legend.position = "bottom")
```

## Choosing the number and location of knots

Typically knots are placed in a uniform fashion; that is, the cutpoints $c$ are determined by first identifying the number of knots $K$ for the model, then partitioning $X$ into uniform quantiles. So if we fit a cubic regression spline with 5 knots on the voting and age logistic regression, it would divide `age` into 5 equal quantiles^[That is, each interval contains the same number of observations rather than each interval having an equal width.] and estimate the cubic spline function for each quantile:

```{r vote-spline}
# estimate model
glm(vote96 ~ ns(age, df = 8), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_vline(xintercept = attr(bs(mh$age, df = 8), "knots"),
             linetype = 2, color = "blue") +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  labs(title = "Predicted probability of voting",
       subtitle = "Natural spline with 5 knots",
       x = "Age",
       y = "Predicted probability of voting")
```

But this still leaves unaddressed the matter of how many knots should we use? Or how many degrees should each polynomial be? While theory should still be our guide, we can also use [cross-validation](/notes/cross-validation/) to determine the optimal number of knots and/or polynomial degrees. For our voting and age model, we can estimate $10$-fold CV MSE for varying numbers of knots for a cubic natural spline:

```{r vote-cv}
# function to simplify things
vote_spline <- function(splits, df = NULL){
  # estimate the model on each fold
  model <- glm(vote96 ~ ns(age, df = df),
                data = analysis(splits))
  
  model_acc <- augment(model, newdata = assessment(splits)) %>%
    accuracy(truth = factor(vote96), estimate = factor(round(.fitted)))
  
  mean(model_acc$.estimate)
}

tune_over_knots <- function(splits, knots){
  vote_spline(splits, df = knots + 3)
}

# estimate CV error for knots in 0:25
results <- vfold_cv(mh, v = 10)

expand(results, id, knots = 1:25) %>%
  left_join(results) %>%
  mutate(acc = map2_dbl(splits, knots, tune_over_knots)) %>%
  group_by(knots) %>%
  summarize(acc = mean(acc)) %>%
  mutate(err = 1 - acc) %>%
  ggplot(aes(knots, err)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Optimal number of knots for natural cubic spline regression",
       x = "Knots",
       y = "10-fold CV error")
```

These results (weakly) suggest the optimal number of knots is around 7. The resulting model produced by these parameters is:

```{r vote-optimal-mod}
glm(vote96 ~ ns(age, df = 10), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  geom_vline(xintercept = attr(bs(mh$age, df = 10), "knots"),
             linetype = 2, color = "blue") +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  labs(title = "Predicted probability of voting",
       subtitle = "Natural spline with 9 knots",
       x = "Age",
       y = "Predicted probability of voting")
```

## Comparison to polynomial regression

```{r vote-spline-poly, dependson = "mhealth"}
# estimate natural spline model with df = 15
vote_age_spline <- glm(vote96 ~ ns(age, df = 15), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)
vote_age_poly <- glm(vote96 ~ poly(age, degree = 15), data = mh, family = binomial) %>%
  cplot("age", what = "prediction", n = 101, draw = FALSE)

bind_rows(
  `Natural cubic spline` = vote_age_spline,
  `Polynomial` = vote_age_poly,
  .id = "model"
) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals, color = model)) +
  # geom_line(aes(y = upper, color = model), linetype = 2) +
  # geom_line(aes(y = lower, color = model), linetype = 2) +
  # geom_hline(yintercept = 0, linetype = 1) +
  geom_rug(data = filter(mh, vote96 == 0), aes(age), alpha = .02, sides = "b") +
  geom_rug(data = filter(mh, vote96 == 1), aes(age), alpha = .02, sides = "t") +
  scale_color_brewer(type = "qual") +
  labs(title = "Predicted probability of voting",
       x = "Age",
       y = "Predicted probability of voting",
       color = NULL) +
  theme(legend.position = "bottom")
```

Splines are generally superior to polynomial regression because their flexibility is defined primarily in terms of the number of knots in the model, rather than the degree of the highest polynomial term (e.g. $X^{15}$), whereas polynomial regression models must use high degree polynomials to achieve similar flexibility. This leads to more overfitting and wilder behavior, especially in the boundary regions. In the comparison above, we fit a natural cubic spline with 15 degrees of freedom (aka 12 knots) and compare it to a degree-15 polynomial regression. Notice the undesirable behavior of the polynomial regression model at the boundaries relative to the natural cubic spline.

# Smoothing splines

**Smoothing splines** take a different approach to producing a spline. The goal is to fit a function $g(x)$ to the observed data well; that is,

$$\min \left\{ \text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2 \right\}$$

With no constraints, we could generate a function that interpolates all of the $y_i$:

```{r sim-perfect-fit}
sim_spline_data <- tibble(
  x = runif(100, min = -1, max = 1),
  y = poly(x, degree = 15, raw = TRUE) %>%
    rowSums() + rnorm(100)
)

ggplot(sim_spline_data, aes(x, y)) +
  geom_point() +
  geom_line() +
  labs(x = expression(X),
       y = expression(Y))
```

Clearly such a function would overfit the data. Instead, we want a function $g$ that makes $\text{RSS}$ small, but that is also smooth.

The approach of a smoothing spline is to find the function that minimizes

$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$

where $\lambda$ is a non-negative tuning parameter. The function that $g$ minimizes is the smoothing spline.

This is the same "Loss + Penalty" form that we used for [ridge/lasso/elastic net regression](/syllabus/selection-regulation/). The second derivative of the function $g(t)$ is a measure of the function's **roughness**: it is large in absolute value if $g(t)$ is wiggly, and it is close to zero otherwise. By integrating over $g''(t)^2$, we capture the total change in $g'(t)$.

Including this as a weighted penalty forces $g(x)$ to impose smooth constraints. Otherwise the function is not minimized appropriately. As $\lambda$ increases, $g$ will become more smooth. When $\lambda = 0$, $g(x)$ is a line that perfectly interpolates the original data points. When $\lambda \rightarrow \infty$, $g$ will be perfectly smooth - it will just be a straight line that passes closely to the training points. In fact, it will be the linear least squares fit line.

What does this function look like?

```{r sim-spline-compare, dependson = "sim-perfect-fit"}
# smooth spline data points
sim_smooth <- smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 6) %>%
  predict %>%
  as_tibble

ggplot(sim_spline_data, aes(x, y)) +
  geom_point() +
  geom_smooth(aes(color = "Cubic spline"),
              method = "lm", formula = y ~ bs(x, df = 6), se = FALSE) +
  geom_smooth(aes(color = "Natural spline"),
              method = "lm", formula = y ~ ns(x, df = 6), se = FALSE) +
  geom_line(data = sim_smooth, aes(color = "Smoothing spline"), size = 1) +
  scale_color_brewer(type = "qual") +
  labs(x = expression(X),
       y = expression(Y),
       color = NULL) +
  theme(legend.position = "bottom")
```

A smoothing spline is strongly related to a natural cubic spline, with a couple notable exceptions. First, $g(x)$ is a natural cubic spline with knots at $x_1, \ldots, x_n$. That is, there is a knot at every observed value of $X$. Second, it is a **shrunken** version where the value of $\lambda$ controls the level of shrinkage.

## Choosing the smoothing parameter $\lambda$

One concern with smoothing splines is the number of degrees of freedom. With $K = n$ knots, we seem to need $n + 4$ degrees of freedom. For a linear regression model, that is not possible.

However, since the tuning parameter $\lambda$ dictates the roughness of the smoothing spline, it also controls the **effective degrees of freedom**. As $\lambda$ increases from $0$ to $\infty$, the effective degrees of freedom decreases from $n$ to $2$.

Degrees of freedom refer to the number of free parameters in a model, such as the number of coefficients fit in a regression model. Since a smoothing spline has $n$ parameters but each parameter is constrained by the penalty component of the loss function, they don't carry the same value as a normal parameter. $df_\lambda$ is a measure of the flexibility of the smoothing spline, based on effectively how many degrees of freedom the model consumes. We can write this as

$$\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda \mathbf{y}$$

where $\hat{\mathbf{g}}_\lambda$ is the solution to

$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$

for a particular choice of $\lambda$. This vector of values can be written as an $n \times n$ matrix $\mathbf{S}_\lambda$ times the response vector $\mathbf{y}$.^[See ESL ch 5.4.1 for the technical definition of $\mathbf{S}_\lambda$.] The effective degrees of freedom is defined as

$$df_\lambda = \sum_{i=1}^n \{\mathbf{S}_\lambda \}_{ii}$$

We do not have to tune the number or location of knots for a smoothing spline - they exist at each $x_1, \ldots, x_n$. We do need to choose a value for $\lambda$. We can use cross-validation to determine which value of $\lambda$ generates the smallest test RSS/MSE. However, this can be computationally intensive. Instead, we can use an analytical short-cut for [LOOCV](/notes/cross-validation/#leave-one-out-cross-validation). This short-cut allows us to estimate a single model using the formula

$$\text{RSS}_{cv}(\lambda) = \sum_{i=1}^n (y_i - \hat{g}_\lambda^{(-i)} (x_i))^2 = \sum_{i=1}^n \left[ \frac{y_i - \hat{g}_\lambda (x_i)}{1 - \{ \mathbf{S}_\lambda \}_{ii}} \right]^2$$

* $\hat{g}_\lambda^{(-i)} (x_i)$ - fitted value for a smoothing spline evaluated at $x_i$, fit using all the training observations except the $i$th observation
* $\hat{g}_\lambda (x_i)$ - fitted value for a smoothing spline evaluated at $x_i$, fit using all the training observations

To compute the LOOCV fits, we only need to use $\hat{g}_\lambda$, which is the original model. In truth, most functions use the same approach for LOOCV for all least squares regression model.^[This trick does not work for GLMs or related modeling strategies.]

Compare the difference between effective degrees of freedom:

```{r sim-smooth-spline-compare, dependson = "sim-perfect-fit"}
list(
  `50` = smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 50),
  `20` = smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 20),
  `2` = smooth.spline(sim_spline_data$x, sim_spline_data$y, df = 2),
  `11` = smooth.spline(sim_spline_data$x, sim_spline_data$y)
) %>%
  map_df(predict, .id = "df") %>%
  mutate(df = factor(df, levels = c(2, 11, 20, 50),
                     labels = c("2", "11 (CV)", "20", "50"))) %>%
  ggplot(aes(x, y)) +
  geom_point(data = sim_spline_data, alpha = .2) +
  geom_line(aes(color = df)) +
  scale_color_brewer(type = "qual", palette = "Dark2", guide = FALSE) +
  facet_wrap(~ df) +
  labs(title = "Effective degrees of freedom",
       x = expression(X),
       y = expression(Y))
```

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* @james2013introduction
* @friedman2001elements
