---
title: Marginal effects
date: 2019-02-04T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Interpreting statistical models
    weight: 1
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(tidymodels)
library(margins)
library(here)
library(rcfss)
library(patchwork)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}


# Statistical background

Least squares regression and generalized linear models are dominant methods of statistical learning in the social sciences. Interpreting the results of least squares models is, generally speaking, straight-forward. The coefficients in a least squares model typically are interpreted as the expected change in the outcome of interest associated with a unit change in the independent variable(s).

For simple applications in models of the form

$$Y = \beta_0 + \beta_1 X_1 + \ldots \beta_p X_p + \epsilon$$

this interpretation is readily calculated. Linear least squares models are generalized to alternative forms, such as non-linear relationships (e.g. polynomial terms) and [interaction terms](/notes/interaction-terms/), as well as generalized linear models like [logistic regression](/notes/logistic-regression/). Parameter estimates in these alternative forms cannot be readily interpreted without additional calculations and analysis.

Substantive interpretations of regression estimates are one approach to simplifying these alternative model results to convey meaningful interpretations of the regression models. The **marginal effect** is one such form of interpretation. Marginal effects are the expected change in $Y$ associated with a unit change in $X$. Mathematically, this is calculated using a partial derivative:

$$\frac{\partial Y}{\partial X}$$

For a simple least squares model with a single independent variable $X_1$:

$$
\begin{align}
Y &= \beta_0 + \beta_1 X_1 \\
\frac{\partial Y}{\partial X_1} &= \beta_1
\end{align}
$$

For a least squares model with two independent variables $X_1, X_2$:

$$
\begin{align}
Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 \\
\frac{\partial Y}{\partial X_1} &= \beta_1 \\
\frac{\partial Y}{\partial X_2} &= \beta_2 \\
\end{align}
$$

In these first two examples, the marginal effect of $X_1, X_2$ is constant across observations in the dataset - the partial derivatives are constant values. These marginal effects are **unconditional**.

Consider instead an interactive model with two independent variables $X_1, X_2$:

$$
\begin{aligned}
Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 \\
\frac{\partial Y}{\partial X_1} &= \beta_1 + \beta_3 X_2 \\
\frac{\partial Y}{\partial X_2} &= \beta_2 + \beta_3 X_1
\end{aligned}
$$

Or a least squares model with two independent variables $X_1, X_2$ and a polynomial term for $X_1$:

$$
\begin{aligned}
Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 \\
\frac{\partial Y}{\partial X_1} &= \beta_1 + 2\beta_2 X_1 \\
\frac{\partial Y}{\partial X_2} &= \beta_3
\end{aligned}
$$

These partial derivatives are no longer constants, therefore the marginal effects are **conditional** on the values of either $X_1$ and/or $X_2$. When marginal effects are conditional, we cannot directly interpret the estimated parameter of interest. Interpreting the results in terms of the marginal effects on the scale of the response variable (i.e. for logistic regression, discuss the model in terms of predicted probability rather than log-odds) will provide superior insight and clarity compared to direct interpretation of the regression parameters.

# Types of marginal effects

Marginal effects are calculated from the partial derivatives of the fitted regression model. There are three potential quantities of interest we can calculate using this method:

1. Marginal effects at representative values (MERs) - the marginal effect of each variable at a particular combination of $X$ values that is theoretically interesting
1. Marginal effects at means (MEMs) - the marginal effects of each variable at the means of the covariates
1. Average marginal effects (AMEs) - the marginal effects at every observed value of $X$, averaged across the resulting effect estimates

AMEs produce a single quantity summary that reflects the full distribution of $X$ as observed in the sample. Together with MERs, AMEs have the potential to convey a considerable amount of information about the influence of each covariate on the outcome. Because AMEs average across the variability in the fitted outcomes, they can also capture variability better than MEMs. While MERs provide a means to understand and communicate model estimates at theoretically important combinations of covariate values, AMEs provide a natural summary measure that respects both the distribution of the original data and does not rely on summarizing a substantively unobserved or unobservable $X$ value (as in MEMs).

# Logistic regression marginal effects

```{r titanic-data, message = FALSE}
library(titanic)
titanic <- titanic_train %>%
  as_tibble() %>%
  # remove missing values
  na.omit()
```

```{r age-woman, dependson = "titanic-data"}
age_woman <- glm(Survived ~ Age + Sex, data = titanic,
                 family = binomial)
summary(age_woman)
```

The [`margins`](https://cran.r-project.org/web/packages/margins/) package for R is an open-source port of the `margins` command in Stata.^[The `statsmodel` package in Python [includes support for calculating marginal effects](https://www.statsmodels.org/dev/search.html?q=get_margeff&check_keywords=yes&area=default).]

## Average marginal effects

To obtain average marginal effects (AMEs), we call `margins()` on the model object:

```{r age-woman-ame, dependson = "age-woman"}
margins(age_woman, type = "link")
```

These are the AMEs expressed in terms of the log-odds, as in the GLM that is how the outcome variable is expressed. In an ordinary least squares regression, there is really only one way of examining marginal effects (that is, on the scale of the outcome variable). In a generalized linear model (e.g., logit), however, it is possible to examine true "marginal effects" (i.e., the marginal contribution of each variable on the scale of the linear predictor) or "partial effects" (i.e., the contribution of each variable on the outcome scale, conditional on the other variables involved in the link function transformation of the linear predictor). The latter are the default in `margins()`, which implicitly sets the argument `margins(x, type = "response")` and passes that through to `prediction()` methods. To obtain the former, simply set `margins(x, type = "link")`. There's some debate about which of these is preferred and even what to call the two different quantities of interest. Regardless of all of that, here's how you obtain either:

```{r response-link, dependson = "age-woman"}
margins(age_woman, type = "link")
margins(age_woman, type = "response")
```

## Marginal effects at representative cases (MERs)

To calculate MERs and MEMs, we use the `at` argument. This differs from marginal effects on subsets of the original data in that it operates on a modified set of the full dataset wherein particular variables have been replaced by specified values. This is helpful because it allows for calculation of marginal effects for **counterfactual** datasets (e.g., what if all women were instead men? what if all democracies were instead autocracies? what if all foreign cars were instead domestic?).

For example, what if we want to know the partial marginal effect of age for men vs. women? We use the `at()` argument to supply the hypothetical values:

```{r age-woman-mer, dependson = "age-woman"}
margins(age_woman, at = list(Sex = c("male", "female")))
```

Because this is a logistic regression model and the values are being calculated on the scale of the response variable (this is the default approach), even though this is not an interaction model we can still see the partial marginal effect of age differs for men vs. women.

Note the difference between these values and the AMEs calculated separately for men and women:

```{r age-woman-subset-ame, dependson = "age-woman"}
margins(age_woman) %>%
  split(.$Sex)
```

This is because the AMEs calculated in the first example replace the values for the specified variables for all observations, whereas in the latter example we still calculate the AMEs based on the full joint probability distribution of the observed sample.

## Marginal effects plots

Marginal effects lend themselves very well to graphical presentations. We can easily generate marginal effects plots using the `cplot()` function, either using the base `graphics` package or `ggplot()`. Consider the effect of age on probability of survival. We can calculate the average predicted value, or alternatively the average marginal effect of age:

```{r age-margin-effect, dependson = "age-woman"}
pred_age <- cplot(age_woman, "Age", what = "prediction", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Predicted probability of survival, given age",
       x = "Age",
       y = "Predicted probability of survival")

ame_age <- cplot(age_woman, "Age", what = "effect", draw = FALSE) %>%
  ggplot(aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0, linetype = 1) +
  labs(title = "Average marginal effect of age",
       x = "Age",
       y = "Marginal effect of age")

pred_age +
  ame_age
```

Not surprisingly, the AME for age always contains $0$ in the 95% confidence interval (recall its p-value). Unlike our [prior example](/notes/logistic-regression/#probability-of-surviving-the-titanic), this method allows easy calculation of the confidence interval to present not just direct effects, but also their statistical significance.

## Interaction models

```{r age-fare, dependson = "titanic-data"}
age_fare_x <- glm(Survived ~ Age * Fare, data = titanic,
                  family = binomial)
summary(age_fare_x)
```

Consider an interaction model interacting age and fare. Now let's present the predicted probability of survival given age:

```{r age-fare-pred, dependson = "age-fare"}
cplot(age_fare_x, "Age")
```

As age increases, the probability of survival decreases. And here it appears to be a statistically significant effect:

```{r age-fare-me, dependson = "age-fare"}
cplot(age_fare_x, "Age", what = "effect")
```

The AME confirms it. But so far we have simply averaged these predicted probabilities/AMEs over all fare values. What if we consider the effect of fare on probability of survival?

```{r age-fare-fare, dependson = "age-fare"}
cplot(age_fare_x, "Fare")
cplot(age_fare_x, "Fare", what = "effect")
```

It is statistically significant and positive until fare is greater than about 300 pounds, at which point the 95% confidence interval contains 0.

To simulataneously interpret the joint effects of age and sex, we can draw a three-dimensional perspective plot:

```{r age-fare-joint, dependson = "age-fare"}
persp(age_fare_x, theta = c(45, 135, 225, 315), what = "prediction")
persp(age_fare_x, theta = c(45, 135, 225, 315), what = "effect")
```

```{r age-fare-joint-plotly, dependson = "age-fare"}
library(plotly)

# predicted prob
surface <- margins:::calculate_surface(x = age_fare_x,
                                       xvar = "Age",
                                       yvar = "Fare",
                                       dx = "Age",
                                       nx = 25L,
                                       ny = 25L,
                                       type = "response",
                                       vcov = stats::vcov(age_fare_x),
                                       what = "prediction")

outcome <- surface[["outcome"]]
xvals <- surface[["xvals"]]
yvals <- surface[["yvals"]]

plot_ly(x = ~xvals, y = ~yvals, z = ~outcome) %>%
  add_surface() %>%
  layout(
    title = "Predicted probability of survival",
    scene = list(
      xaxis = list(title = "Age"),
      yaxis = list(title = "Fare"),
      zaxis = list(title = "Predicted probability of survival")
    ))

# marginal effect
surface <- margins:::calculate_surface(x = age_fare_x,
                                       xvar = "Age",
                                       yvar = "Fare",
                                       dx = "Age",
                                       nx = 25L,
                                       ny = 25L,
                                       type = "response",
                                       vcov = stats::vcov(age_fare_x),
                                       what = "effect")

outcome <- surface[["outcome"]]
xvals <- surface[["xvals"]]
yvals <- surface[["yvals"]]

plot_ly(x = ~xvals, y = ~yvals, z = ~outcome) %>%
  add_surface() %>%
  layout(
    title = "Marginal effect on survival",
    scene = list(
      xaxis = list(title = "Age"),
      yaxis = list(title = "Fare"),
      zaxis = list(title = "AME")
    ))
```

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* @james2013introduction
* @friedman2001elements
* [Interpreting Regression Results using Average Marginal Effects with R's `margins`](https://cran.r-project.org/web/packages/margins/vignettes/TechnicalDetails.pdf)
