---
title: Decision trees
date: 2019-02-25T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Tree-based inference
    weight: 1
---


<div id="TOC">
<ul>
<li><a href="#decision-trees"><span class="toc-section-number">1</span> Decision trees</a></li>
<li><a href="#regression-trees"><span class="toc-section-number">2</span> Regression trees</a><ul>
<li><a href="#single-predictor"><span class="toc-section-number">2.1</span> Single predictor</a></li>
<li><a href="#multiple-predictors"><span class="toc-section-number">2.2</span> Multiple predictors</a></li>
<li><a href="#estimation-procedure"><span class="toc-section-number">2.3</span> Estimation procedure</a></li>
<li><a href="#pruning-the-tree"><span class="toc-section-number">2.4</span> Pruning the tree</a></li>
</ul></li>
<li><a href="#classification-trees"><span class="toc-section-number">3</span> Classification trees</a></li>
<li><a href="#trees-vs.regression"><span class="toc-section-number">4</span> Trees vs. regression</a></li>
<li><a href="#other-things-of-note-with-decision-trees"><span class="toc-section-number">5</span> Other things of note with decision trees</a><ul>
<li><a href="#categorical-predictors"><span class="toc-section-number">5.1</span> Categorical predictors</a></li>
<li><a href="#missing-values"><span class="toc-section-number">5.2</span> Missing values</a></li>
<li><a href="#connection-to-mars"><span class="toc-section-number">5.3</span> Connection to MARS</a></li>
</ul></li>
<li><a href="#benefitsdrawbacks-to-decision-trees"><span class="toc-section-number">6</span> Benefits/drawbacks to decision trees</a></li>
<li><a href="#session-info"><span class="toc-section-number">7</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">8</span> References</a></li>
</ul>
</div>

<div id="decision-trees" class="section level1">
<h1><span class="header-section-number">1</span> Decision trees</h1>
<div class="figure">
<img src="https://s-media-cache-ak0.pinimg.com/originals/7a/89/ff/7a89ff67b4ce34204c23135cbf35acfa.jpg" />

</div>
<div class="figure">
<img src="https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700" />

</div>
<div class="figure">
<img src="https://s-media-cache-ak0.pinimg.com/564x/0b/87/df/0b87df1a54474716384f8ec94b52eab9.jpg" />

</div>
<p><strong>Decision trees</strong> are intuitive concepts for making decisions. They are also useful methods for regression and classification. They work by splitting the observations into a number of regions, and predictions are made based on the mean or mode of the training observations in that region.</p>
</div>
<div id="regression-trees" class="section level1">
<h1><span class="header-section-number">2</span> Regression trees</h1>
<div id="single-predictor" class="section level2">
<h2><span class="header-section-number">2.1</span> Single predictor</h2>
<p>Let’s first consider a basic linear regression model of the relationship between horsepower and highway mileage from the <code>Auto</code> dataset.</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-lm-1.png" width="672" /></p>
<p>As we recall, a strictly linear model is a poor fit for the data since the relationship actually appears to be quadratic. But unless we <a href="/notes/global-methods/">relax our linear assumption</a>, this is the best OLS model we can estimate.</p>
<p>Let’s compare this instead to a decision tree using horsepower to predict highway mileage. Decision trees work through a process of <strong>stratification</strong>:</p>
<ol style="list-style-type: decimal">
<li>Divide the predictor space (<span class="math inline">\(X_1, X_2, \dots, X_p\)</span>) into <span class="math inline">\(J\)</span> distinct and non-overlapping regions <span class="math inline">\(R_1, R_2, \dots, R_J\)</span>.</li>
<li>For every observation in region <span class="math inline">\(R_j\)</span>, we make the same prediction which is the mean of the response variable <span class="math inline">\(Y\)</span> for all observations in <span class="math inline">\(R_j\)</span>.</li>
</ol>
<p>This process is iterative: during the first iteration, we segment the predictor space <span class="math inline">\(X\)</span> into two regions <span class="math inline">\(R_1, R_2\)</span>. In the context of a decision tree with a single predictor, that process results in decision trees like the following:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree2-1.png" width="672" /></p>
<p>On the left is the decision tree after the first iteration, and on the right is the decision tree estimation of the relationship between horsepower and highway mileage. The tree consists of three different components:</p>
<ul>
<li>Each outcome is a <strong>terminal node</strong> or a <strong>leaf</strong></li>
<li>Splits occur at <strong>internal nodes</strong></li>
<li>The segments connecting each node are called <strong>branches</strong></li>
</ul>
<p>This model has two terminal nodes (29.0377551 and 17.8540816), one internal node (horsepower <span class="math inline">\(&lt;93.5\)</span>), and two branches. For observations with horsepower <span class="math inline">\(&lt;93.5\)</span>, the model estimates highway mileage of 29.0377551. For observations with horsepower <span class="math inline">\(&gt;93.5\)</span>, the model estimates highway mileage of 17.8540816. The resulting relationship “curve” (see right) looks like a step function. Each segment of the function is the mean of the observations inside that region.</p>
<p>If we proceed to the next iteration, the decision tree segments <span class="math inline">\(R_1\)</span> further.</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree3-1.png" width="672" /></p>
<p>Now there are three terminal nodes (33.6661972, 26.4088, 17.8540816), two internal nodes (horsepower <span class="math inline">\(&lt;93.5\)</span> and horsepower <span class="math inline">\(&lt;70.5\)</span>), and three branches. Interpreting the decision tree is still relatively intuitive:</p>
<ul>
<li>If horsepower <span class="math inline">\(&gt;93.5\)</span>, then the model estimates highway mileage to be 17.8540816.</li>
<li>If horsepower <span class="math inline">\(&lt;93.5\)</span>, then we proceed down the left branch to the next internal node.
<ul>
<li>If horsepower <span class="math inline">\(&lt;70.5\)</span>, then the model estimates highway mileage to be 33.6661972.</li>
<li>If horsepower <span class="math inline">\(&gt;70.5\)</span>, then the model estimates highway mileage to be 26.4088.</li>
</ul></li>
</ul>
<p>If we continued the iterative process many many times, we’d get a decision tree that looks like this:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-treeall-1.png" width="672" /></p>
<p>There are 77 nodes (internal and terminal) in this decision tree, with 39 different regions and 39 different predicted values depending on the observation’s value for horsepower. Notice though that the step function actually looks similar to a quadratic smoothing line, matching our expectations of the relationship. In fact, compared to the linear model (23.9436629) the decision tree generates a far lower training MSE (14.4980387).<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
</div>
<div id="multiple-predictors" class="section level2">
<h2><span class="header-section-number">2.2</span> Multiple predictors</h2>
<p>With just a single predictor, the regions are a function of that one predictor. If we add a second predictor (say, vehicle weight), the regions become a function of <strong>both</strong> predictors and can be visualized as grids or boxes.</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-weight-1.png" width="672" /></p>
<ul>
<li>If weight <span class="math inline">\(&gt;2764.5\)</span>, then the model estimates highway mileage to be approximately 17.7691542.</li>
<li>If weight <span class="math inline">\(&lt;2764.5\)</span>, then we proceed down the left branch to the next internal node.
<ul>
<li>If horsepower <span class="math inline">\(&lt;70.5\)</span>, then the model estimates highway mileage to be 33.6797101.</li>
<li>If horsepower <span class="math inline">\(&gt;70.5\)</span>, then the model estimates highway mileage to be 27.0106557.</li>
</ul></li>
</ul>
<p>We can continue to build the tree up by adding additional nodes:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-weight-i-tree-1.gif" /><!-- --></p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-weight-i-feat-space-1.gif" /><!-- --></p>
</div>
<div id="estimation-procedure" class="section level2">
<h2><span class="header-section-number">2.3</span> Estimation procedure</h2>
<p>We have already identified that decision trees use stratification to divide the observations into <span class="math inline">\(R_J\)</span> regions. Like in linear regression, our goal is to minimize the residual sum of the squared errors (RSS), defined for a decision tree as:</p>
<p><span class="math display">\[\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2\]</span></p>
<p>where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the observations in the <span class="math inline">\(j\)</span>th region. In order to do this, decision trees implement a <strong>recursive binary strategy</strong>, also known as <strong>classification and regression trees</strong> (CART). The process begins at the top of the tree (<strong>top-down</strong>) and successively splits the data into a new region. This split generates two new branches in the tree. Rather than looking forward to select the optimal split among all future possibilities, this approach is <strong>greedy</strong> in that it selects the best split <strong>at that particular step</strong>. Given all the potential splits that could be performed on one of the predictors <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> predictors, the algorithm assigns a cutpoint <span class="math inline">\(s\)</span> that splits the data in the manner that reduces the RSS by the largest amount.</p>
<p><span class="math display">\[\min \left\{ SSE = \sum_{i \in R_1} (y_i - \hat{y}_{R_1})^2 + \sum_{i \in R_2} (y_i - \hat{y}_{R_2})^2 \right\}\]</span></p>
<p>As the number of predictors <span class="math inline">\(p\)</span> and observations <span class="math inline">\(N\)</span> increases, the more potential cutpoints the algorithm must consider. However even with relatively large numbers of predictors and observations, the computational process is quite efficient.</p>
<p>This process continues until some designated stopping criteria is reached, otherwise it could continue until each training observation is sorted into its own node (i.e. overfitting). A reasonable value could be five or ten. Once this iterative process stops and we have created <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> regions, we can generate predicted values for the response of a given test observation by calculating the mean of the training observations for the region in which the test observation belongs.</p>
</div>
<div id="pruning-the-tree" class="section level2">
<h2><span class="header-section-number">2.4</span> Pruning the tree</h2>
<p>Notice that we stop splitting the tree in order to prevent overfitting. Even with the above process, decision trees are highly susceptible to overfitting due to its natural complexity. And if we simply set the stopping criteria at a higher level, we may miss crucial branches later on in the process. Instead we want a method that allows us to grow a large tree and preserve the most important branches or elements.</p>
<div class="figure">
<img src="https://na.rdcpix.com/1162715347/e7baeacffee094b5d79c2ec1708c500ew-c0xd-w685_h860_q80.jpg" />

</div>
<p>Tree size can be though of as a <strong>tuning parameter</strong> that govern’s the model’s complexity. Pruning a tree is one method for creating an optimal tree size. <strong>Cost complexity pruning</strong> is the most common version of pruning. This requires growing a large tree <span class="math inline">\(T_0\)</span>, stopping the growth process only when some minimum node size is reached (say 5). From here, we consider subtrees from the overall tree and compare their relative performance.</p>
<p>Rather than comparing all possible subtrees, we use a cost complexity parameter <span class="math inline">\(\alpha\)</span> that penalizes the objective function for the number of terminal nodes of the tree <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\min \left\{ \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_M})^2 + \alpha |T| \right\}\]</span></p>
<p>For a given value of <span class="math inline">\(\alpha\)</span>, we find the smallest pruned tree that has the lowest penalized error. <span class="math inline">\(\alpha\)</span> controls the trade-off between the subtree’s complexity and its fit to the training data. It uses the same type of <span class="math inline">\(L_1\)</span> norm penalty as <a href="/syllabus/selection-regulation/">lasso regression</a>. Smaller penalties tend to produce more complex models, which results in larger trees. Whereas larger penalties result in much smaller trees. Therefore in order to achieve a larger tree, the reduction in SSE must be greater than the cost complexity penalty. We can use <a href="/notes/cross-validation/#k-fold-cross-validation"><span class="math inline">\(k\)</span>-fold cross-validation</a> to identify the optimal <span class="math inline">\(\alpha\)</span>, and therefore the optimal subtree.</p>
<p>For example, here is the full tree grown for the <code>horsepower + weight</code> decision tree:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-default-1.png" width="672" /></p>
<p>Let’s use <span class="math inline">\(10\)</span>-fold CV to select the optimal tree size:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-default-prune-1.png" width="672" /></p>
<p>The minimum cross-validated test MSE is for 6 terminal nodes. Here’s what that tree looks like:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-best-1.png" width="672" /></p>
</div>
</div>
<div id="classification-trees" class="section level1">
<h1><span class="header-section-number">3</span> Classification trees</h1>
<p>A <strong>classification tree</strong> is similar to a regression tree, except that the response variable is qualitative. In making predictions, we would predict for a test set observation the most commonly occurring class value in the given region. However we will also consider the <strong>class proportions</strong>, or the proportion of training observations in the region <span class="math inline">\(R_j\)</span> that fall into a given class.</p>
<p>Rather than using RSS to grow the tree, we have three options for minimizing error. An obvious choice might be the <strong>classification error rate</strong>, or the proportion of training observations in a given region that do not belong to the most common class:</p>
<p><span class="math display">\[E = 1 - \max_{k}(\hat{p}_{mk})\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{mk}\)</span> is the proportion of training observations in region <span class="math inline">\(m\)</span> that do not belong to the most common class <span class="math inline">\(k\)</span>.</p>
<p>In practice, two other methods grow better and more accurate trees. The <strong>Gini index</strong> is defined as:</p>
<p><span class="math display">\[G = \sum_{k = 1}^k \hat{p}_{mk} (1 - \hat{p}_{mk})\]</span></p>
<p>and is a measure of node <strong>purity</strong> (i.e. the total amount of variance across the <span class="math inline">\(K\)</span> classes). The higher the proportion of observations belonging to a single class, the closer this value will be to 0.</p>
<p>The alternative is <strong>cross-entropy</strong>:</p>
<p><span class="math display">\[D = - \sum_{k = 1}^K \hat{p}_{mk} \log(\hat{p}_{mk})\]</span></p>
<p>As more observations are closer to or near 0 or 1, cross-entropy will shrink towards zero. So for classification trees, each split can be evaluated using one of these criteria, though again it is typically the Gini index or cross-entropy.</p>
<p>Let’s return to our running Titanic example. I want to predict who lives and who dies during this event. Instead of using <a href="/notes/logistic-regression/">logistic regression</a>, I’m going to calculate a decision tree based on a passenger’s age and gender. Here’s what that decision tree looks like:</p>
<p><img src="/notes/decision-trees_files/figure-html/titanic_tree-1.png" width="672" /></p>
<p><img src="/notes/decision-trees_files/figure-html/titanic-tree-prune-1.png" width="672" /></p>
<p>Here I select 7 as the optimal number of nodes.</p>
<p><img src="/notes/decision-trees_files/figure-html/titanic-tree-best-1.png" width="672" /></p>
<p>Notice that some branches split and lead to the same outcome. For instance, the bottom-left branch assigns males with an age less than 13 but both greater than and less than <span class="math inline">\(24.75\)</span> to <code>Died</code>. This is because splitting the node leads to increased <strong>node purity</strong> where we are even more confident in our predictions. Think about it. Here are the outcomes in the training observations for males older than 13 years old:</p>
<table>
<caption><span id="tab:titanic-m-13">Table 3.1: </span>Males older than or equal to 13 on the Titanic</caption>
<thead>
<tr class="header">
<th align="left">Outcome</th>
<th align="right">Number of training observations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Died</td>
<td align="right">344</td>
</tr>
<tr class="even">
<td align="left">Survived</td>
<td align="right">72</td>
</tr>
</tbody>
</table>
<p>We would predict for all of these observations that the individual died, being incorrect 72 times. What happens if we split this subset even further?</p>
<table>
<thead>
<tr class="header">
<th align="left">Less than 24.75 years old</th>
<th align="left">Outcome</th>
<th align="right">Number of training observations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FALSE</td>
<td align="left">Died</td>
<td align="right">232</td>
</tr>
<tr class="even">
<td align="left">FALSE</td>
<td align="left">Survived</td>
<td align="right">60</td>
</tr>
<tr class="odd">
<td align="left">TRUE</td>
<td align="left">Died</td>
<td align="right">112</td>
</tr>
<tr class="even">
<td align="left">TRUE</td>
<td align="left">Survived</td>
<td align="right">12</td>
</tr>
</tbody>
</table>
<p>For males between 13 and 24.75 years old, the decision tree prediction achieves higher node purity - it more accurately predicts all of the training observations as dead. If we had a test observation for a 16-year-old male, we’d be more confident in our prediction than if we had terminated the node at <span class="math inline">\(\text{age} &lt; 13\)</span>. While this does not improve our error rate (we would have made the same prediction regardless), it does improve our Gini index and cross-entropy which are the measures used to grow the tree.</p>
</div>
<div id="trees-vs.regression" class="section level1">
<h1><span class="header-section-number">4</span> Trees vs. regression</h1>
<p>Linear regression and decision trees utilize entirely different functional forms. Linear regression assumes linear and additive relationships between predictors and the response:</p>
<p><span class="math display">\[f(X) = \beta_0 + \sum_{j = 1}^p X_j \beta_j\]</span></p>
<p>Whereas decision trees assume the observations can be partitioned into the feature space:</p>
<p><span class="math display">\[f(X) = \sum_{m = 1}^M c_m \cdot 1_{X \in R_m}\]</span></p>
<p>If the relationship between the predictor(s) and the response are truly linear and additive, then linear regression will likely perform better than a decision tree. If the relationship is highly complex and non-linear, then decision trees may be the better option. Using resampling methods such as cross-validation can help you to decide the appropriate statistical learning method.</p>
</div>
<div id="other-things-of-note-with-decision-trees" class="section level1">
<h1><span class="header-section-number">5</span> Other things of note with decision trees</h1>
<div id="categorical-predictors" class="section level2">
<h2><span class="header-section-number">5.1</span> Categorical predictors</h2>
<p>When splitting a predictor with <span class="math inline">\(q\)</span> unordered values, there are <span class="math inline">\(2^{q - 1} - 1\)</span> possible partitions of the <span class="math inline">\(q\)</span> values into two groups. Figuring out the optimal partition when <span class="math inline">\(q\)</span> is large could be prohibitively difficult. If <span class="math inline">\(q\)</span> were a binary outcome, it would be much simpler.</p>
<p>Unlike with regression-based models, we do not have to perform one-hot encoding of categorical variables prior to estimating decision trees. The decision tree procedure instead orders the predictor classes according to the proportion falling into outcome class 1. Then we split the predictor as if it were an ordered predictor. In fact, this turns out to be the optimal split.</p>
<blockquote>
<p>Note that this procedure only works for binary outcomes of interest. If this is a regression tree or classification tree with more than two possible outcomes, this procedure does not work.</p>
</blockquote>
</div>
<div id="missing-values" class="section level2">
<h2><span class="header-section-number">5.2</span> Missing values</h2>
<p>If observations have missing values, with regression models we have two main options:</p>
<ol style="list-style-type: decimal">
<li>Discard the observation (listwise deletion)</li>
<li>Impute (fill in) the missing value</li>
</ol>
<p>Decision trees offer two additional procedures.</p>
<ol style="list-style-type: decimal">
<li>For categorical variables, create a new category for missing values. That is, treat missing values as if they are another unique category.</li>
<li>Construct <strong>surrogate variables</strong>. When we split a node, only use observations for which the predictor is not missing. Then for any future nodes further down, form a list of surrogate predictors and split points. The first surrogate is the predictor and corresponding split point that best mimics the training data achieved by the primary split. The second surrogate is the predictor and corresponding split point that does second best, and so on. When sending observations down the tree either in the training phase or during prediction, use the surrogate splits in order, if the primary splitting predictor is missing.</li>
</ol>
</div>
<div id="connection-to-mars" class="section level2">
<h2><span class="header-section-number">5.3</span> Connection to MARS</h2>
<p>While <a href="/notes/multivariate-adaptive-regression-splines/">MARS</a> and decision trees may seem quite different, they are actually strongly related to one another. Take the MARS procedure and make two modifications:</p>
<ul>
<li>Replace the piecewise linear basis functions with step functions <span class="math inline">\(I(x - t &gt; 0)\)</span> and <span class="math inline">\(I(x - t \leq 0)\)</span></li>
<li>When a model term is involved in a multiplication by a candidate term, it gets replaced by the interaction and is not available for further interactions</li>
</ul>
<p>With these changes, the MARS forward procedure is the same as the CART tree-growing algorithm.</p>
</div>
</div>
<div id="benefitsdrawbacks-to-decision-trees" class="section level1">
<h1><span class="header-section-number">6</span> Benefits/drawbacks to decision trees</h1>
<p>Decision trees are an entirely different method of estimating functional forms as compared to linear regression. There are some benefits to trees:</p>
<ul>
<li>They are easy to explain. Most people, even if they lack statistical training, can understand decision trees.</li>
<li>They are easily presented as visualizations, and pretty interpretable.</li>
<li>Qualitative predictors are easily handled without the need to create a long series of dummy variables.</li>
</ul>
<p>However there are also drawbacks to trees:</p>
<ul>
<li>They cannot accurately capture additive structures. That is, if the relationship between <span class="math inline">\(X_1, X_2\)</span> and <span class="math inline">\(Y\)</span> is additive, a tree grown through binary splits is unlikely to detect this relationship since it would have to perform virtually identical splits down separate branches of the tree.</li>
<li>Their accuracy rates are generally lower than other regression and classification approaches.</li>
<li><p>Trees can be non-robust. That is, a small change in the data or inclusion/exclusion of a handful of observations can dramatically alter the final estimated tree. For example, let’s estimate a decision tree for the highway mileage example (<span class="math inline">\(N = 392\)</span>) by splitting the data into a training/test set (70/30%) and estimating the test MSE, and repeat this process 1000 times using random combinations of training/test sets:</p>
<p><img src="/notes/decision-trees_files/figure-html/auto-tree-val-1.png" width="672" /></p>
<p>The distribution of test MSEs is quite large for each of the splits, indicating substantial variance in our estimate of the test MSE.</p></li>
</ul>
<p>This illustrates the most important characteristic of decision trees: they are a <strong>low-bias</strong>, <strong>high-variance</strong> modeling strategy.</p>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">7</span> Session Info</h1>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.3        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-02-25                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package       * version    date       lib
##  assertthat      0.2.0      2017-04-11 [2]
##  backports       1.1.3      2018-12-14 [2]
##  base64enc       0.1-3      2015-07-28 [2]
##  bayesplot       1.6.0      2018-08-02 [2]
##  blogdown        0.10       2019-01-09 [1]
##  bookdown        0.9        2018-12-21 [1]
##  broom         * 0.5.1      2018-12-05 [2]
##  callr           3.1.1      2018-12-21 [2]
##  cellranger      1.1.0      2016-07-27 [2]
##  class           7.3-15     2019-01-01 [2]
##  cli             1.0.1      2018-09-25 [1]
##  codetools       0.2-16     2018-12-24 [2]
##  colorspace      1.4-0      2019-01-13 [2]
##  colourpicker    1.0        2017-09-27 [2]
##  crayon          1.3.4      2017-09-16 [2]
##  crosstalk       1.0.0      2016-12-21 [2]
##  desc            1.2.0      2018-05-01 [2]
##  devtools        2.0.1      2018-10-26 [1]
##  dials         * 0.0.2      2018-12-09 [1]
##  digest          0.6.18     2018-10-10 [1]
##  dplyr         * 0.8.0.1    2019-02-15 [1]
##  DT              0.5        2018-11-05 [2]
##  dygraphs        1.1.1.6    2018-07-11 [2]
##  evaluate        0.13       2019-02-12 [2]
##  farver          1.1.0      2018-11-20 [1]
##  forcats       * 0.4.0      2019-02-17 [2]
##  fs              1.2.6      2018-08-23 [1]
##  gbm           * 2.1.5      2019-01-14 [2]
##  generics        0.0.2      2018-11-29 [1]
##  gganimate     * 1.0.1      2019-02-15 [1]
##  ggdendro      * 0.1-20     2019-02-21 [1]
##  ggplot2       * 3.1.0      2018-10-25 [1]
##  ggridges        0.5.1      2018-09-27 [2]
##  glue            1.3.0      2018-07-17 [2]
##  gower           0.1.2      2017-02-23 [2]
##  gridExtra     * 2.3        2017-09-09 [2]
##  gtable          0.2.0      2016-02-26 [2]
##  gtools          3.8.1      2018-06-26 [2]
##  haven           2.1.0      2019-02-19 [2]
##  here            0.1        2017-05-28 [2]
##  hms             0.4.2      2018-03-10 [2]
##  htmltools       0.3.6      2017-04-28 [1]
##  htmlwidgets     1.3        2018-09-30 [2]
##  httpuv          1.4.5.1    2018-12-18 [2]
##  httr            1.4.0      2018-12-11 [2]
##  igraph          1.2.4      2019-02-13 [2]
##  infer         * 0.4.0      2018-11-15 [1]
##  inline          0.3.15     2018-05-18 [2]
##  ipred           0.9-8      2018-11-05 [1]
##  ISLR          * 1.2        2017-10-20 [2]
##  janeaustenr     0.1.5      2017-06-10 [2]
##  jsonlite        1.6        2018-12-07 [2]
##  knitr           1.21       2018-12-10 [2]
##  labeling        0.3        2014-08-23 [2]
##  later           0.8.0      2019-02-11 [2]
##  lattice         0.20-38    2018-11-04 [2]
##  lava            1.6.5      2019-02-12 [2]
##  lazyeval        0.2.1      2017-10-29 [2]
##  lme4            1.1-20     2019-02-04 [2]
##  loo             2.0.0      2018-04-11 [2]
##  lubridate       1.7.4      2018-04-11 [2]
##  magrittr      * 1.5        2014-11-22 [2]
##  markdown        0.9        2018-12-07 [2]
##  MASS            7.3-51.1   2018-11-01 [2]
##  Matrix          1.2-15     2018-11-01 [2]
##  matrixStats     0.54.0     2018-07-23 [2]
##  memoise         1.1.0      2017-04-21 [2]
##  mime            0.6        2018-10-05 [1]
##  miniUI          0.1.1.1    2018-05-18 [2]
##  minqa           1.2.4      2014-10-09 [2]
##  modelr        * 0.1.4      2019-02-18 [2]
##  munsell         0.5.0      2018-06-12 [2]
##  nlme            3.1-137    2018-04-07 [2]
##  nloptr          1.2.1      2018-10-03 [2]
##  nnet            7.3-12     2016-02-02 [2]
##  parsnip       * 0.0.1      2018-11-12 [1]
##  patchwork     * 0.0.1      2018-09-06 [1]
##  pillar          1.3.1      2018-12-15 [2]
##  pkgbuild        1.0.2      2018-10-16 [1]
##  pkgconfig       2.0.2      2018-08-16 [2]
##  pkgload         1.0.2      2018-10-29 [1]
##  plyr            1.8.4      2016-06-08 [2]
##  prettyunits     1.0.2      2015-07-13 [2]
##  pROC          * 1.13.0     2018-09-24 [1]
##  processx        3.2.1      2018-12-05 [2]
##  prodlim         2018.04.18 2018-04-18 [2]
##  progress        1.2.0      2018-06-14 [2]
##  promises        1.0.1      2018-04-13 [2]
##  ps              1.3.0      2018-12-21 [2]
##  purrr         * 0.3.0      2019-01-27 [2]
##  R6              2.4.0      2019-02-14 [1]
##  randomForest  * 4.6-14     2018-03-25 [2]
##  rcfss         * 0.1.5      2019-01-24 [1]
##  Rcpp            1.0.0      2018-11-07 [1]
##  readr         * 1.3.1      2018-12-21 [2]
##  readxl          1.3.0      2019-02-15 [2]
##  recipes       * 0.1.4      2018-11-19 [1]
##  remotes         2.0.2      2018-10-30 [1]
##  reshape2        1.4.3      2017-12-11 [2]
##  rlang           0.3.1      2019-01-08 [1]
##  rmarkdown       1.11       2018-12-08 [2]
##  rpart           4.1-13     2018-02-23 [1]
##  rprojroot       1.3-2      2018-01-03 [2]
##  rsample       * 0.0.4      2019-01-07 [1]
##  rsconnect       0.8.13     2019-01-10 [2]
##  rstan           2.18.2     2018-11-07 [2]
##  rstanarm        2.18.2     2018-11-10 [2]
##  rstantools      1.5.1      2018-08-22 [2]
##  rstudioapi      0.9.0      2019-01-09 [1]
##  rvest           0.3.2      2016-06-17 [2]
##  scales        * 1.0.0      2018-08-09 [1]
##  sessioninfo     1.1.1      2018-11-05 [1]
##  shiny           1.2.0      2018-11-02 [2]
##  shinyjs         1.0        2018-01-08 [2]
##  shinystan       2.5.0      2018-05-01 [2]
##  shinythemes     1.1.2      2018-11-06 [2]
##  SnowballC       0.6.0      2019-01-15 [2]
##  StanHeaders     2.18.1     2019-01-28 [2]
##  stringi         1.3.1      2019-02-13 [1]
##  stringr       * 1.4.0      2019-02-10 [1]
##  survival        2.43-3     2018-11-26 [2]
##  testthat        2.0.1      2018-10-13 [2]
##  threejs         0.3.1      2017-08-13 [2]
##  tibble        * 2.0.1      2019-01-12 [2]
##  tidymodels    * 0.0.2      2018-11-27 [1]
##  tidyposterior   0.0.2      2018-11-15 [1]
##  tidypredict     0.3.0      2019-01-10 [1]
##  tidyr         * 0.8.2.9000 2019-02-11 [1]
##  tidyselect      0.2.5      2018-10-11 [1]
##  tidytext        0.2.0      2018-10-17 [1]
##  tidyverse     * 1.2.1      2017-11-14 [2]
##  timeDate        3043.102   2018-02-21 [2]
##  titanic       * 0.1.0      2015-08-31 [2]
##  tokenizers      0.2.1      2018-03-29 [2]
##  tree          * 1.0-39     2018-03-17 [2]
##  tweenr          1.0.1      2018-12-14 [1]
##  usethis         1.4.0      2018-08-14 [1]
##  withr           2.1.2      2018-03-15 [2]
##  xfun            0.5        2019-02-20 [1]
##  xml2            1.2.0      2018-01-24 [2]
##  xtable          1.8-3      2018-08-29 [2]
##  xts             0.11-2     2018-11-05 [2]
##  yaml            2.2.0      2018-07-25 [2]
##  yardstick     * 0.0.2      2018-11-05 [1]
##  zoo             1.8-4      2018-09-19 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (bensoltoff/ggdendro@9c9c6e7)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  local                               
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (tidyverse/tidyr@0b27690)    
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">8</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Yes, we know <a href="/notes/model-accuracy/#training-vs.test-error">the pitfalls of using training MSE for model comparison</a>. It’s just an example because we haven’t split the data into a validation set.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
