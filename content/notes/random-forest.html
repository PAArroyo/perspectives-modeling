---
title: Random forests
date: 2019-02-27T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Tree-based inference
    weight: 3
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#limitations-of-bagging-tree-aggregation"><span class="toc-section-number">1</span> Limitations of bagging tree aggregation</a></li>
<li><a href="#random-forests"><span class="toc-section-number">2</span> Random forests</a></li>
<li><a href="#tuning-a-random-forest"><span class="toc-section-number">3</span> Tuning a random forest</a><ul>
<li><a href="#default-values"><span class="toc-section-number">3.1</span> Default values</a></li>
<li><a href="#tuning-parameter-options"><span class="toc-section-number">3.2</span> Tuning parameter options</a></li>
</ul></li>
<li><a href="#interpreting-a-random-forest"><span class="toc-section-number">4</span> Interpreting a random forest</a><ul>
<li><a href="#feature-importance"><span class="toc-section-number">4.1</span> Feature importance</a></li>
<li><a href="#partial-dependence"><span class="toc-section-number">4.2</span> Partial dependence</a></li>
</ul></li>
<li><a href="#comparison-to-bagging"><span class="toc-section-number">5</span> Comparison to bagging</a></li>
<li><a href="#session-info"><span class="toc-section-number">6</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">7</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(randomForest)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(rpart.plot)
<span class="kw">library</span>(ranger)
<span class="kw">library</span>(iml)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<div id="limitations-of-bagging-tree-aggregation" class="section level1">
<h1><span class="header-section-number">1</span> Limitations of bagging tree aggregation</h1>
<p>The problem with <a href="/notes/bagging/">bagging</a> is that if there is a single dominant predictor in the dataset, most trees will use the same predictor for the first split and ensure correlation and similarity among the trees. Consider a set of six decision trees grown using the <code>ISLR::Auto</code> dataset to predict fuel efficiency (<code>mpg</code>), where each decision tree is constructed from a bootstrapped sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate bootstrapped samples</span>
bag_trees &lt;-<span class="st"> </span><span class="kw">bootstraps</span>(ISLR<span class="op">::</span>Auto, <span class="dt">times =</span> <span class="dv">6</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">tree =</span> <span class="kw">map</span>(splits, <span class="op">~</span><span class="st"> </span><span class="kw">rpart</span>(mpg <span class="op">~</span><span class="st"> </span>.,
                                    <span class="dt">data =</span> <span class="kw">analysis</span>(.x) <span class="op">%&gt;%</span>
<span class="st">                                      </span><span class="kw">select</span>(<span class="op">-</span>name),
                                    <span class="dt">method =</span> <span class="st">&quot;anova&quot;</span>,
                                    <span class="dt">model =</span> <span class="ot">TRUE</span>)))

<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(bag_trees)) {
  <span class="kw">rpart.plot</span>(bag_trees<span class="op">$</span>tree[[i]],
             <span class="dt">main =</span> <span class="kw">str_c</span>(<span class="st">&quot;Decision Tree&quot;</span>, i, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>))
}</code></pre></div>
<p><img src="/notes/random-forest_files/figure-html/mpg-bag-trees-1.png" width="672" /></p>
<p>Almost every tree splits on the same initial predictor (<code>displacement</code>). The resulting downstream branches also appear quite similar across trees. Because bagged trees are grown sufficiently deep without pruning, they will have low bias. The tree aggregation introduces a random component into the tree-building process (i.e. bootstrap sampling) which reduces the variance compared to a single tree’s prediction, but each tree is not independent from each other: all of the trees are grown from essentially the same underlying data structure using the same growth and pruning algorithms. We should expect the individual trees to look similar to one another.</p>
<p>This means the predictions from each <span class="math inline">\(B\)</span> bootstrap sample are <strong>identically distributed</strong> (<strong>i.d.</strong>) random variables, each with variance <span class="math inline">\(\sigma^2\)</span>. If they were independent from one another (<strong>independently and identically distributed</strong> or <strong>i.i.d.</strong>), the variance of their average over <span class="math inline">\(B\)</span> random variables would simply be</p>
<p><span class="math display">\[\frac{1}{B} \sigma^2\]</span></p>
<p>However the variance for an average of i.d. variables with positive pairwise correlation <span class="math inline">\(\rho\)</span> is</p>
<p><span class="math display">\[\rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2\]</span></p>
<p>As <span class="math inline">\(B\)</span> increases, the second term shrinks towards zero, but the first always remains. Therefore the size of the correlation of pairs of bagged trees limits the benefits of averaging. While bagging reduces the variance of our estimates, we will always be constrained by <span class="math inline">\(\rho \sigma^2\)</span>.</p>
</div>
<div id="random-forests" class="section level1">
<h1><span class="header-section-number">2</span> Random forests</h1>
<p>To achieve better variance reduction, we need to minimize the correlation between the trees. If we can make the predictions from the <span class="math inline">\(B\)</span> samples i.i.d., then their average prediction will have smaller variance compared to bagging. That is, we need more randomness in the tree-growing process. <strong>Random forests</strong> do this in two ways:</p>
<ol style="list-style-type: decimal">
<li>Bootstrap sampling - like bagging, random forests generate <span class="math inline">\(B\)</span> bootstrap resampled data sets, which makes them different and somewhat decorrelated</li>
<li>Split-variable randomization - each time a split is to be performed, rather than considering all possible variables on which to perform a split, the search for the split variable is limited to a random subset of <span class="math inline">\(m \leq p\)</span> variables.
<ul>
<li>For regression trees, the default value for <span class="math inline">\(m\)</span> is <span class="math inline">\(\frac{p}{3}\)</span> and the minimum node size is five.</li>
<li>For classification trees, the default value for <span class="math inline">\(m\)</span> is <span class="math inline">\(\sqrt{p}\)</span> and the minimum node size is one.</li>
<li>Note these are only default values - <span class="math inline">\(m\)</span> and the minimum node size are tuning parameters in the model and can be tuned to the specific dataset.</li>
<li>When <span class="math inline">\(m = p\)</span>, the random forests are equivalent to bagging.</li>
</ul></li>
</ol>
<p>The basic algorithm for a random forest is:</p>
<ol style="list-style-type: decimal">
<li>For <span class="math inline">\(b=1\)</span> to <span class="math inline">\(B\)</span>:
<ol style="list-style-type: lower-alpha">
<li>Draw a bootstrap sample <span class="math inline">\(\mathbf{Z}^*\)</span> of size <span class="math inline">\(N\)</span> from the training data</li>
<li>Grow a decision tree <span class="math inline">\(T_b\)</span> to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree until the minimum node size <span class="math inline">\(n_\text{min}\)</span> is reached:
<ol style="list-style-type: lower-alpha">
<li>Select <span class="math inline">\(m\)</span> variables at random from the <span class="math inline">\(p\)</span> predictors</li>
<li>Pick the best variable/split-point among the <span class="math inline">\(m\)</span></li>
<li>Split the node into two child nodes</li>
</ol></li>
</ol></li>
<li>Output the ensemble of trees <span class="math inline">\(\{ T_b \}_1^B\)</span></li>
</ol>
<p>To make a prediction at a new point <span class="math inline">\(x\)</span>:</p>
<ul>
<li>Regression - take the average prediction of all the decision trees</li>
<li>Classification - take the most commonly predicted class of all the decision trees</li>
</ul>
<p>Because random forests are another tree-aggregation method, we can still use <a href="/notes/bagging/#out-of-bag-estimates">out-of-bag estimates</a> to estimate the error rate of the model. Likewise, we can use measures of <a href="/notes/global-interpretation/">feature importance, partial dependence plots, and feature interactions</a> to substantively interpret how different predictors are associated with the outcome of interest.</p>
</div>
<div id="tuning-a-random-forest" class="section level1">
<h1><span class="header-section-number">3</span> Tuning a random forest</h1>
<div id="default-values" class="section level2">
<h2><span class="header-section-number">3.1</span> Default values</h2>
<p>Random forests are convenient because they do not require much tuning to achieve optimal results. Even using default hyperparameter settings, models can perform fairly well. Let’s use a random forest model to predict house sale price in the Ames housing data set. Here I don’t modify any of the default values for the <code>randomForest()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.</span>
<span class="co"># Use set.seed for reproducibility</span>

<span class="kw">set.seed</span>(<span class="dv">123</span>)
ames_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(AmesHousing<span class="op">::</span><span class="kw">make_ames</span>(), <span class="dt">prop =</span> .<span class="dv">7</span>)
ames_train &lt;-<span class="st"> </span><span class="kw">training</span>(ames_split)
ames_test  &lt;-<span class="st"> </span><span class="kw">testing</span>(ames_split)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for reproduciblity</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># default RF model</span>
m1 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">data    =</span> ames_train
)

m1</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 661089658
##                     % Var explained: 89.8</code></pre>
<p>We can first consider how many trees we needed to grow to minimize our test error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(
  <span class="dt">mse =</span> m1<span class="op">$</span>mse
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">ntrees =</span> <span class="kw">row_number</span>()
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(ntrees, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">which.min</span>(m1<span class="op">$</span>mse), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Number of trees grown&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;OOB MSE&quot;</span>)</code></pre></div>
<p><img src="/notes/random-forest_files/figure-html/ames-rf-plot-1.png" width="672" /></p>
<p>As the number of trees grown increases, OOB MSE decreases rapidly until about 50 then slows down. The optimal random forest with these hyperparameter settings only requires 447 trees.</p>
<p>How does the OOB estimate compare to an actual validation set estimate?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create training and validation data </span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
valid_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(ames_train, .<span class="dv">8</span>)

<span class="co"># training data</span>
ames_train_v2 &lt;-<span class="st"> </span><span class="kw">analysis</span>(valid_split)

<span class="co"># validation data</span>
ames_valid &lt;-<span class="st"> </span><span class="kw">assessment</span>(valid_split)
x_test &lt;-<span class="st"> </span><span class="kw">select</span>(ames_valid, <span class="op">-</span>Sale_Price)
y_test &lt;-<span class="st"> </span>ames_valid<span class="op">$</span>Sale_Price

rf_oob_comp &lt;-<span class="st"> </span><span class="kw">randomForest</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">data =</span> ames_train_v2,
  <span class="dt">xtest =</span> x_test,
  <span class="dt">ytest =</span> y_test
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract OOB &amp; validation errors</span>
oob &lt;-<span class="st"> </span><span class="kw">sqrt</span>(rf_oob_comp<span class="op">$</span>mse)
validation &lt;-<span class="st"> </span><span class="kw">sqrt</span>(rf_oob_comp<span class="op">$</span>test<span class="op">$</span>mse)

<span class="co"># compare error rates</span>
<span class="kw">tibble</span>(
  <span class="st">`</span><span class="dt">Out of Bag Error</span><span class="st">`</span> =<span class="st"> </span>oob,
  <span class="st">`</span><span class="dt">Test error</span><span class="st">`</span> =<span class="st"> </span>validation
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ntrees =</span> <span class="kw">row_number</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(Metric, RMSE, <span class="op">-</span>ntrees) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(ntrees, RMSE, <span class="dt">color =</span> Metric)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Number of trees&quot;</span>)</code></pre></div>
<p><img src="/notes/random-forest_files/figure-html/ames-rf-oob-val-plot-1.png" width="672" /></p>
<p>While they are not precisely the same value, they definitely have similar trends. With an RMSE of less than $30K, this model performs better than a fully tuned bagging model or even a regularized regression model (i.e. elastic net).</p>
</div>
<div id="tuning-parameter-options" class="section level2">
<h2><span class="header-section-number">3.2</span> Tuning parameter options</h2>
<p>The most common tuning parameters for random forest models are:</p>
<ul>
<li>Number of trees - we want enough trees to stabilize our estimates of the error, but using too many trees is computationally inefficient</li>
<li>Number of variables to randomly sample at each split - if <span class="math inline">\(m=p\)</span>, the model equates to bagging. If <span class="math inline">\(m=1\)</span>, the split variable is completely random. A common approach starts with 5 values evenly spaced across the range from <span class="math inline">\(2\)</span> to <span class="math inline">\(p\)</span></li>
<li>Sample size for each sample - if we sample with replacement (bootstrap), our sample size is <span class="math inline">\(N\)</span>. Instead, we could sample <strong>without replacement</strong> and only sample a fraction of the original sample. Decreasing the sample size can reduce training time but increase the bias of our estimates. Increasing the sample size can increase performance but at the risk of overfitting since it introduces more variance.</li>
<li>Minimum number of observations within the terminal nodes - this controls the complexity of the trees. Small node size allows for deeper, more complex trees (lower bias, higher variance), whereas larger node results in shallower trees (higher bias, lower variance).</li>
<li>Maximum number of terminal nodes - another method for controling the complexity of the tree. More nodes leads to deeper trees, fewer nodes leads to shallower trees</li>
</ul>
<p>We can perform a grid search over several hyperparameters, but if you do this make sure to choose an efficient function that implements a random forest model. Otherwise if each random forest takes a minute to grow, hyperparameter tuning over dozens of combinations of hyperparameters will take close to an hour. In R, the <code>ranger</code> package implements an efficient random forest algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># hyperparameter grid search</span>
hyper_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
  <span class="dt">mtry =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">30</span>, <span class="dt">by =</span> <span class="dv">2</span>),
  <span class="dt">node_size =</span> <span class="kw">seq</span>(<span class="dv">3</span>, <span class="dv">9</span>, <span class="dt">by =</span> <span class="dv">2</span>),
  <span class="dt">sampe_size =</span> <span class="kw">c</span>(.<span class="dv">55</span>, .<span class="dv">632</span>, .<span class="dv">70</span>, .<span class="dv">80</span>),
  <span class="dt">OOB_RMSE =</span> <span class="dv">0</span>
)

<span class="co"># total number of combinations</span>
<span class="kw">nrow</span>(hyper_grid)</code></pre></div>
<pre><code>## [1] 96</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(hyper_grid)) {
  <span class="co"># train model</span>
  model &lt;-<span class="st"> </span><span class="kw">ranger</span>(
    <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., 
    <span class="dt">data =</span> ames_train, 
    <span class="dt">num.trees =</span> <span class="dv">500</span>,
    <span class="dt">mtry =</span> hyper_grid<span class="op">$</span>mtry[i],
    <span class="dt">min.node.size =</span> hyper_grid<span class="op">$</span>node_size[i],
    <span class="dt">sample.fraction =</span> hyper_grid<span class="op">$</span>sampe_size[i],
    <span class="dt">seed =</span> <span class="dv">123</span>
  )
  
  <span class="co"># add OOB error to grid</span>
  hyper_grid<span class="op">$</span>OOB_RMSE[i] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(model<span class="op">$</span>prediction.error)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hyper_grid &lt;-<span class="st"> </span>hyper_grid <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(OOB_RMSE)
hyper_grid <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>(<span class="dv">10</span>)</code></pre></div>
<pre><code>##    mtry node_size sampe_size OOB_RMSE
## 1    20         5        0.8 25918.20
## 2    20         3        0.8 25963.96
## 3    28         3        0.8 25997.78
## 4    22         5        0.8 26041.05
## 5    22         3        0.8 26050.63
## 6    20         7        0.8 26061.72
## 7    26         3        0.8 26069.40
## 8    28         5        0.8 26069.83
## 9    26         7        0.8 26075.71
## 10   20         9        0.8 26091.08</code></pre>
<p>These results indicate the best random forest model uses <span class="math inline">\(m=20\)</span>, terminal node size of 5 observations, and a sample size of 0.8. Let’s repeat this model to get a better expectation of our error rate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">OOB_RMSE &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;numeric&quot;</span>, <span class="dt">length =</span> <span class="dv">100</span>)

<span class="kw">set.seed</span>(<span class="dv">123</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_along</span>(OOB_RMSE)) {
  optimal_ranger &lt;-<span class="st"> </span><span class="kw">ranger</span>(
    <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>., 
    <span class="dt">data =</span> ames_train, 
    <span class="dt">num.trees =</span> <span class="dv">500</span>,
    <span class="dt">mtry =</span> hyper_grid<span class="op">$</span>mtry[[<span class="dv">1</span>]],
    <span class="dt">min.node.size =</span> hyper_grid<span class="op">$</span>node_size[[<span class="dv">1</span>]],
    <span class="dt">sample.fraction =</span> hyper_grid<span class="op">$</span>sampe_size[[<span class="dv">1</span>]]
  )
  
  OOB_RMSE[i] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(optimal_ranger<span class="op">$</span>prediction.error)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">RMSE =</span> OOB_RMSE) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(RMSE)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of OOB RMSE&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Repeats = 100&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;OOB RMSE&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Frequency&quot;</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/notes/random-forest_files/figure-html/ames-rf-tune-final-plot-1.png" width="672" /></p>
</div>
</div>
<div id="interpreting-a-random-forest" class="section level1">
<h1><span class="header-section-number">4</span> Interpreting a random forest</h1>
<div id="feature-importance" class="section level2">
<h2><span class="header-section-number">4.1</span> Feature importance</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_ranger &lt;-<span class="st"> </span><span class="cf">function</span>(model, newdata){
  results &lt;-<span class="st"> </span><span class="kw">predict</span>(model, newdata)
  <span class="kw">return</span>(results<span class="op">$</span>prediction)
}

predictor_rf &lt;-<span class="st"> </span>Predictor<span class="op">$</span><span class="kw">new</span>(
  <span class="dt">model =</span> optimal_ranger,
  <span class="dt">data =</span> <span class="kw">select</span>(ames_train, <span class="op">-</span><span class="st"> </span>Sale_Price),
  <span class="dt">y =</span> ames_train<span class="op">$</span>Sale_Price,
  <span class="dt">predict.fun =</span> pred_ranger
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doParallel)</code></pre></div>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cl &lt;-<span class="st"> </span><span class="kw">makePSOCKcluster</span>(<span class="dv">3</span>)
<span class="kw">registerDoParallel</span>(cl)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">feat_imp_rf &lt;-<span class="st"> </span>FeatureImp<span class="op">$</span><span class="kw">new</span>(predictor_rf, <span class="dt">loss =</span> <span class="st">&quot;mse&quot;</span>, <span class="dt">parallel =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(feat_imp_rf) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Random forest&quot;</span>)</code></pre></div>
<p><img src="/notes/random-forest_files/figure-html/ames-rf-feat-imp-plot-1.png" width="672" /></p>
</div>
<div id="partial-dependence" class="section level2">
<h2><span class="header-section-number">4.2</span> Partial dependence</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pdp_Overall_Qual &lt;-<span class="st"> </span>FeatureEffect<span class="op">$</span><span class="kw">new</span>(predictor_rf,
                                      <span class="st">&quot;Overall_Qual&quot;</span>,
                                      <span class="dt">method =</span> <span class="st">&quot;pdp+ice&quot;</span>)
pdp_Gr_Liv_Area &lt;-<span class="st"> </span>FeatureEffect<span class="op">$</span><span class="kw">new</span>(predictor_rf,
                                     <span class="st">&quot;Gr_Liv_Area&quot;</span>,
                                     <span class="dt">method =</span> <span class="st">&quot;pdp+ice&quot;</span>,
                                     <span class="dt">center.at =</span> <span class="kw">min</span>(ames_train<span class="op">$</span>Gr_Liv_Area),
                                     <span class="dt">grid.size =</span> <span class="dv">50</span>)
pdp_Year_Built &lt;-<span class="st"> </span>FeatureEffect<span class="op">$</span><span class="kw">new</span>(predictor_rf,
                                    <span class="st">&quot;Year_Built&quot;</span>,
                                    <span class="dt">method =</span> <span class="st">&quot;pdp+ice&quot;</span>,
                                    <span class="dt">center.at =</span> <span class="kw">min</span>(ames_train<span class="op">$</span>Year_Built),
                                    <span class="dt">grid.size =</span> <span class="dv">50</span>)

p1 &lt;-<span class="st"> </span><span class="kw">plot</span>(pdp_Overall_Qual)
p2 &lt;-<span class="st"> </span><span class="kw">plot</span>(pdp_Gr_Liv_Area)
p3 &lt;-<span class="st"> </span><span class="kw">plot</span>(pdp_Year_Built)

p1 <span class="op">+</span><span class="st"> </span>p2 <span class="op">+</span><span class="st"> </span>p3 <span class="op">+</span>
<span class="st">  </span><span class="kw">plot_layout</span>(<span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="/notes/random-forest_files/figure-html/ames-rf-pdp-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stopCluster</span>(cl)</code></pre></div>
</div>
</div>
<div id="comparison-to-bagging" class="section level1">
<h1><span class="header-section-number">5</span> Comparison to bagging</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bag_oob &lt;-<span class="st"> </span><span class="kw">randomForest</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">data =</span> ames_train,
  <span class="dt">num.trees =</span> <span class="dv">500</span>,
  <span class="dt">mtry =</span> <span class="kw">ncol</span>(ames_train) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,
  <span class="dt">nodesize =</span> hyper_grid<span class="op">$</span>node_size[[<span class="dv">1</span>]],
  <span class="dt">replace =</span> <span class="ot">FALSE</span>,
  <span class="dt">sampsize =</span> <span class="kw">ceiling</span>(hyper_grid<span class="op">$</span>sampe_size[[<span class="dv">1</span>]] <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(ames_train))
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rf_oob &lt;-<span class="st"> </span><span class="kw">randomForest</span>(
  <span class="dt">formula =</span> Sale_Price <span class="op">~</span><span class="st"> </span>.,
  <span class="dt">data =</span> ames_train,
  <span class="dt">num.trees =</span> <span class="dv">500</span>,
  <span class="dt">mtry =</span> hyper_grid<span class="op">$</span>mtry[[<span class="dv">1</span>]],
  <span class="dt">nodesize =</span> hyper_grid<span class="op">$</span>node_size[[<span class="dv">1</span>]],
  <span class="dt">replace =</span> <span class="ot">FALSE</span>,
  <span class="dt">sampsize =</span> <span class="kw">ceiling</span>(hyper_grid<span class="op">$</span>sampe_size[[<span class="dv">1</span>]] <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(ames_train))
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(
  <span class="dt">Bagging =</span> <span class="kw">sqrt</span>(bag_oob<span class="op">$</span>mse),
  <span class="st">`</span><span class="dt">Random Forest</span><span class="st">`</span> =<span class="st"> </span><span class="kw">sqrt</span>(rf_oob<span class="op">$</span>mse)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ntrees =</span> <span class="kw">row_number</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(Metric, RMSE, <span class="op">-</span>ntrees) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(ntrees, RMSE, <span class="dt">color =</span> Metric)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>dollar) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Number of trees&quot;</span>)</code></pre></div>
<p><img src="/notes/random-forest_files/figure-html/rf-bag-compare-1.png" width="672" /></p>
<p>Holding the tuning parameters constant, random forest leads to a lower test error compared to the equivalent bagging model.</p>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.3        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-02-26                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package       * version    date       lib
##  assertthat      0.2.0      2017-04-11 [2]
##  backports       1.1.3      2018-12-14 [2]
##  base64enc       0.1-3      2015-07-28 [2]
##  bayesplot       1.6.0      2018-08-02 [2]
##  blogdown        0.10       2019-01-09 [1]
##  bookdown        0.9        2018-12-21 [1]
##  broom         * 0.5.1      2018-12-05 [2]
##  callr           3.1.1      2018-12-21 [2]
##  cellranger      1.1.0      2016-07-27 [2]
##  checkmate       1.9.1      2019-01-15 [2]
##  class           7.3-15     2019-01-01 [2]
##  cli             1.0.1      2018-09-25 [1]
##  codetools       0.2-16     2018-12-24 [2]
##  colorspace      1.4-0      2019-01-13 [2]
##  colourpicker    1.0        2017-09-27 [2]
##  crayon          1.3.4      2017-09-16 [2]
##  crosstalk       1.0.0      2016-12-21 [2]
##  data.table      1.12.0     2019-01-13 [2]
##  desc            1.2.0      2018-05-01 [2]
##  devtools        2.0.1      2018-10-26 [1]
##  dials         * 0.0.2      2018-12-09 [1]
##  digest          0.6.18     2018-10-10 [1]
##  doParallel    * 1.0.14     2018-09-24 [1]
##  dplyr         * 0.8.0.1    2019-02-15 [1]
##  DT              0.5        2018-11-05 [2]
##  dygraphs        1.1.1.6    2018-07-11 [2]
##  evaluate        0.13       2019-02-12 [2]
##  forcats       * 0.4.0      2019-02-17 [2]
##  foreach       * 1.4.4      2017-12-12 [2]
##  Formula         1.2-3      2018-05-03 [2]
##  fs              1.2.6      2018-08-23 [1]
##  generics        0.0.2      2018-11-29 [1]
##  ggplot2       * 3.1.0      2018-10-25 [1]
##  ggridges        0.5.1      2018-09-27 [2]
##  glmnet          2.0-16     2018-04-02 [1]
##  glue            1.3.0      2018-07-17 [2]
##  gower           0.1.2      2017-02-23 [2]
##  gridExtra       2.3        2017-09-09 [2]
##  gtable          0.2.0      2016-02-26 [2]
##  gtools          3.8.1      2018-06-26 [2]
##  haven           2.1.0      2019-02-19 [2]
##  here            0.1        2017-05-28 [2]
##  hms             0.4.2      2018-03-10 [2]
##  htmltools       0.3.6      2017-04-28 [1]
##  htmlwidgets     1.3        2018-09-30 [2]
##  httpuv          1.4.5.1    2018-12-18 [2]
##  httr            1.4.0      2018-12-11 [2]
##  igraph          1.2.4      2019-02-13 [2]
##  iml           * 0.9.0      2019-02-05 [1]
##  infer         * 0.4.0      2018-11-15 [1]
##  inline          0.3.15     2018-05-18 [2]
##  inum            1.0-0      2017-12-12 [1]
##  ipred           0.9-8      2018-11-05 [1]
##  iterators     * 1.0.10     2018-07-13 [2]
##  janeaustenr     0.1.5      2017-06-10 [2]
##  jsonlite        1.6        2018-12-07 [2]
##  knitr           1.21       2018-12-10 [2]
##  later           0.8.0      2019-02-11 [2]
##  lattice         0.20-38    2018-11-04 [2]
##  lava            1.6.5      2019-02-12 [2]
##  lazyeval        0.2.1      2017-10-29 [2]
##  libcoin         1.0-3      2019-02-18 [1]
##  lme4            1.1-20     2019-02-04 [2]
##  loo             2.0.0      2018-04-11 [2]
##  lubridate       1.7.4      2018-04-11 [2]
##  magrittr        1.5        2014-11-22 [2]
##  markdown        0.9        2018-12-07 [2]
##  MASS            7.3-51.1   2018-11-01 [2]
##  Matrix          1.2-15     2018-11-01 [2]
##  matrixStats     0.54.0     2018-07-23 [2]
##  memoise         1.1.0      2017-04-21 [2]
##  Metrics         0.1.4      2018-07-09 [1]
##  mime            0.6        2018-10-05 [1]
##  miniUI          0.1.1.1    2018-05-18 [2]
##  minqa           1.2.4      2014-10-09 [2]
##  modelr          0.1.4      2019-02-18 [2]
##  munsell         0.5.0      2018-06-12 [2]
##  mvtnorm         1.0-8      2018-05-31 [2]
##  nlme            3.1-137    2018-04-07 [2]
##  nloptr          1.2.1      2018-10-03 [2]
##  nnet            7.3-12     2016-02-02 [2]
##  parsnip       * 0.0.1      2018-11-12 [1]
##  partykit        1.2-3      2019-01-31 [1]
##  patchwork     * 0.0.1      2018-09-06 [1]
##  pillar          1.3.1      2018-12-15 [2]
##  pkgbuild        1.0.2      2018-10-16 [1]
##  pkgconfig       2.0.2      2018-08-16 [2]
##  pkgload         1.0.2      2018-10-29 [1]
##  plyr            1.8.4      2016-06-08 [2]
##  prediction      0.3.6.2    2019-01-31 [2]
##  prettyunits     1.0.2      2015-07-13 [2]
##  pROC            1.13.0     2018-09-24 [1]
##  processx        3.2.1      2018-12-05 [2]
##  prodlim         2018.04.18 2018-04-18 [2]
##  promises        1.0.1      2018-04-13 [2]
##  ps              1.3.0      2018-12-21 [2]
##  purrr         * 0.3.0      2019-01-27 [2]
##  R6              2.4.0      2019-02-14 [1]
##  randomForest  * 4.6-14     2018-03-25 [2]
##  ranger        * 0.11.1     2019-01-24 [1]
##  rcfss         * 0.1.5      2019-01-24 [1]
##  Rcpp            1.0.0      2018-11-07 [1]
##  readr         * 1.3.1      2018-12-21 [2]
##  readxl          1.3.0      2019-02-15 [2]
##  recipes       * 0.1.4      2018-11-19 [1]
##  remotes         2.0.2      2018-10-30 [1]
##  reshape2        1.4.3      2017-12-11 [2]
##  rlang           0.3.1      2019-01-08 [1]
##  rmarkdown       1.11       2018-12-08 [2]
##  rpart         * 4.1-13     2018-02-23 [1]
##  rpart.plot    * 3.0.6      2018-11-24 [1]
##  rprojroot       1.3-2      2018-01-03 [2]
##  rsample       * 0.0.4      2019-01-07 [1]
##  rsconnect       0.8.13     2019-01-10 [2]
##  rstan           2.18.2     2018-11-07 [2]
##  rstanarm        2.18.2     2018-11-10 [2]
##  rstantools      1.5.1      2018-08-22 [2]
##  rstudioapi      0.9.0      2019-01-09 [1]
##  rvest           0.3.2      2016-06-17 [2]
##  scales        * 1.0.0      2018-08-09 [1]
##  sessioninfo     1.1.1      2018-11-05 [1]
##  shiny           1.2.0      2018-11-02 [2]
##  shinyjs         1.0        2018-01-08 [2]
##  shinystan       2.5.0      2018-05-01 [2]
##  shinythemes     1.1.2      2018-11-06 [2]
##  SnowballC       0.6.0      2019-01-15 [2]
##  StanHeaders     2.18.1     2019-01-28 [2]
##  stringi         1.3.1      2019-02-13 [1]
##  stringr       * 1.4.0      2019-02-10 [1]
##  survival        2.43-3     2018-11-26 [2]
##  testthat        2.0.1      2018-10-13 [2]
##  threejs         0.3.1      2017-08-13 [2]
##  tibble        * 2.0.1      2019-01-12 [2]
##  tidymodels    * 0.0.2      2018-11-27 [1]
##  tidyposterior   0.0.2      2018-11-15 [1]
##  tidypredict     0.3.0      2019-01-10 [1]
##  tidyr         * 0.8.2.9000 2019-02-11 [1]
##  tidyselect      0.2.5      2018-10-11 [1]
##  tidytext        0.2.0      2018-10-17 [1]
##  tidyverse     * 1.2.1      2017-11-14 [2]
##  timeDate        3043.102   2018-02-21 [2]
##  tokenizers      0.2.1      2018-03-29 [2]
##  usethis         1.4.0      2018-08-14 [1]
##  withr           2.1.2      2018-03-15 [2]
##  xfun            0.5        2019-02-20 [1]
##  xml2            1.2.0      2018-01-24 [2]
##  xtable          1.8-3      2018-08-29 [2]
##  xts             0.11-2     2018-11-05 [2]
##  yaml            2.2.0      2018-07-25 [2]
##  yardstick     * 0.0.2      2018-11-05 [1]
##  zoo             1.8-4      2018-09-19 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.2)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  local                               
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (tidyverse/tidyr@0b27690)    
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">7</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
