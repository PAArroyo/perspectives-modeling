---
title: Global methods
date: 2019-02-18T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Moving beyond linearity
    weight: 1
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#linearity-in-linear-models"><span class="toc-section-number">1</span> Linearity in linear models</a><ul>
<li><a href="#when-to-relax-the-assumption"><span class="toc-section-number">1.1</span> When to relax the assumption</a><ul>
<li><a href="#non-linearity-of-the-data"><span class="toc-section-number">1.1.1</span> Non-linearity of the data</a></li>
<li><a href="#non-constant-variance-of-the-error-terms"><span class="toc-section-number">1.1.2</span> Non-constant variance of the error terms</a></li>
</ul></li>
</ul></li>
<li><a href="#monotonic-transformations"><span class="toc-section-number">2</span> Monotonic transformations</a><ul>
<li><a href="#which-transformation-should-i-use"><span class="toc-section-number">2.1</span> Which transformation should I use?</a></li>
<li><a href="#interpreting-transformed-variables"><span class="toc-section-number">2.2</span> Interpreting transformed variables</a><ul>
<li><a href="#log-transformations"><span class="toc-section-number">2.2.1</span> Log transformations</a></li>
</ul></li>
<li><a href="#polynomial-regressions"><span class="toc-section-number">2.3</span> Polynomial regressions</a><ul>
<li><a href="#biden-and-age"><span class="toc-section-number">2.3.1</span> Biden and age</a></li>
<li><a href="#voter-turnout-and-mental-health"><span class="toc-section-number">2.3.2</span> Voter turnout and mental health</a></li>
</ul></li>
</ul></li>
<li><a href="#step-functions"><span class="toc-section-number">3</span> Step functions</a><ul>
<li><a href="#age-and-voting"><span class="toc-section-number">3.1</span> Age and voting</a></li>
</ul></li>
<li><a href="#basis-functions"><span class="toc-section-number">4</span> Basis functions</a></li>
<li><a href="#regression-splines"><span class="toc-section-number">5</span> Regression splines</a><ul>
<li><a href="#piecewise-polynomials"><span class="toc-section-number">5.1</span> Piecewise polynomials</a></li>
<li><a href="#constraints-and-splines"><span class="toc-section-number">5.2</span> Constraints and splines</a></li>
<li><a href="#basis-splines-vs.natural-splines"><span class="toc-section-number">5.3</span> Basis splines vs. natural splines</a></li>
<li><a href="#choosing-the-number-and-location-of-knots"><span class="toc-section-number">5.4</span> Choosing the number and location of knots</a></li>
</ul></li>
<li><a href="#smoothing-splines"><span class="toc-section-number">6</span> Smoothing splines</a></li>
<li><a href="#multivariate-adaptive-regression-splines-mars"><span class="toc-section-number">7</span> Multivariate adaptive regression splines (MARS)</a></li>
<li><a href="#local-regression"><span class="toc-section-number">8</span> Local regression</a><ul>
<li><a href="#algorithm-for-local-linear-regression"><span class="toc-section-number">8.1</span> Algorithm for local linear regression</a></li>
</ul></li>
<li><a href="#generalized-additive-models"><span class="toc-section-number">9</span> Generalized additive models</a><ul>
<li><a href="#gams-for-regression-problems"><span class="toc-section-number">9.1</span> GAMs for regression problems</a></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">10</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">11</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(titanic)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(splines)
<span class="kw">library</span>(ISLR)
<span class="kw">library</span>(lattice)
<span class="kw">library</span>(gam)
<span class="kw">library</span>(here)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(margins)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<div id="linearity-in-linear-models" class="section level1">
<h1><span class="header-section-number">1</span> Linearity in linear models</h1>
<p>Linear models are a commonly used statistical learning method because they are intuitive and easy to interpret. The drawback is that in order to create that intuitiveness and interpretability, linear models also make strong assumptions of a <strong>linear relationship</strong> between the predictor(s) and response variable. It is an approximation, and in the real-world most relationships are not strictly linear. We could turn to more advanced statistical learning methods such as decision trees and support vector machines which do not impose a linear assumption, however conducting inference using those methods is more difficult. Instead, we would like to relax the linearity assumption while still maintaining the interpretability of linear models.</p>
<div id="when-to-relax-the-assumption" class="section level2">
<h2><span class="header-section-number">1.1</span> When to relax the assumption</h2>
<div id="non-linearity-of-the-data" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Non-linearity of the data</h3>
<p>Linear regression models assume the relationship between predictors and the response variable is a straight line. Here I simulate the data generating process:</p>
<p><span class="math display">\[Y = 2 + 3X + \epsilon\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is random error distributed normally <span class="math inline">\(N(0,3)\)</span>.</p>
<p><img src="/notes/global-methods_files/figure-html/sim-linear-1.png" width="672" /></p>
<p>When this assumption holds, the <strong>residuals</strong> of the observations should be distributed normally with an expected error <span class="math inline">\(E(\epsilon) = 0\)</span> and not be correlated with the fitted values.</p>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/notes/global-methods_files/figure-html/sim-linear-resid-1.png" width="672" /></p>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/notes/global-methods_files/figure-html/sim-linear-resid-2.png" width="672" /></p>
<p>Inferences we draw from the model will be accurate and the accuracy of the model will be high.</p>
<p>However if the relationship is not actually linear, then we should see a discernable pattern in the residuals plot. Now I simulate a data generating process:</p>
<p><span class="math display">\[Y = 2 + 3X + 2X^2 + \epsilon\]</span></p>
<p>with the same distribution of <span class="math inline">\(\epsilon\)</span> and fit a linear model to the data:</p>
<p><span class="math display">\[\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X + \epsilon\]</span></p>
<p><img src="/notes/global-methods_files/figure-html/sim-nonlinear-1.png" width="672" /></p>
<p>Now that the assumption no longer holds true, we see a distinct polynomial relationship between the predicted values <span class="math inline">\(\hat{y}_i\)</span> and the residuals <span class="math inline">\((y_i - \hat{y}_i)^2\)</span>.</p>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/notes/global-methods_files/figure-html/sim-nonlinear-resid-1.png" width="672" /></p>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/notes/global-methods_files/figure-html/sim-nonlinear-resid-2.png" width="672" /></p>
<p>If we want to draw accurate inferences from the model, we need to relax the linearity assumption in some way.</p>
</div>
<div id="non-constant-variance-of-the-error-terms" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Non-constant variance of the error terms</h3>
<p>Another assumption of linear regression is that the error terms <span class="math inline">\(\epsilon_i\)</span> have a constant variance, <span class="math inline">\(\text{Var}(\epsilon_i) = \sigma^2\)</span>. This is called <strong>homoscedasticity</strong>. Remember that the <a href="/notes/ols-diagnostics/#non-constant-error-variance">standard errors directly rely upon the estimate of this value</a>:</p>
<p><span class="math display">\[\widehat{s.e.}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^{2} (X^{T}X)^{-1}_{jj}}\]</span></p>
<p>If the variances of the error terms are non-constant (aka <strong>heteroscedastic</strong>), our estimates of the parameters <span class="math inline">\(\hat{\beta}\)</span> will still be unbiased because they do not depend on <span class="math inline">\(\sigma^2\)</span>. However our estimates of the standard errors will be inaccurate - they will either be inflated or deflated, leading to incorrect inferences about the statistical significance of predictor variables.</p>
<p>We can uncover homo- or heteroscedasticity through the use of the residual plot. Below is data generated from the process:</p>
<p><span class="math display">\[Y = 2 + 3X + \epsilon\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is random error distributed normally <span class="math inline">\(N(0,1)\)</span>.</p>
<pre><code>## Smoothing formula not specified. Using: y ~ qss(x, lambda = 5)</code></pre>
<pre><code>## Warning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct</code></pre>
<p><img src="/notes/global-methods_files/figure-html/sim-homo-1.png" width="672" /></p>
<p>Compare this to a linear model fit to the data generating process:</p>
<p><span class="math display">\[Y = 2 + 3X + \epsilon\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is random error distributed normally <span class="math inline">\(N(0,\frac{X}{2})\)</span>. Note that the variance for the error term of each observation <span class="math inline">\(\epsilon_i\)</span> is not constant, and is itself a function of <span class="math inline">\(X\)</span>.</p>
<pre><code>## Smoothing formula not specified. Using: y ~ qss(x, lambda = 5)</code></pre>
<p><img src="/notes/global-methods_files/figure-html/sim-hetero-1.png" width="672" /></p>
<p>We see a distinct funnel-shape to the relationship between the predicted values and the residuals. This is because by assuming the variance is constant, we substantially over or underestimate the actual response <span class="math inline">\(Y_i\)</span> as <span class="math inline">\(X_i\)</span> increases.</p>
</div>
</div>
</div>
<div id="monotonic-transformations" class="section level1">
<h1><span class="header-section-number">2</span> Monotonic transformations</h1>
<p>A <strong>monotonic function</strong> is a function for transforming a set of numbers into a different set of numbers so that the rank order of the original set of numbers is preserved. One major family of monotonic functions is the <strong>ladder of powers</strong>.</p>
<table>
<thead>
<tr class="header">
<th>Transformation</th>
<th>Power</th>
<th><span class="math inline">\(f(X)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cube</td>
<td>3</td>
<td><span class="math inline">\(X^3\)</span></td>
</tr>
<tr class="even">
<td>Square</td>
<td>2</td>
<td><span class="math inline">\(X^2\)</span></td>
</tr>
<tr class="odd">
<td>Identity</td>
<td>1</td>
<td><span class="math inline">\(X\)</span></td>
</tr>
<tr class="even">
<td>Square root</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\sqrt{X}\)</span></td>
</tr>
<tr class="odd">
<td>Cube root</td>
<td><span class="math inline">\(\frac{1}{3}\)</span></td>
<td><span class="math inline">\(\sqrt[3]{X}\)</span></td>
</tr>
<tr class="even">
<td>Log</td>
<td>0 (sort of)</td>
<td><span class="math inline">\(\log(X)\)</span></td>
</tr>
</tbody>
</table>
<p><img src="/notes/global-methods_files/figure-html/power-ladder-1.png" width="672" /></p>
<ul>
<li>Higher-order power transformations “inflate” large values and “compress” small ones</li>
<li>Lower-order power transformations “compress” large values and “inflate” small ones</li>
</ul>
<p>In order for power transformations to “work”, all the values of <span class="math inline">\(X\)</span> need to be <strong>positive</strong> (remember that <span class="math inline">\(\log(0) = -\infty\)</span>). If you need to, you can adjust <span class="math inline">\(X\)</span> first to ensure it only contains positive values by adding the absolute value of the lowest (negative) value of <span class="math inline">\(X\)</span> (call it <span class="math inline">\(X_l\)</span>), plus some small additional value <span class="math inline">\(\epsilon\)</span> to <span class="math inline">\(X\)</span> itself:</p>
<p><span class="math display">\[X^* = X + (|X_l| + \epsilon)\]</span></p>
<div id="which-transformation-should-i-use" class="section level2">
<h2><span class="header-section-number">2.1</span> Which transformation should I use?</h2>
<p>This depends on the situation. Typically we use these transformations to induce linearity between <span class="math inline">\(Y\)</span> and one or more predictors <span class="math inline">\(X\)</span>. Tukey and Mosteller suggest a “bulging rule” for power transformations to make things more linear:</p>
<p><img src="/notes/global-methods_files/figure-html/bulge-rule-1.png" width="672" /></p>
<p>Notice that you are not limited to transforming your predictors <span class="math inline">\(X\)</span> - <strong>you can also transform the response variable <span class="math inline">\(Y\)</span></strong>. You can use simple scatterplots and residual plots to help determine if a transformation is necessary, but these are not a substitute for <strong>good theory</strong>. Is there a theoretical reason why the relationship should be curvilinear rather than strictly linear? If so, use that as a guide to determine an appropriate transformation.</p>
</div>
<div id="interpreting-transformed-variables" class="section level2">
<h2><span class="header-section-number">2.2</span> Interpreting transformed variables</h2>
<div id="log-transformations" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Log transformations</h3>
<p>Interpreting the transformed variables is relatively straightforward. For instance, let’s use the example of a one-sided log transformation of <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[\log(Y_i) = \beta_0 + \beta_{1}X_i + \epsilon_i\]</span></p>
<p>The expected value for <span class="math inline">\(Y\)</span> is therefore:</p>
<p><span class="math display">\[{\mathrm{E}}(Y) = e^{\beta_0 + \beta_{1}X_i}\]</span></p>
<p>This means that the effect of a one-unit increase in <span class="math inline">\(X\)</span> on the expected value of <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math display">\[\frac{\partial {\mathrm{E}}(Y)}{\partial X} = e^{\beta_1}\]</span></p>
<p>What used to be a strictly linear relationship is now multiplicative (and non-linear). So for instance if <span class="math inline">\(\beta_1 = 0.69\)</span> then <span class="math inline">\(e^{\beta_1} = 2\)</span>. So for every one-unit change in <span class="math inline">\(X\)</span>, we believe the expected value of <span class="math inline">\(Y\)</span> will double. Conversely, if <span class="math inline">\(\beta_1 = -0.69\)</span> then <span class="math inline">\(e^{\beta_1} = 0.5\)</span>, meaning a one-unit increase in <span class="math inline">\(X\)</span> is associated with a decrease in <span class="math inline">\(\hat{Y}\)</span> by about <span class="math inline">\(50\%\)</span>.</p>
<p>The same thing holds true for a regression where <span class="math inline">\(X\)</span> is the transformed variable:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_{1} \log(X_i) + \epsilon_i\]</span></p>
<p>Now the expected value of <span class="math inline">\(Y\)</span> varies linearly with the natural log of <span class="math inline">\(X\)</span>. Substantively, this is also a multiplicative relationship but in the “opposite direction”. That is, a multiplicative increase in <span class="math inline">\(X\)</span> has a linear increase on <span class="math inline">\(Y\)</span> of <span class="math inline">\(\beta_1\)</span> magnitude.</p>
<div id="log-log-regressions" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Log-log regressions</h4>
<p>A special case is the instance where both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are logarithmically transformed:</p>
<p><span class="math display">\[\log(Y_i) = \beta_0 + \beta_{1} \log(X_i) + \dots + \epsilon_i\]</span></p>
<p>The parameter <span class="math inline">\(\beta_1\)</span> can be interpreted as the <strong>elasticity</strong> of <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\text{Elasticity}_{YX} = \frac{\% \Delta Y}{\% \Delta X}\]</span></p>
<p>This can be interpreted a few different ways:</p>
<ul>
<li>A direct means of interpreting a nonlinear effect: a one-percent change in <span class="math inline">\(X\)</span> leads to a <span class="math inline">\(\hat{\beta}_1\)</span>-percent change in <span class="math inline">\(Y\)</span>.</li>
<li>A double multiplicative relationship: an increase in <span class="math inline">\(X\)</span> of some multiplicative factor <span class="math inline">\(\delta\)</span> leads to a multiplicative increase in <span class="math inline">\(Y\)</span> equal to <span class="math inline">\(\delta^{\beta_1}\)</span>.</li>
<li>If we have an estimate of <span class="math inline">\(\hat{\beta}_1 = 0.5\)</span> in a log-log regression, and you want to know the effect on the predicted value of <span class="math inline">\(Y\)</span> of tripling <span class="math inline">\(X\)</span>, that effect is equal to <span class="math inline">\(3^{0.5} \approx = 1.73\)</span>. That is, tripling <span class="math inline">\(X\)</span> is associated with an increase in the expected value of <span class="math inline">\(Y\)</span> by a factor of 1.73 (i.e. increase it by 73 percent).</li>
</ul>
</div>
</div>
</div>
<div id="polynomial-regressions" class="section level2">
<h2><span class="header-section-number">2.3</span> Polynomial regressions</h2>
<p><strong>Polynomial regression</strong> is a technique we have already discussed at length and is just a special case of monotonic transformations. Rather than fitting the standard linear model:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_{1}x_{i} + \epsilon_{i}\]</span></p>
<p>we instead fit a polynomial function:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_{1}x_{i} + \beta_{2}x_i^2 + \beta_{3}x_i^3 + \dots + \beta_{d}x_i^d + \epsilon_i\]</span></p>
<p>As <span class="math inline">\(d\)</span> increases, the linear model’s flexibility increases. We still use ordinary least squares (OLS) regression (or a Normal GLM) to estimate the parameters, which are also interpreted in the same way.</p>
<div id="biden-and-age" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Biden and age</h3>
<p>Let’s take a look at the Joe Biden feeling thermometer data again and estimate a polynomial regression of the relationship between age and attitudes towards Biden:</p>
<p><span class="math display">\[\text{Biden}_i = \beta_0 + \beta_1 \text{Age} + \beta_2 \text{Age}^2 + \beta_3 \text{Age}^3 + \beta_4 \text{Age}^4\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get data</span>
biden &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;biden.csv&quot;</span>))</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   biden = col_double(),
##   female = col_double(),
##   age = col_double(),
##   educ = col_double(),
##   dem = col_double(),
##   rep = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model</span>
biden_age &lt;-<span class="st"> </span><span class="kw">glm</span>(biden <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(age<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(age<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(age<span class="op">^</span><span class="dv">4</span>), <span class="dt">data =</span> biden)
<span class="kw">tidy</span>(biden_age)</code></pre></div>
<pre><code>## # A tibble: 5 x 5
##   term            estimate   std.error statistic p.value
##   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  37.5        24.9             1.50   0.133
## 2 age           2.37        2.28            1.04   0.299
## 3 I(age^2)     -0.0833      0.0730         -1.14   0.254
## 4 I(age^3)      0.00122     0.000976        1.25   0.211
## 5 I(age^4)     -0.00000622  0.00000464     -1.34   0.180</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate the predicted values and confidence interval</span>
biden_pred &lt;-<span class="st"> </span><span class="kw">augment</span>(biden_age, <span class="dt">newdata =</span> biden) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred_low =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit,
         <span class="dt">pred_high =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit)

<span class="co"># plot the curve</span>
<span class="kw">ggplot</span>(biden_pred, <span class="kw">aes</span>(age)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> biden, <span class="kw">aes</span>(age, biden), <span class="dt">alpha =</span> .<span class="dv">05</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Polynomial regression of Biden feeling thermometer&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;With 95% confidence interval&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted Biden thermometer rating&quot;</span>)</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-age-1.png" width="672" /></p>
<p>When interpreting the model, we don’t look to any individual parameters since they are all based on the same variable. Instead we fit the function to the full range of potential values for age and examine the relationship.</p>
<p>In the figure above I graphed the predicted values with 95% confidence intervals. In the case of ordinary linear regression, this is easy to estimate. The <strong>standard error</strong> is a measure of variance for the estimated parameter and defined by the square root of the diagonal of the variance-covariance matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vcov</span>(biden_age) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">kable</span>(<span class="dt">caption =</span> <span class="st">&quot;Variance-covariance matrix of Biden polynomial regression&quot;</span>,
        <span class="dt">digits =</span> <span class="dv">5</span>)</code></pre></div>
<table>
<caption><span id="tab:biden-matrix">Table 2.1: </span>Variance-covariance matrix of Biden polynomial regression</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">(Intercept)</th>
<th align="right">I(age^1)</th>
<th align="right">I(age^2)</th>
<th align="right">I(age^3)</th>
<th align="right">I(age^4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="right">620.00316</td>
<td align="right">-56.31558</td>
<td align="right">1.76432</td>
<td align="right">-0.02291</td>
<td align="right">0.00011</td>
</tr>
<tr class="even">
<td>I(age^1)</td>
<td align="right">-56.31558</td>
<td align="right">5.20765</td>
<td align="right">-0.16556</td>
<td align="right">0.00218</td>
<td align="right">-0.00001</td>
</tr>
<tr class="odd">
<td>I(age^2)</td>
<td align="right">1.76432</td>
<td align="right">-0.16556</td>
<td align="right">0.00533</td>
<td align="right">-0.00007</td>
<td align="right">0.00000</td>
</tr>
<tr class="even">
<td>I(age^3)</td>
<td align="right">-0.02291</td>
<td align="right">0.00218</td>
<td align="right">-0.00007</td>
<td align="right">0.00000</td>
<td align="right">0.00000</td>
</tr>
<tr class="odd">
<td>I(age^4)</td>
<td align="right">0.00011</td>
<td align="right">-0.00001</td>
<td align="right">0.00000</td>
<td align="right">0.00000</td>
<td align="right">0.00000</td>
</tr>
</tbody>
</table>
<p>Confidence intervals are typically plus/minus 1.96 times the standard error for the parameter. However for polynomial regression, this is more complicated. Suppose we compute the fit at a particular value of age, <span class="math inline">\(x_0\)</span>:</p>
<p><span class="math display">\[\hat{f}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_{0} + \hat{\beta}_2 x_{0}^2 + \hat{\beta}_3 x_{0}^3 + \hat{\beta}_4 x_{0}^4\]</span></p>
<p>What is the variance of the fit for this point, i.e. <span class="math inline">\(\text{Var}(\hat{f}(x_o))\)</span>. The variance is now a function not only of <span class="math inline">\(\hat{\beta}_1\)</span>, but the variance of each of the estimated parameters <span class="math inline">\(\hat{\beta}_j\)</span> as well as the covariances between the pairs of estimated parameters (i.e. the off-diagonal elements). We use all of this information to estimate the <strong>pointwise</strong> standard error of <span class="math inline">\(\hat{f}(x_0)\)</span>, which is the square-root of the variance <span class="math inline">\(\text{Var}(\hat{f}(x_o))\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot marginal effect</span>
<span class="kw">cplot</span>(biden_age, <span class="st">&quot;age&quot;</span>, <span class="dt">dx =</span> <span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;effect&quot;</span>, <span class="dt">plot =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> biden, <span class="kw">aes</span>(<span class="dt">x =</span> age)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Average marginal effect of age&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Marginal effect of age&quot;</span>)</code></pre></div>
<pre><code>## Warning in plot.window(...): &quot;plot&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in plot.xy(xy, type, ...): &quot;plot&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in axis(side = side, at = at, labels = labels, ...): &quot;plot&quot; is not
## a graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): &quot;plot&quot; is not
## a graphical parameter</code></pre>
<pre><code>## Warning in box(...): &quot;plot&quot; is not a graphical parameter</code></pre>
<pre><code>## Warning in title(...): &quot;plot&quot; is not a graphical parameter</code></pre>
<p><img src="/notes/global-methods_files/figure-html/biden-margins-1.png" width="672" /><img src="/notes/global-methods_files/figure-html/biden-margins-2.png" width="672" /></p>
</div>
<div id="voter-turnout-and-mental-health" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Voter turnout and mental health</h3>
<p>Likewise, we can use polynomial regression for classification problems as well. Consider the mental health and voting data. Let’s estimate a logistic regression model of the relationship between mental health and voter turnout:</p>
<p><span class="math display">\[\Pr(\text{Voter turnout} = \text{Yes} | \text{mhealth}) = \frac{\exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}{1 + \exp[\beta_0 + \beta_1 \text{mhealth} + \beta_2 \text{mhealth}^2 + \beta_3 \text{mhealth}^3 + \beta_4 \text{mhealth}^4]}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load data</span>
mh &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;mental_health.csv&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>na.omit</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   vote96 = col_double(),
##   mhealth_sum = col_double(),
##   age = col_double(),
##   educ = col_double(),
##   black = col_double(),
##   female = col_double(),
##   married = col_double(),
##   inc10 = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model</span>
mh_mod &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>mhealth_sum <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(mhealth_sum<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span>
<span class="st">                </span><span class="kw">I</span>(mhealth_sum<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(mhealth_sum<span class="op">^</span><span class="dv">4</span>), <span class="dt">data =</span> mh,
              <span class="dt">family =</span> binomial)
<span class="kw">tidy</span>(mh_mod)</code></pre></div>
<pre><code>## # A tibble: 5 x 5
##   term              estimate std.error statistic  p.value
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)       1.13      0.132         8.54 1.38e-17
## 2 mhealth_sum       0.294     0.175         1.68 9.31e- 2
## 3 I(mhealth_sum^2) -0.196     0.0623       -3.16 1.60e- 3
## 4 I(mhealth_sum^3)  0.0228    0.00738       3.09 1.97e- 3
## 5 I(mhealth_sum^4) -0.000749  0.000271     -2.76 5.70e- 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># log-odds</span>
## predicted probability
pred_mh &lt;-<span class="st"> </span><span class="kw">cplot</span>(mh_mod, <span class="st">&quot;mhealth_sum&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>,
                 <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Log-odds of voting&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Mental health&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted log-odds of voting&quot;</span>)</code></pre></div>
<pre><code>##         xvals       yvals       upper       lower
## 1   0.0000000  1.12571274  1.38416215  0.86726333
## 2   0.6666667  1.24112780  1.42212873  1.06012686
## 3   1.3333333  1.22042721  1.41286952  1.02798490
## 4   2.0000000  1.09889602  1.30263898  0.89515307
## 5   2.6666667  0.90826810  1.10807306  0.70846314
## 6   3.3333333  0.67672612  0.86998426  0.48346797
## 7   4.0000000  0.42890155  0.62723331  0.23056978
## 8   4.6666667  0.18587469  0.40418523 -0.03243586
## 9   5.3333333 -0.03482536  0.21069543 -0.28034616
## 10  6.0000000 -0.21922069  0.05175913 -0.49020051
## 11  6.6666667 -0.35688457 -0.06663363 -0.64713551
## 12  7.3333333 -0.44094146 -0.13631636 -0.74556656
## 13  8.0000000 -0.46806703 -0.14744985 -0.78868421
## 14  8.6666667 -0.43848811 -0.09102462 -0.78595159
## 15  9.3333333 -0.35598273  0.03593576 -0.74790123
## 16 10.0000000 -0.22788012  0.22584455 -0.68160480
## 17 10.6666667 -0.06506070  0.46096817 -0.59108956
## 18 11.3333333  0.11804395  0.71726375 -0.48117586
## 19 12.0000000  0.30345102  0.96862635 -0.36172430
## 20 12.6666667  0.46962655  1.19208459 -0.25283149</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## marginal effect
ame_mh &lt;-<span class="st"> </span><span class="kw">cplot</span>(mh_mod, <span class="st">&quot;mhealth_sum&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;effect&quot;</span>,
                 <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Average marginal effect&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Mental health&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Marginal effect of mental health&quot;</span>)

pred_mh <span class="op">+</span>
<span class="st">  </span>ame_mh</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/mhealth-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># probability</span>
## predicted probability
pred_mh &lt;-<span class="st"> </span><span class="kw">cplot</span>(mh_mod, <span class="st">&quot;mhealth_sum&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Mental health&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##         xvals     yvals     upper     lower
## 1   0.0000000 0.7550468 0.8028473 0.7072463
## 2   0.6666667 0.7757603 0.8072465 0.7442740
## 3   1.3333333 0.7721387 0.8059971 0.7382803
## 4   2.0000000 0.7500532 0.7882496 0.7118568
## 5   2.6666667 0.7126456 0.7535621 0.6717292
## 6   3.3333333 0.6630076 0.7061870 0.6198282
## 7   4.0000000 0.6056113 0.6529821 0.5582405
## 8   4.6666667 0.5463353 0.6004443 0.4922264
## 9   5.3333333 0.4912945 0.5526561 0.4299329
## 10  6.0000000 0.4454133 0.5123508 0.3784757
## 11  6.6666667 0.4117139 0.4820143 0.3414135
## 12  7.3333333 0.3915167 0.4640879 0.3189454
## 13  8.0000000 0.3850739 0.4609934 0.3091543
## 14  8.6666667 0.3921013 0.4749219 0.3092806
## 15  9.3333333 0.4119324 0.5068723 0.3169924
## 16 10.0000000 0.4432752 0.5552465 0.3313040
## 17 10.6666667 0.4837406 0.6151087 0.3523724
## 18 11.3333333 0.5294768 0.6787611 0.3801925
## 19 12.0000000 0.5752859 0.7378096 0.4127623
## 20 12.6666667 0.6152954 0.7863062 0.4442845</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## marginal effect
ame_mh &lt;-<span class="st"> </span><span class="kw">cplot</span>(mh_mod, <span class="st">&quot;mhealth_sum&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;effect&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Average marginal effect&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Mental health&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Marginal effect of mental health&quot;</span>)

pred_mh <span class="op">+</span>
<span class="st">  </span>ame_mh</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/mhealth-2.png" width="672" /></p>
<p>With logistic regression we presume the relationship between <span class="math inline">\(X\)</span> and the log-odds of <span class="math inline">\(Y\)</span> is linear, whereas the relationship between <span class="math inline">\(X\)</span> and the probability of <span class="math inline">\(Y\)</span> is curvilinear, defined by the logit function <span class="math inline">\(\log \left( \frac{\Pr(Y)}{1 - \Pr(Y)} \right)\)</span>. Now with the polynomial terms, even the predicted log-odds relationship is <strong>curvilinear</strong>. Notice that the confidence intervals are narrow for lower mental health scores and larger for higher values. This is because there are fewer individuals with high levels of depression, so the variance is larger around these scores for mental health.</p>
</div>
</div>
</div>
<div id="step-functions" class="section level1">
<h1><span class="header-section-number">3</span> Step functions</h1>
<p>Monotonic transformations impose a <strong>global</strong> structure on the non-linear function of <span class="math inline">\(X\)</span>. That is, the function applies across the entire range of <span class="math inline">\(X\)</span>. Instead, we may believe that the non-linear behavior of <span class="math inline">\(X\)</span> is instead a <strong>local</strong> structure that varies across the range of <span class="math inline">\(X\)</span>. We can use <strong>step functions</strong> to avoid imposing a global structure. In step functions, you split the predictor <span class="math inline">\(X\)</span> into <strong>bins</strong> and fit a different constant to each bin. This converts a continuous variable into an <strong>ordered categorical variable</strong>.</p>
<p>We create <span class="math inline">\(c_1, c_2, \dots, c_K\)</span> cutpoints in the range of <span class="math inline">\(X\)</span>, and construct <span class="math inline">\(K + 1\)</span> new variables <span class="math inline">\(C_1(X), C_2(X), \dots, C_K(X)\)</span>. Each of the new variables is an <strong>indicator</strong> or <strong>dummy</strong> variable that returns a 1 if <span class="math inline">\(x_i\)</span> falls within the range of the cutpoints and 0 otherwise. For any given <span class="math inline">\(x_i\)</span>, the sum of these variables will be precisely <span class="math inline">\(1\)</span>. We can then fit the linear regression model to the new indicator variables as predictors:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 C_1 (x_i) + \beta_2 C_2 (x_i) + \dots + \beta_K C_K (x_i) + \epsilon_i\]</span></p>
<div id="age-and-voting" class="section level2">
<h2><span class="header-section-number">3.1</span> Age and voting</h2>
<p>We can fit a piecewise constant regression to the voting data set, regressing age on the probability of voting. Dividing age into 5 bins looks like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plain model</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span>age, <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;No step function&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4779743 0.5348983 0.4210502
## 2  22.875 0.5029885 0.5549411 0.4510359
## 3  25.750 0.5279877 0.5749931 0.4809824
## 4  28.625 0.5528474 0.5950949 0.5106000
## 5  31.500 0.5774456 0.6153122 0.5395791
## 6  34.375 0.6016657 0.6357339 0.5675974
## 7  37.250 0.6253980 0.6564561 0.5943400
## 8  40.125 0.6485424 0.6775438 0.6195410
## 9  43.000 0.6710090 0.6989747 0.6430434
## 10 45.875 0.6927200 0.7205980 0.6648420
## 11 48.750 0.7136097 0.7421509 0.6850684
## 12 51.625 0.7336253 0.7633259 0.7039247
## 13 54.500 0.7527268 0.7838390 0.7216146
## 14 57.375 0.7708863 0.8034666 0.7383060
## 15 60.250 0.7880876 0.8220516 0.7541237
## 16 63.125 0.8043254 0.8394959 0.7691548
## 17 66.000 0.8196037 0.8557479 0.7834594
## 18 68.875 0.8339353 0.8707921 0.7970786
## 19 71.750 0.8473405 0.8846395 0.8100416
## 20 74.625 0.8598454 0.8973211 0.8223698</code></pre>
<p><img src="/notes/global-methods_files/figure-html/vote96-step-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># polynomial model</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(age, <span class="dt">degree =</span> <span class="dv">5</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Fifth-order polynomial&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.3775270 0.5409587 0.2140954
## 2  22.875 0.4393519 0.5236684 0.3550354
## 3  25.750 0.4845746 0.5478418 0.4213074
## 4  28.625 0.5204955 0.5805168 0.4604741
## 5  31.500 0.5531903 0.6069805 0.4994000
## 6  34.375 0.5867036 0.6346386 0.5387687
## 7  37.250 0.6229922 0.6695651 0.5764193
## 8  40.125 0.6621109 0.7095919 0.6146299
## 9  43.000 0.7026023 0.7495562 0.6556485
## 10 45.875 0.7421207 0.7865259 0.6977155
## 11 48.750 0.7781904 0.8198925 0.7364884
## 12 51.625 0.8088498 0.8494750 0.7682246
## 13 54.500 0.8329463 0.8741543 0.7917384
## 14 57.375 0.8500464 0.8924344 0.8076584
## 15 60.250 0.8601067 0.9034699 0.8167436
## 16 63.125 0.8630967 0.9071461 0.8190473
## 17 66.000 0.8586980 0.9038325 0.8135634
## 18 68.875 0.8461551 0.8945535 0.7977566
## 19 71.750 0.8243799 0.8811625 0.7675973
## 20 74.625 0.7925149 0.8648739 0.7201560</code></pre>
<p><img src="/notes/global-methods_files/figure-html/vote96-step-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># stepwise model with 5 intervals</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">cut_interval</span>(age, <span class="dv">5</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fitted), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fitted), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;5 intervals&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/vote96-step-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># stepwise model with 10 intervals</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">cut_interval</span>(age, <span class="dv">10</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fitted), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fitted), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;10 intervals&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/vote96-step-4.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># stepwise model with 25 intervals</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">cut_interval</span>(age, <span class="dv">25</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span>prediction <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> age)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fitted), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fitted), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;25 intervals&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/vote96-step-5.png" width="672" /></p>
<p>The drawback to step functions is unless there are natural breakpoints in the predictors, piecewise-constant functions can easily miss the actual relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<div id="basis-functions" class="section level1">
<h1><span class="header-section-number">4</span> Basis functions</h1>
<p><strong>Basis functions</strong> are a family of functions or transformations applied to a variable <span class="math inline">\(X\)</span>: <span class="math inline">\(b_1(X), b_2(X), \ldots, b_K(X)\)</span>. Instead of fitting the linear model to <span class="math inline">\(X\)</span>, we fit the model:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \ldots + \beta_K b_K(x_i) + \epsilon_i\]</span></p>
<p>The functional form of the basis function is determined in advanced and is fixed and known. Polynomial and step functions are specific types of basis functions. For polynomial regression, <span class="math inline">\(b_j(x_i) = x_i^j\)</span> where <span class="math inline">\(j\)</span> is the polynomial degree. Since this linear model is just a transformation of the predictors <span class="math inline">\(X_i\)</span>, we can use least squares to estimate the model and apply all the standard statistical inferential techniques to evaluate and interpret the model.</p>
</div>
<div id="regression-splines" class="section level1">
<h1><span class="header-section-number">5</span> Regression splines</h1>
<p><strong>Regression splines</strong> extend polynomial transformations and piecewise constant regression by fitting separate polynomial functions over different regions of <span class="math inline">\(X\)</span>.</p>
<div id="piecewise-polynomials" class="section level2">
<h2><span class="header-section-number">5.1</span> Piecewise polynomials</h2>
<p><strong>Piecewise polynomial regression</strong> fits separate low-degree polynomials for different regions of <span class="math inline">\(X\)</span>. For instance, a cubic piecewise polynomial regression model takes the form:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i\]</span></p>
<p>where the <span class="math inline">\(\beta\)</span>s are now vectors with different values for different parts of the range of <span class="math inline">\(X\)</span>. The points where the parameters change are called <strong>knots</strong>.</p>
<p>The special case of a piecewise cubic polynomial with 0 knots is just an ordinary cubic polynomial:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i\]</span></p>
<p>Likewise, the special case of a piecewise constant polynomial (i.e. polynomial with degree <span class="math inline">\(0\)</span>) is piecewise constant regression.</p>
<p>A piecewise cubic polynomial with a single knot at point <span class="math inline">\(c\)</span> takes the form:</p>
<p><span class="math display">\[y_i = \begin{cases} 
      \beta_{01} + \beta_{11}x_i^2 + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i &amp; \text{if } x_i &lt; c \\
      \beta_{02} + \beta_{12}x_i^2 + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i &amp; \text{if } x_i \geq c
   \end{cases}\]</span></p>
<p>In essence, we fit two separate polynomial functions to the data. Each function is estimated using least squares applied to the individual functions and subsets of data. The more knots <span class="math inline">\(K\)</span> used, the more flexible the piecewise polynomial.</p>
<p>While the example here uses a cubic (third-order) polynomial, we don’t have to use it. We could select any higher-order degree for the polynomial.</p>
<p>Of course this leads to a somewhat odd and discontinuous function:</p>
<p><img src="/notes/global-methods_files/figure-html/sim-piecewise-1.png" width="672" /></p>
</div>
<div id="constraints-and-splines" class="section level2">
<h2><span class="header-section-number">5.2</span> Constraints and splines</h2>
<p>Instead of a discontinuous piecewise function, we’d rather have something continuous. That is, we impose the <strong>constraint</strong> that the fitted curve must be continuous.</p>
<p><img src="/notes/global-methods_files/figure-html/sim-spline-1.png" width="672" /></p>
<p>But notice that this constraint is insufficient. Now the function is continuous, but still looks unnatural because of the V-shaped join. We should add two additional constraints: not only should the fitted curve be continuous, but the first and second <strong>derivatives</strong> should also be continuous at the knot. This will generate a fitted curve that is continuous and <strong>smooth</strong>.</p>
<p><img src="/notes/global-methods_files/figure-html/sim-spline-smooth-1.png" width="672" /></p>
<p>By increasing the number of knots, we will increase the flexibility of the resulting spline:</p>
<p><img src="/notes/global-methods_files/figure-html/sim-spline-smooth-5-1.png" width="672" /></p>
</div>
<div id="basis-splines-vs.natural-splines" class="section level2">
<h2><span class="header-section-number">5.3</span> Basis splines vs. natural splines</h2>
<p>One drawback to the splines as currently presented (known as <strong>basis splines</strong>) is that they have high variance at the outer range of the predictors. Consider a basis spline with 3 knots applied to the voting and age logistic regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Basis spline with 3 knots&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4282572 0.6315037 0.2250108
## 2  22.875 0.4302940 0.5164704 0.3441176
## 3  25.750 0.4627694 0.5435835 0.3819553
## 4  28.625 0.5123251 0.5778767 0.4467736
## 5  31.500 0.5643390 0.6248809 0.5037972
## 6  34.375 0.6046836 0.6687555 0.5406117
## 7  37.250 0.6321075 0.6856957 0.5785192
## 8  40.125 0.6572544 0.7098796 0.6046293
## 9  43.000 0.6899492 0.7485517 0.6313467
## 10 45.875 0.7318233 0.7852485 0.6783982
## 11 48.750 0.7748341 0.8202808 0.7293873
## 12 51.625 0.8122626 0.8565635 0.7679616
## 13 54.500 0.8402300 0.8873578 0.7931022
## 14 57.375 0.8570720 0.9048012 0.8093428
## 15 60.250 0.8638145 0.9093047 0.8183243
## 16 63.125 0.8626314 0.9058251 0.8194377
## 17 66.000 0.8545801 0.8984857 0.8106745
## 18 68.875 0.8399964 0.8899250 0.7900678
## 19 71.750 0.8188883 0.8797559 0.7580207
## 20 74.625 0.7912946 0.8650531 0.7175361</code></pre>
<p><img src="/notes/global-methods_files/figure-html/vote-age-basis-spline-1.png" width="672" /></p>
<p>Notice the 95% confidence interval balloons at both low and high values for age. To control for this, we impose <strong>boundary constraints</strong>: the function is required to be linear at the boundary (the region where <span class="math inline">\(X\)</span> is smaller than the smallest knot, or larger than the largest knot). This type of spline is known as a <strong>natural spline</strong>. This leads to more stable estimates at the boundaries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate basis spline model</span>
vote_age_basis &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4282572 0.6315037 0.2250108
## 2  22.875 0.4302940 0.5164704 0.3441176
## 3  25.750 0.4627694 0.5435835 0.3819553
## 4  28.625 0.5123251 0.5778767 0.4467736
## 5  31.500 0.5643390 0.6248809 0.5037972
## 6  34.375 0.6046836 0.6687555 0.5406117
## 7  37.250 0.6321075 0.6856957 0.5785192
## 8  40.125 0.6572544 0.7098796 0.6046293
## 9  43.000 0.6899492 0.7485517 0.6313467
## 10 45.875 0.7318233 0.7852485 0.6783982
## 11 48.750 0.7748341 0.8202808 0.7293873
## 12 51.625 0.8122626 0.8565635 0.7679616
## 13 54.500 0.8402300 0.8873578 0.7931022
## 14 57.375 0.8570720 0.9048012 0.8093428
## 15 60.250 0.8638145 0.9093047 0.8183243
## 16 63.125 0.8626314 0.9058251 0.8194377
## 17 66.000 0.8545801 0.8984857 0.8106745
## 18 68.875 0.8399964 0.8899250 0.7900678
## 19 71.750 0.8188883 0.8797559 0.7580207
## 20 74.625 0.7912946 0.8650531 0.7175361</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vote_age_natural &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">6</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4104826 0.5694648 0.2515004
## 2  22.875 0.4360929 0.5241522 0.3480335
## 3  25.750 0.4671766 0.5368465 0.3975066
## 4  28.625 0.5088239 0.5837505 0.4338973
## 5  31.500 0.5624939 0.6227874 0.5022005
## 6  34.375 0.6105011 0.6791545 0.5418478
## 7  37.250 0.6345767 0.6965424 0.5726110
## 8  40.125 0.6485917 0.7086990 0.5884845
## 9  43.000 0.6785472 0.7470780 0.6100163
## 10 45.875 0.7301420 0.7856302 0.6746538
## 11 48.750 0.7836706 0.8343419 0.7329993
## 12 51.625 0.8239502 0.8782916 0.7696088
## 13 54.500 0.8470984 0.8976731 0.7965238
## 14 57.375 0.8581347 0.9018701 0.8143992
## 15 60.250 0.8606929 0.9014754 0.8199105
## 16 63.125 0.8568879 0.9017039 0.8120720
## 17 66.000 0.8480399 0.8996926 0.7963872
## 18 68.875 0.8344507 0.8913064 0.7775950
## 19 71.750 0.8158132 0.8755124 0.7561140
## 20 74.625 0.7917365 0.8538166 0.7296564</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">bind_rows</span>(
  <span class="dt">Basis =</span> vote_age_basis,
  <span class="dt">Natural =</span> vote_age_natural,
  <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals, <span class="dt">color =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper, <span class="dt">color =</span> model), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower, <span class="dt">color =</span> model), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Spline&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/vote-age-natural-spline-1.png" width="672" /></p>
</div>
<div id="choosing-the-number-and-location-of-knots" class="section level2">
<h2><span class="header-section-number">5.4</span> Choosing the number and location of knots</h2>
<p>Typically knots are placed in a uniform fashion; that is, the cutpoints <span class="math inline">\(c\)</span> are determined by first identifying the number of knots <span class="math inline">\(K\)</span> for the model, then partitioning <span class="math inline">\(X\)</span> into uniform quantiles. So if we fit a cubic regression spline with 5 knots on the voting and age logistic regression, it would divide <code>age</code> into 5 equal quantiles<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> and estimate the cubic spline function for each quantile:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model</span>
<span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">8</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">attr</span>(<span class="kw">bs</span>(mh<span class="op">$</span>age, <span class="dt">df =</span> <span class="dv">8</span>), <span class="st">&quot;knots&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Natural spline with 5 knots&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4365875 0.6152524 0.2579225
## 2  22.875 0.4328343 0.5205456 0.3451230
## 3  25.750 0.4498344 0.5346405 0.3650284
## 4  28.625 0.5067269 0.5790485 0.4344054
## 5  31.500 0.5821232 0.6651290 0.4991173
## 6  34.375 0.6231946 0.6923669 0.5540224
## 7  37.250 0.6237312 0.7086129 0.5388495
## 8  40.125 0.6228755 0.6946885 0.5510625
## 9  43.000 0.6680380 0.7513065 0.5847694
## 10 45.875 0.7529138 0.8133569 0.6924707
## 11 48.750 0.8127571 0.8773629 0.7481513
## 12 51.625 0.8315533 0.8859030 0.7772036
## 13 54.500 0.8316128 0.8860993 0.7771264
## 14 57.375 0.8309324 0.8956626 0.7662023
## 15 60.250 0.8363073 0.8975423 0.7750724
## 16 63.125 0.8434146 0.8953149 0.7915143
## 17 66.000 0.8478102 0.8979973 0.7976231
## 18 68.875 0.8453973 0.9022059 0.7885887
## 19 71.750 0.8334613 0.8969400 0.7699826
## 20 74.625 0.8114817 0.8792868 0.7436766</code></pre>
<p><img src="/notes/global-methods_files/figure-html/vote-spline-1.png" width="672" /></p>
<p>But this still leaves unaddressed the matter of how many knots should we use? Or how many degrees should each polynomial be? While theory should still be our guide, we can also use <a href="/notes/cross-validation/">cross-validation</a> to determine the optimal number of knots and/or polynomial degrees. For our voting and age model, we can estimate <span class="math inline">\(10\)</span>-fold CV MSE for varying numbers of knots for a cubic natural spline:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to simplify things</span>
vote_spline &lt;-<span class="st"> </span><span class="cf">function</span>(splits, <span class="dt">df =</span> <span class="ot">NULL</span>){
  <span class="co"># estimate the model on each fold</span>
  model &lt;-<span class="st"> </span><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> df),
                <span class="dt">data =</span> <span class="kw">analysis</span>(splits))
  
  model_acc &lt;-<span class="st"> </span><span class="kw">augment</span>(model, <span class="dt">newdata =</span> <span class="kw">assessment</span>(splits)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">accuracy</span>(<span class="dt">truth =</span> <span class="kw">factor</span>(vote96), <span class="dt">estimate =</span> <span class="kw">factor</span>(<span class="kw">round</span>(.fitted)))
  
  <span class="kw">mean</span>(model_acc<span class="op">$</span>.estimate)
}

tune_over_knots &lt;-<span class="st"> </span><span class="cf">function</span>(splits, knots){
  <span class="kw">vote_spline</span>(splits, <span class="dt">df =</span> knots <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)
}

<span class="co"># estimate CV error for knots in 0:25</span>
results &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(mh, <span class="dt">v =</span> <span class="dv">10</span>)

<span class="kw">expand</span>(results, id, <span class="dt">knots =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(results) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">acc =</span> <span class="kw">map2_dbl</span>(splits, knots, tune_over_knots)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(knots) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">acc =</span> <span class="kw">mean</span>(acc)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">err =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>acc) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(knots, err)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>percent) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Optimal number of knots for natural cubic spline regression&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Knots&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;10-fold CV error&quot;</span>)</code></pre></div>
<pre><code>## Joining, by = &quot;id&quot;</code></pre>
<p><img src="/notes/global-methods_files/figure-html/wage-cv-1.png" width="672" /></p>
<p>These results (weakly) suggest the optimal number of knots is around 7. The resulting model produced by these parameters is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glm</span>(vote96 <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(age, <span class="dt">df =</span> <span class="dv">10</span>), <span class="dt">data =</span> mh, <span class="dt">family =</span> binomial) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">cplot</span>(<span class="st">&quot;age&quot;</span>, <span class="dt">what =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">draw =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> xvals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> yvals)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> upper), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> lower), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">attr</span>(<span class="kw">bs</span>(mh<span class="op">$</span>age, <span class="dt">df =</span> <span class="dv">10</span>), <span class="st">&quot;knots&quot;</span>),
             <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">0</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;b&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="dt">data =</span> <span class="kw">filter</span>(mh, vote96 <span class="op">==</span><span class="st"> </span><span class="dv">1</span>), <span class="kw">aes</span>(age), <span class="dt">alpha =</span> .<span class="dv">02</span>, <span class="dt">sides =</span> <span class="st">&quot;t&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Natural spline with 9 knots&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of voting&quot;</span>)</code></pre></div>
<pre><code>##     xvals     yvals     upper     lower
## 1  20.000 0.4282471 0.6252063 0.2312880
## 2  22.875 0.4348659 0.5276855 0.3420463
## 3  25.750 0.4556856 0.5549905 0.3563806
## 4  28.625 0.5031728 0.5913505 0.4149951
## 5  31.500 0.5759378 0.6593652 0.4925103
## 6  34.375 0.6289635 0.7183163 0.5396107
## 7  37.250 0.6243092 0.7120114 0.5366070
## 8  40.125 0.6185019 0.6979372 0.5390666
## 9  43.000 0.6683266 0.7593541 0.5772991
## 10 45.875 0.7578417 0.8313389 0.6843446
## 11 48.750 0.8104157 0.8759382 0.7448932
## 12 51.625 0.8263481 0.8944110 0.7582853
## 13 54.500 0.8319732 0.9015882 0.7623582
## 14 57.375 0.8351908 0.8945150 0.7758667
## 15 60.250 0.8384688 0.9091522 0.7677855
## 16 63.125 0.8428727 0.9123006 0.7734447
## 17 66.000 0.8456804 0.9030068 0.7883540
## 18 68.875 0.8435757 0.8995645 0.7875869
## 19 71.750 0.8330172 0.8994600 0.7665745
## 20 74.625 0.8119896 0.8872808 0.7366983</code></pre>
<p><img src="/notes/global-methods_files/figure-html/wage-optimal-mod-1.png" width="672" /></p>
</div>
</div>
<div id="smoothing-splines" class="section level1">
<h1><span class="header-section-number">6</span> Smoothing splines</h1>
</div>
<div id="multivariate-adaptive-regression-splines-mars" class="section level1">
<h1><span class="header-section-number">7</span> Multivariate adaptive regression splines (MARS)</h1>
</div>
<div id="local-regression" class="section level1">
<h1><span class="header-section-number">8</span> Local regression</h1>
<p><strong>Locally weighted scatterplot smoothing</strong> (local regression, LOWESS, or LOESS) fits a separate non-linear function at each target point <span class="math inline">\(x_0\)</span> using only the nearby training observations. This method estimates a regression line based on localized subsets of the data, building up the global function <span class="math inline">\(f\)</span> point-by-point.</p>
<div id="algorithm-for-local-linear-regression" class="section level2">
<h2><span class="header-section-number">8.1</span> Algorithm for local linear regression</h2>
<ol style="list-style-type: decimal">
<li>Gather the fraction <span class="math inline">\(s = \frac{k}{n}\)</span> of training points whose <span class="math inline">\(x_i\)</span> are closest to <span class="math inline">\(x_0\)</span>.</li>
<li>Assign a weight <span class="math inline">\(K_{i0} = K(x_i, x_0)\)</span> to each point in the neighborhood, so that the point furthest from <span class="math inline">\(x_0\)</span> has a weight of 0, and the closest has the highest weight. All but these <span class="math inline">\(k\)</span> nearest neighbors get a weight of 0.</li>
<li>Fit a <strong>weighted least squares regression</strong> of the <span class="math inline">\(y_i\)</span> on the <span class="math inline">\(x_i\)</span> using the aformentioned weights. Weighted least squares allows us to adjust the variance for each observation <span class="math inline">\(\sigma_i^2\)</span> by weights <span class="math inline">\(w_i\)</span>. This is useful in correcting for heteroscedasticity in OLS, and also allows us to estimate the local regression.</li>
<li>The fitted value at <span class="math inline">\(x_0\)</span> is given by <span class="math inline">\(\hat{f}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0\)</span></li>
</ol>
<p>To obtain the local regression fit at a new value of <span class="math inline">\(x_0\)</span>, we have to perform this same procedure again adjusting the weights for the new region of data.</p>
<p>Here is an example of a local linear regression on the <code>ethanol</code> dataset in the <code>lattice</code> package:</p>
<p><img src="/notes/global-methods_files/figure-html/loess-1.png" width="672" /></p>
<p>The LOESS is built up point-by-point:</p>
<p><img src="/notes/global-methods_files/figure-html/loess_buildup-1.gif" /><!-- --></p>
<p>One important argument you can control with LOESS is the <strong>span</strong>, or how smooth the LOESS function will become. A larger span will result in a smoother curve, but may not be as accurate.</p>
<p><img src="/notes/global-methods_files/figure-html/loess_span-1.gif" /><!-- --></p>
</div>
</div>
<div id="generalized-additive-models" class="section level1">
<h1><span class="header-section-number">9</span> Generalized additive models</h1>
<p>So far each of these nonlinear methods has been implemented for a single predictor <span class="math inline">\(X\)</span>. To generalize this approach to multiple predictors <span class="math inline">\(X_1, X_2, \dots, X_p\)</span>, we need to utilize a <strong>generalized additive model</strong> (GAM). GAMs extend the linear model by allowing non-linear functions of each of the variables, while also maintaining the <strong>additive assumption</strong> that each variable independently and additively shapes the response variable <span class="math inline">\(Y\)</span>. GAMs work for both quantitative and qualitative responses.</p>
<div id="gams-for-regression-problems" class="section level2">
<h2><span class="header-section-number">9.1</span> GAMs for regression problems</h2>
<p>To extend the multiple linear regression model</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_{1} X_{i1} + \beta_{2} X_{i2} + \dots + \beta_{p} X_{ip} + \epsilon_i\]</span></p>
<p>and allow for non-linear relationships between each predictor and the response variable, we replace each linear component <span class="math inline">\(\beta_{j} x_{ij}\)</span> with a smooth, non-linear function <span class="math inline">\(f_j(x_{ij})\)</span>:</p>
<p><span class="math display">\[y_i = \beta_0 + \sum_{j = 1}^p f_j(x_{ij}) + \epsilon_i\]</span></p>
<p><span class="math display">\[y_i = \beta_0 + f_1(x_{i1}) + \beta_{2} f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i\]</span></p>
<p>As you can see, each <span class="math inline">\(f_j\)</span> provides an <strong>additive</strong> component to the overall model of <span class="math inline">\(y_i\)</span>. We estimate each <span class="math inline">\(f_j\)</span> then add together all of their contributions. Importantly, each functional component is estimated <strong>simultaneously</strong> so that it is truly still a multiple variable regression model. Let’s estimate a GAM for the Biden dataset using the model</p>
<p><span class="math display">\[\text{Biden} = \beta_0 + f_1(\text{Age}) + f_2(\text{Education}) + f_3(\text{Gender}) + \epsilon\]</span></p>
<p>Where <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are cubic splines with 2 knots and <span class="math inline">\(f_3\)</span> generates a separate constant for males and females using traditional dummy variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gam)

<span class="co"># estimate model for splines on age and education plus dichotomous female</span>
biden_gam &lt;-<span class="st"> </span><span class="kw">gam</span>(biden <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">df =</span> <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span><span class="kw">bs</span>(educ, <span class="dt">df =</span> <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span>female, <span class="dt">data =</span> biden)
<span class="kw">summary</span>(biden_gam)</code></pre></div>
<pre><code>## 
## Call: gam(formula = biden ~ bs(age, df = 5) + bs(educ, df = 5) + female, 
##     data = biden)
## Deviance Residuals:
##      Min       1Q   Median       3Q      Max 
## -67.4989 -13.5223   0.5484  17.9535  46.5817 
## 
## (Dispersion Parameter for gaussian family taken to be 532.4635)
## 
##     Null Deviance: 994143.5 on 1806 degrees of freedom
## Residual Deviance: 955771.9 on 1795 degrees of freedom
## AIC: 16485.47 
## 
## Number of Local Scoring Iterations: 2 
## 
## Anova for Parametric Effects
##                    Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## bs(age, df = 5)     5   4038   807.5  1.5166    0.1814    
## bs(educ, df = 5)    5  16660  3332.1  6.2579 9.174e-06 ***
## female              1  17673 17673.4 33.1918 9.805e-09 ***
## Residuals        1795 955772   532.5                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get graphs of each term</span>
biden_gam_terms &lt;-<span class="st"> </span><span class="kw">preplot</span>(biden_gam, <span class="dt">se =</span> <span class="ot">TRUE</span>, <span class="dt">rug =</span> <span class="ot">FALSE</span>)

## age
<span class="kw">tibble</span>(<span class="dt">x =</span> biden_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(age, df = 5)</span><span class="st">`</span><span class="op">$</span>x,
           <span class="dt">y =</span> biden_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(age, df = 5)</span><span class="st">`</span><span class="op">$</span>y,
           <span class="dt">se.fit =</span> biden_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(age, df = 5)</span><span class="st">`</span><span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Biden feeling thermometer&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Cubic spline&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">1</span>](age)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-gam-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## education
<span class="kw">tibble</span>(<span class="dt">x =</span> biden_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(educ, df = 5)</span><span class="st">`</span><span class="op">$</span>x,
           <span class="dt">y =</span> biden_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(educ, df = 5)</span><span class="st">`</span><span class="op">$</span>y,
           <span class="dt">se.fit =</span> biden_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(educ, df = 5)</span><span class="st">`</span><span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Biden feeling thermometer&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Cubic spline&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Education&quot;</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">2</span>](education)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-gam-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## gender
<span class="kw">tibble</span>(<span class="dt">x =</span> biden_gam_terms<span class="op">$</span>female<span class="op">$</span>x,
           <span class="dt">y =</span> biden_gam_terms<span class="op">$</span>female<span class="op">$</span>y,
           <span class="dt">se.fit =</span> biden_gam_terms<span class="op">$</span>female<span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span>unique <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">x =</span> <span class="kw">factor</span>(x, <span class="dt">levels =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y, <span class="dt">ymin =</span> y_low, <span class="dt">ymax =</span> y_high)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Biden feeling thermometer&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">3</span>](gender)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-gam-3.png" width="672" /></p>
<p>For age, there does not appear to be a substantial or significant relationship with Biden feeling thermometers after controlling for education and gender. The cubic spline is relatively flat and the 95% confidence interval is wide. For education, the effect appears substantial and statistically significant; as education increases, predicted Biden feeling thermometer ratings decrease until approximately 15 years of formal education, then increase again for those with college or post-graduate degrees. Finally, for gender the difference between males and females is substantial and statistically distinguishable from 0.</p>
<p>Instead of cubic splines, we could use local regression. At this point we can no longer use OLS to fit the model, but instead uses a <strong>backfitting</strong> process to model each predictor simultaneously.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate model for splines on age and education plus dichotomous female</span>
biden_gam_local &lt;-<span class="st"> </span><span class="kw">gam</span>(biden <span class="op">~</span><span class="st"> </span><span class="kw">lo</span>(age) <span class="op">+</span><span class="st"> </span><span class="kw">lo</span>(educ) <span class="op">+</span><span class="st"> </span>female, <span class="dt">data =</span> biden)
<span class="kw">summary</span>(biden_gam_local)</code></pre></div>
<pre><code>## 
## Call: gam(formula = biden ~ lo(age) + lo(educ) + female, data = biden)
## Deviance Residuals:
##    Min     1Q Median     3Q    Max 
## -66.66 -13.92   1.28  18.97  44.03 
## 
## (Dispersion Parameter for gaussian family taken to be 533.8858)
## 
##     Null Deviance: 994143.5 on 1806 degrees of freedom
## Residual Deviance: 959708.2 on 1797.591 degrees of freedom
## AIC: 16487.72 
## 
## Number of Local Scoring Iterations: 2 
## 
## Anova for Parametric Effects
##               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## lo(age)      1.0   1406  1406.4  2.6344    0.1047    
## lo(educ)     1.0   8338  8337.7 15.6171 8.055e-05 ***
## female       1.0  17750 17750.3 33.2474 9.532e-09 ***
## Residuals 1797.6 959708   533.9                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Anova for Nonparametric Effects
##             Npar Df Npar F    Pr(F)   
## (Intercept)                           
## lo(age)         2.4 0.8267 0.454360   
## lo(educ)        3.1 3.8734 0.008521 **
## female                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get graphs of each term</span>
biden_gam_local_terms &lt;-<span class="st"> </span><span class="kw">preplot</span>(biden_gam_local, <span class="dt">se =</span> <span class="ot">TRUE</span>, <span class="dt">rug =</span> <span class="ot">FALSE</span>)

## age
<span class="kw">tibble</span>(<span class="dt">x =</span> biden_gam_local_terms<span class="op">$</span><span class="st">`</span><span class="dt">lo(age)</span><span class="st">`</span><span class="op">$</span>x,
           <span class="dt">y =</span> biden_gam_local_terms<span class="op">$</span><span class="st">`</span><span class="dt">lo(age)</span><span class="st">`</span><span class="op">$</span>y,
           <span class="dt">se.fit =</span> biden_gam_local_terms<span class="op">$</span><span class="st">`</span><span class="dt">lo(age)</span><span class="st">`</span><span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Biden feeling thermometer&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Local regression&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">1</span>](age)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-gam-local-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## education
<span class="kw">tibble</span>(<span class="dt">x =</span> biden_gam_local_terms<span class="op">$</span><span class="st">`</span><span class="dt">lo(educ)</span><span class="st">`</span><span class="op">$</span>x,
           <span class="dt">y =</span> biden_gam_local_terms<span class="op">$</span><span class="st">`</span><span class="dt">lo(educ)</span><span class="st">`</span><span class="op">$</span>y,
           <span class="dt">se.fit =</span> biden_gam_local_terms<span class="op">$</span><span class="st">`</span><span class="dt">lo(educ)</span><span class="st">`</span><span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Biden feeling thermometer&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Local regression&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Education&quot;</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">2</span>](education)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-gam-local-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## gender
<span class="kw">tibble</span>(<span class="dt">x =</span> biden_gam_local_terms<span class="op">$</span>female<span class="op">$</span>x,
           <span class="dt">y =</span> biden_gam_local_terms<span class="op">$</span>female<span class="op">$</span>y,
           <span class="dt">se.fit =</span> biden_gam_local_terms<span class="op">$</span>female<span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span>unique <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">x =</span> <span class="kw">factor</span>(x, <span class="dt">levels =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y, <span class="dt">ymin =</span> y_low, <span class="dt">ymax =</span> y_high)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Biden feeling thermometer&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">3</span>](gender)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/biden-gam-local-3.png" width="672" /></p>
<p>The results are pretty similar to the cubic splines.</p>
<p>We can also use GAMs for classification problems, like the Titanic example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(titanic)

<span class="co"># estimate model for splines on age and education plus dichotomous female</span>
titanic_gam &lt;-<span class="st"> </span><span class="kw">gam</span>(Survived <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(Age, <span class="dt">df =</span> <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span><span class="kw">bs</span>(Fare, <span class="dt">df =</span> <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span>Sex, <span class="dt">data =</span> titanic_train,
                   <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(titanic_gam)</code></pre></div>
<pre><code>## 
## Call: gam(formula = Survived ~ bs(Age, df = 5) + bs(Fare, df = 5) + 
##     Sex, family = binomial, data = titanic_train)
## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.7426 -0.6055 -0.4912  0.7351  2.2300 
## 
## (Dispersion Parameter for binomial family taken to be 1)
## 
##     Null Deviance: 964.516 on 713 degrees of freedom
## Residual Deviance: 684.4498 on 702 degrees of freedom
## AIC: 708.4498 
## 177 observations deleted due to missingness 
## 
## Number of Local Scoring Iterations: 9 
## 
## Anova for Parametric Effects
##                   Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
## bs(Age, df = 5)    5   8.41   1.682   1.6035    0.1568    
## bs(Fare, df = 5)   5  40.86   8.172   7.7905  3.79e-07 ***
## Sex                1 141.62 141.617 135.0111 &lt; 2.2e-16 ***
## Residuals        702 736.35   1.049                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get graphs of each term</span>
titanic_gam_terms &lt;-<span class="st"> </span><span class="kw">preplot</span>(titanic_gam, <span class="dt">se =</span> <span class="ot">TRUE</span>, <span class="dt">rug =</span> <span class="ot">FALSE</span>)

## age
<span class="kw">tibble</span>(<span class="dt">x =</span> titanic_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(Age, df = 5)</span><span class="st">`</span><span class="op">$</span>x,
           <span class="dt">y =</span> titanic_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(Age, df = 5)</span><span class="st">`</span><span class="op">$</span>y,
           <span class="dt">se.fit =</span> titanic_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(Age, df = 5)</span><span class="st">`</span><span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Titanic survival&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Cubic spline&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">1</span>](age)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/titanic-gam-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## fare
<span class="kw">tibble</span>(<span class="dt">x =</span> titanic_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(Fare, df = 5)</span><span class="st">`</span><span class="op">$</span>x,
           <span class="dt">y =</span> titanic_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(Fare, df = 5)</span><span class="st">`</span><span class="op">$</span>y,
           <span class="dt">se.fit =</span> titanic_gam_terms<span class="op">$</span><span class="st">`</span><span class="dt">bs(Fare, df = 5)</span><span class="st">`</span><span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_low), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> y_high), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Titanic survival&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Cubic spline&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Fare&quot;</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">2</span>](fare)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/titanic-gam-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## gender
<span class="kw">tibble</span>(<span class="dt">x =</span> titanic_gam_terms<span class="op">$</span>Sex<span class="op">$</span>x,
           <span class="dt">y =</span> titanic_gam_terms<span class="op">$</span>Sex<span class="op">$</span>y,
           <span class="dt">se.fit =</span> titanic_gam_terms<span class="op">$</span>Sex<span class="op">$</span>se.y) <span class="op">%&gt;%</span>
<span class="st">  </span>unique <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_low =</span> y <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">y_high =</span> y <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>se.fit,
         <span class="dt">x =</span> <span class="kw">factor</span>(x, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;male&quot;</span>, <span class="st">&quot;female&quot;</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y, <span class="dt">ymin =</span> y_low, <span class="dt">ymax =</span> y_high)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;GAM of Titanic survival&quot;</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">y =</span> <span class="kw">expression</span>(f[<span class="dv">3</span>](gender)))</code></pre></div>
<p><img src="/notes/global-methods_files/figure-html/titanic-gam-3.png" width="672" /></p>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">10</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-02-15                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package       * version    date       lib
##  assertthat      0.2.0      2017-04-11 [2]
##  backports       1.1.3      2018-12-14 [2]
##  base64enc       0.1-3      2015-07-28 [2]
##  bayesplot       1.6.0      2018-08-02 [2]
##  bindr           0.1.1      2018-03-13 [2]
##  bindrcpp        0.2.2      2018-03-29 [1]
##  blogdown        0.10       2019-01-09 [1]
##  bookdown        0.9        2018-12-21 [1]
##  broom         * 0.5.1      2018-12-05 [2]
##  callr           3.1.1      2018-12-21 [2]
##  cellranger      1.1.0      2016-07-27 [2]
##  class           7.3-15     2019-01-01 [2]
##  cli             1.0.1      2018-09-25 [1]
##  codetools       0.2-16     2018-12-24 [2]
##  colorspace      1.4-0      2019-01-13 [2]
##  colourpicker    1.0        2017-09-27 [2]
##  crayon          1.3.4      2017-09-16 [2]
##  crosstalk       1.0.0      2016-12-21 [2]
##  data.table      1.12.0     2019-01-13 [2]
##  desc            1.2.0      2018-05-01 [2]
##  devtools        2.0.1      2018-10-26 [1]
##  dials         * 0.0.2      2018-12-09 [1]
##  digest          0.6.18     2018-10-10 [1]
##  dplyr         * 0.7.8      2018-11-10 [1]
##  DT              0.5        2018-11-05 [2]
##  dygraphs        1.1.1.6    2018-07-11 [2]
##  evaluate        0.12       2018-10-09 [2]
##  forcats       * 0.3.0      2018-02-19 [2]
##  foreach       * 1.4.4      2017-12-12 [2]
##  fs              1.2.6      2018-08-23 [1]
##  gam           * 1.16       2018-07-20 [2]
##  generics        0.0.2      2018-11-29 [1]
##  ggplot2       * 3.1.0      2018-10-25 [1]
##  ggridges        0.5.1      2018-09-27 [2]
##  glue            1.3.0      2018-07-17 [2]
##  gower           0.1.2      2017-02-23 [2]
##  gridExtra       2.3        2017-09-09 [2]
##  gtable          0.2.0      2016-02-26 [2]
##  gtools          3.8.1      2018-06-26 [2]
##  haven           2.0.0      2018-11-22 [2]
##  here          * 0.1        2017-05-28 [2]
##  hms             0.4.2      2018-03-10 [2]
##  htmltools       0.3.6      2017-04-28 [1]
##  htmlwidgets     1.3        2018-09-30 [2]
##  httpuv          1.4.5.1    2018-12-18 [2]
##  httr            1.4.0      2018-12-11 [2]
##  igraph          1.2.2      2018-07-27 [2]
##  infer         * 0.4.0      2018-11-15 [1]
##  inline          0.3.15     2018-05-18 [2]
##  ipred           0.9-8      2018-11-05 [1]
##  ISLR          * 1.2        2017-10-20 [2]
##  iterators       1.0.10     2018-07-13 [2]
##  janeaustenr     0.1.5      2017-06-10 [2]
##  jsonlite        1.6        2018-12-07 [2]
##  knitr         * 1.21       2018-12-10 [2]
##  later           0.7.5      2018-09-18 [2]
##  lattice       * 0.20-38    2018-11-04 [2]
##  lava            1.6.4      2018-11-25 [2]
##  lazyeval        0.2.1      2017-10-29 [2]
##  lme4            1.1-19     2018-11-10 [2]
##  loo             2.0.0      2018-04-11 [2]
##  lubridate       1.7.4      2018-04-11 [2]
##  magrittr        1.5        2014-11-22 [2]
##  margins       * 0.3.23     2018-05-22 [2]
##  markdown        0.9        2018-12-07 [2]
##  MASS            7.3-51.1   2018-11-01 [2]
##  Matrix          1.2-15     2018-11-01 [2]
##  matrixStats     0.54.0     2018-07-23 [2]
##  memoise         1.1.0      2017-04-21 [2]
##  mime            0.6        2018-10-05 [1]
##  miniUI          0.1.1.1    2018-05-18 [2]
##  minqa           1.2.4      2014-10-09 [2]
##  modelr          0.1.2      2018-05-11 [2]
##  munsell         0.5.0      2018-06-12 [2]
##  nlme            3.1-137    2018-04-07 [2]
##  nloptr          1.2.1      2018-10-03 [2]
##  nnet            7.3-12     2016-02-02 [2]
##  parsnip       * 0.0.1      2018-11-12 [1]
##  patchwork     * 0.0.1      2018-09-06 [1]
##  pillar          1.3.1      2018-12-15 [2]
##  pkgbuild        1.0.2      2018-10-16 [1]
##  pkgconfig       2.0.2      2018-08-16 [2]
##  pkgload         1.0.2      2018-10-29 [1]
##  plyr            1.8.4      2016-06-08 [2]
##  prediction      0.3.6.1    2018-12-04 [2]
##  prettyunits     1.0.2      2015-07-13 [2]
##  pROC            1.13.0     2018-09-24 [1]
##  processx        3.2.1      2018-12-05 [2]
##  prodlim         2018.04.18 2018-04-18 [2]
##  promises        1.0.1      2018-04-13 [2]
##  ps              1.3.0      2018-12-21 [2]
##  purrr         * 0.3.0      2019-01-27 [2]
##  R6              2.3.0      2018-10-04 [1]
##  rcfss         * 0.1.5      2019-01-24 [1]
##  Rcpp            1.0.0      2018-11-07 [1]
##  readr         * 1.3.1      2018-12-21 [2]
##  readxl          1.2.0      2018-12-19 [2]
##  recipes       * 0.1.4      2018-11-19 [1]
##  remotes         2.0.2      2018-10-30 [1]
##  reshape2        1.4.3      2017-12-11 [2]
##  rlang           0.3.1      2019-01-08 [1]
##  rmarkdown       1.11       2018-12-08 [2]
##  rpart           4.1-13     2018-02-23 [1]
##  rprojroot       1.3-2      2018-01-03 [2]
##  rsample       * 0.0.4      2019-01-07 [1]
##  rsconnect       0.8.13     2019-01-10 [2]
##  rstan           2.18.2     2018-11-07 [2]
##  rstanarm        2.18.2     2018-11-10 [2]
##  rstantools      1.5.1      2018-08-22 [2]
##  rstudioapi      0.9.0      2019-01-09 [1]
##  rvest           0.3.2      2016-06-17 [2]
##  scales        * 1.0.0      2018-08-09 [1]
##  sessioninfo     1.1.1      2018-11-05 [1]
##  shiny           1.2.0      2018-11-02 [2]
##  shinyjs         1.0        2018-01-08 [2]
##  shinystan       2.5.0      2018-05-01 [2]
##  shinythemes     1.1.2      2018-11-06 [2]
##  SnowballC       0.6.0      2019-01-15 [2]
##  StanHeaders     2.18.0-1   2018-12-13 [2]
##  stringi         1.2.4      2018-07-20 [2]
##  stringr       * 1.3.1      2018-05-10 [2]
##  survival        2.43-3     2018-11-26 [2]
##  testthat        2.0.1      2018-10-13 [2]
##  threejs         0.3.1      2017-08-13 [2]
##  tibble        * 2.0.1      2019-01-12 [2]
##  tidymodels    * 0.0.2      2018-11-27 [1]
##  tidyposterior   0.0.2      2018-11-15 [1]
##  tidypredict     0.3.0      2019-01-10 [1]
##  tidyr         * 0.8.2.9000 2019-02-11 [1]
##  tidyselect      0.2.5      2018-10-11 [1]
##  tidytext        0.2.0      2018-10-17 [1]
##  tidyverse     * 1.2.1      2017-11-14 [2]
##  timeDate        3043.102   2018-02-21 [2]
##  titanic       * 0.1.0      2015-08-31 [2]
##  tokenizers      0.2.1      2018-03-29 [2]
##  usethis         1.4.0      2018-08-14 [1]
##  withr           2.1.2      2018-03-15 [2]
##  xfun            0.4        2018-10-23 [1]
##  xml2            1.2.0      2018-01-24 [2]
##  xtable          1.8-3      2018-08-29 [2]
##  xts             0.11-2     2018-11-05 [2]
##  yaml            2.2.0      2018-07-25 [2]
##  yardstick     * 0.0.2      2018-11-05 [1]
##  zoo             1.8-4      2018-09-19 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  local                               
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (tidyverse/tidyr@0b27690)    
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">11</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That is, each interval contains the same number of observations rather than each interval having an equal width.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
