---
title: Support vector machines
date: 2019-03-04T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Separating hyperplanes
    weight: 1
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#maximal-margin-classifier"><span class="toc-section-number">1</span> Maximal margin classifier</a><ul>
<li><a href="#hyperplanes"><span class="toc-section-number">1.1</span> Hyperplanes</a></li>
<li><a href="#classification-using-a-separating-hyperplane"><span class="toc-section-number">1.2</span> Classification using a separating hyperplane</a></li>
<li><a href="#maximal-margin-classifier-1"><span class="toc-section-number">1.3</span> Maximal margin classifier</a><ul>
<li><a href="#constructing-the-maximal-margin-hyperplane"><span class="toc-section-number">1.3.1</span> Constructing the maximal margin hyperplane</a></li>
<li><a href="#non-separable-cases"><span class="toc-section-number">1.3.2</span> Non-separable cases</a></li>
</ul></li>
</ul></li>
<li><a href="#support-vector-classifier"><span class="toc-section-number">2</span> Support vector classifier</a></li>
<li><a href="#support-vector-machines"><span class="toc-section-number">3</span> Support vector machines</a><ul>
<li><a href="#non-linear-decision-boundaries"><span class="toc-section-number">3.1</span> Non-linear decision boundaries</a></li>
<li><a href="#support-vector-machines-1"><span class="toc-section-number">3.2</span> Support vector machines</a><ul>
<li><a href="#kernels"><span class="toc-section-number">3.2.1</span> Kernels</a></li>
<li><a href="#kernels-for-svms"><span class="toc-section-number">3.2.2</span> Kernels for SVMs</a></li>
</ul></li>
</ul></li>
<li><a href="#applying-and-interpreting-svms-using-voter-turnout"><span class="toc-section-number">4</span> Applying and interpreting SVMs using voter turnout</a><ul>
<li><a href="#linear-kernel"><span class="toc-section-number">4.1</span> Linear kernel</a></li>
<li><a href="#polynomial-kernel"><span class="toc-section-number">4.2</span> Polynomial kernel</a></li>
<li><a href="#radial-kernel"><span class="toc-section-number">4.3</span> Radial kernel</a></li>
<li><a href="#comparison-to-other-models"><span class="toc-section-number">4.4</span> Comparison to other models</a></li>
</ul></li>
<li><a href="#support-vector-regression"><span class="toc-section-number">5</span> Support vector regression</a><ul>
<li><a href="#linear-svr"><span class="toc-section-number">5.1</span> Linear SVR</a></li>
<li><a href="#non-linear-svr"><span class="toc-section-number">5.2</span> Non-linear SVR</a></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">6</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">7</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(iml)
<span class="kw">library</span>(here)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(e1071)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(pROC)
<span class="kw">library</span>(kernlab)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<p><strong>Support vector machines</strong> (SVMs) are a popular statistical learning method for classification tasks.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> SVMs build on several important concepts, that while related are distinct from one another. We will first discuss the logic of these individual components, then demonstrate how to estimate and interpret SVMs, and compare model results using this method to other statistical learning procedures we have discussed so far.</p>
<div id="maximal-margin-classifier" class="section level1">
<h1><span class="header-section-number">1</span> Maximal margin classifier</h1>
<div id="hyperplanes" class="section level2">
<h2><span class="header-section-number">1.1</span> Hyperplanes</h2>
<p>In <span class="math inline">\(p\)</span>-dimensional space, a <strong>hyperplane</strong> is a flat subspace of <span class="math inline">\(p - 1\)</span> dimensions that is <em>affine</em> (does not need to pass through the origin). In two dimensions, a hyperplane is a flat one-dimensional subspace (also known as a <strong>line</strong>). In three dimensions, a hyper plane is a flat two-dimensional subspace (also known as a <strong>plane</strong>). In higher dimensions it gets harder to visualize this concept, but the definition still holds true.</p>
<p>In two dimensions, the mathematical equation for a hyperplane is:</p>
<p><span class="math display">\[\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0\]</span></p>
<p>Any <span class="math inline">\(X = (X_1, X_2)^T\)</span> for which this equation holds is a point on the hyperplane (line). This functional form generalizes to <span class="math inline">\(p\)</span> dimensions quite easily:</p>
<p><span class="math display">\[\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0\]</span></p>
<p>Again, for any point <span class="math inline">\(X = (X_1, X_2, \dots, X_p)^T\)</span> in <span class="math inline">\(p\)</span>-dimensional space (i.e. a vector of length <span class="math inline">\(p\)</span>) that equals 0, then <span class="math inline">\(X\)</span> lies on the hyperplane.</p>
<p>For <span class="math inline">\(X\)</span> that does not meet this condition, then the data point lies on either side of the hyperplane:</p>
<p><span class="math display">\[\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p &gt; 0\]</span></p>
<p><span class="math display">\[\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p &lt; 0\]</span></p>
<p>The hyperplane therefore divides the <span class="math inline">\(p\)</span>-dimensional space into two halves. To determine on which side of the hyperplane an observation lies, we simply calculate the sign of the corresponding hyperplane equation.</p>
<p><img src="/notes/support-vector-machines_files/figure-html/hyperplane-1.png" width="672" /></p>
</div>
<div id="classification-using-a-separating-hyperplane" class="section level2">
<h2><span class="header-section-number">1.2</span> Classification using a separating hyperplane</h2>
<p>Let’s represent a hypothetical classification problem as the following: suppose we have an <span class="math inline">\(n \times p\)</span> data matrix <span class="math inline">\(\mathbf{X}\)</span> that consists of <span class="math inline">\(n\)</span> training observations with <span class="math inline">\(p\)</span> predictors in <span class="math inline">\(p\)</span>-dimensional space:</p>
<p><span class="math display">\[x_1 = \begin{pmatrix}
  x_{11} \\
  \vdots \\
  x_{1p}
 \end{pmatrix},
 \dots, x_n = \begin{pmatrix}
  x_{n1} \\
  \vdots \\
  x_{np}
 \end{pmatrix}\]</span></p>
<p>These observations fall into one of two classes <span class="math inline">\(y_1, \dots, y_n \in \{-1, 1 \}\)</span> where <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span> represent two separate classes or categories. We also have a test observation <span class="math inline">\(x^*\)</span> which is a <span class="math inline">\(p\)</span>-vector of observed predictors <span class="math inline">\(x^* = (x_1^*, \dots, x_p^*)\)</span>. We want to develop a model that classifies the test observation correctly given our knowledge of the training observations. Previously we have used methods such as logistic regression (where the response variable is coded <span class="math inline">\(\{0, 1 \}\)</span>) and decision trees to perform this task. Now we want to use a hyperplane to <strong>separate</strong> the training observations into the two possible classes.</p>
<p>A <strong>separating hyperplane</strong> perfectly separates training observations into their class labels. Observations in the blue class are coded as <span class="math inline">\(y_i = 1\)</span> those from the red class as <span class="math inline">\(y_i = -1\)</span>. So a separating hyperplane takes on the properties:</p>
<p><span class="math display">\[\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} &gt; 0, \text{if } y_i = 1\]</span> <span class="math display">\[\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} &lt; 0, \text{if } y_i = -1\]</span></p>
<pre><code>## Warning: funs() is soft deprecated as of dplyr 0.8.0
## please use list() instead
## 
## # Before:
## funs(name = f(.)
## 
## # After: 
## list(name = ~f(.))
## This warning is displayed once per session.</code></pre>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-1.png" width="672" /></p>
<p>If a separating hyperplane exists, then we can classify test observations based on their location relative to the hyperplane:</p>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-decision-1.png" width="672" /></p>
<p>Classifications are based off the sign of <span class="math inline">\(f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*\)</span>. If <span class="math inline">\(f(x^*)\)</span> is positive, then we predict the test observation is <span class="math inline">\(1\)</span>. If <span class="math inline">\(f(x^*)\)</span> is negative, then we predict the test observation is <span class="math inline">\(-1\)</span>. We can also consider the <strong>magnitude</strong> of <span class="math inline">\(f(x^*)\)</span>: the farther the magnitude is away from zero, then the farther the test observation falls from the hyperplane. We can be more confident of our predictions for observations far from the hyperplane, and less so for observations near the hyperplane (i.e. <span class="math inline">\(f(x^*)\)</span> close to zero). The classifier resulting from the separating hyperplane <span class="math inline">\(f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*\)</span> is a <strong>linear decision boundary</strong> because the function itself is a linear form.</p>
</div>
<div id="maximal-margin-classifier-1" class="section level2">
<h2><span class="header-section-number">1.3</span> Maximal margin classifier</h2>
<p>As we saw previously, if the data can be perfectly separated by a hyperplane it is likely true that there are <strong>multiple potential separating hyperplanes</strong>. We need a method for identifying the <em>optimal</em> separating hyperplane. This is known as the <strong>maximal margin hyperplane</strong>, which is the separating hyperplane that is farthest from the training observations. The <strong>margin</strong> is the smallest possible (perpendicular) distance between a training observation and the separating hyperplane. This distance is simply <span class="math inline">\(\hat{f}(x_i)\)</span>. The maximal margin hyperplane defines the hyperplane that minimizes the marginal distance across all training observations, and can be used to classify the test observation <span class="math inline">\(x^*\)</span> based on which side of the hyperplane it lies. This is known as the <strong>maximal margin classifier</strong>. The expectation is that a classifier with a large margin for the training observations will also have a large margin for the test observations, leading to accurate classifications. As with the other methods we have discussed so far, this is an assumption and it is still possible to overfit the training data using the maximal margin classifier.</p>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-margin-1.png" width="672" /></p>
<p>Two observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These observations are called the <strong>support vectors</strong>. They are vectors in <span class="math inline">\(p\)</span>-dimensional space and “support” the maximal margin hyperplane because if the observations shifted at all in their predictor values <span class="math inline">\(X\)</span>, then the maximal margin hyperplane would shift as well. In fact, the maximal margin hyperplane is defined entirely by the support vectors; changes to the other observations would not effect the separating hyperplane as long as the changed observations do not cross the boundary set by the margin.</p>
<div id="constructing-the-maximal-margin-hyperplane" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Constructing the maximal margin hyperplane</h3>
<p>Constructing the maximal margin hyperplane is a (relatively) straight forward affair. Consider a set of <span class="math inline">\(n\)</span> training observations with some number of real number predictors <span class="math inline">\(x_1, \dots, x_n \in \mathbb{R}^p\)</span> and associated class labels <span class="math inline">\(y_1, \dots, y_n \in \{-1, 1\}\)</span>. We want to solve the optimization problem:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0, \beta_1, \dots, \beta_p}{\max} &amp; &amp; M \\
&amp; \text{subject to} &amp; &amp;  \sum_{j=1}^p \beta_j^2 = 1, \\
&amp; &amp; &amp; y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n \\
\end{aligned}
\]</span></p>
<p>This is simpler than it looks. <span class="math inline">\(y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n\)</span> requires the maximal margin hyperplane to sort observations on the correct side of the hyperplane with some amount of cushion <span class="math inline">\(M\)</span> (the margin), provided <span class="math inline">\(M\)</span> is positive. The requirement <span class="math inline">\(\sum_{j=1}^p \beta_j^2 = 1\)</span> means that not only are the observations sorted onto the correct sides of the hyperplane, but that the function <span class="math inline">\(y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip})\)</span> defines the <strong>perpendicular distance</strong> between the observation <span class="math inline">\(y_i\)</span> and the hyperplane. Therefore <span class="math inline">\(M\)</span> defines the margin of the hyperplane (i.e. the amount of cushion between the hyperplane and the closest training observations), so we select values for the parameters <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> to maximize <span class="math inline">\(M\)</span>; that is, obtain the largest amount of cushion possible given the training observations.</p>
<p>Another way of expressing this problem is</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\beta, \beta_0}{\min} &amp; &amp; \| \beta \| \\
&amp; \text{subject to} &amp; &amp; y_i(x_i^{T}\beta + \beta_0) \geq 1 \; \forall \; i = 1, \dots, n \\
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[\| \beta \| = \sum_{j=1}^p \beta_j^2\]</span></p>
<p>Note that under this formulation, <span class="math inline">\(M = \frac{1}{\| \beta \|}\)</span>, and now we have a minimization problem.</p>
</div>
<div id="non-separable-cases" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Non-separable cases</h3>
<p>Unfortunately the maximal margin classifier only works if there exists a separating hyperplane for the data. If the cases cannot be perfectly separated by a hyperplane, then we can never satisfy the conditions of the maximal margin classifier.</p>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-nosep-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="support-vector-classifier" class="section level1">
<h1><span class="header-section-number">2</span> Support vector classifier</h1>
<p><strong>Support vector classifiers</strong> relax the requirement of the maximal margin classifier by allowing the separating hyperplane to not <strong>perfectly</strong> separate the observations; instead, it can make some errors. This is reasonable when:</p>
<ol style="list-style-type: decimal">
<li>There exists no perfectly separating hyperplane</li>
<li>A perfectly separating hyperplane is too sensitive to individual training observations, generating potentially very small margins or overfitting the training set.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></li>
</ol>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-sensitive-1.png" width="672" /><img src="/notes/support-vector-machines_files/figure-html/sim-sensitive-2.png" width="672" /></p>
<p>Instead, we want a separating hyperplane that does not perfectly separate the two classes but provides greater robustness to individual observations and better classification of <strong>most</strong> training observations. We are willing to sacrifice accuracy on a few observations if the resulting hyperplane performs better across the remaining observations.</p>
<p>This approach is called the <strong>support vector classifier</strong>. It allows observations to not only exist on the wrong side of the margin (i.e. inside the cushion defined by <span class="math inline">\(M\)</span>), but also exist on the wrong side of the hyperplane.</p>
<p>The approach is the same as the maximal margin classifier but the optimization problem is slightly different:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n}{\max} &amp; &amp; M \\
&amp; \text{subject to} &amp; &amp;  \sum_{j=1}^p \beta_j^2 = 1, \\
&amp; &amp; &amp; y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M(1 - \epsilon_i), \\
&amp; &amp; &amp; \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C \\
\end{aligned}
\]</span></p>
<p>As in the maximal margin classifier, we attempt to optimize <span class="math inline">\(M\)</span> to generate the largest possible margin. However now we allow some error <span class="math inline">\(\epsilon_i\)</span> for each observation so that they can fall on the wrong side of the margin or hyperplane.</p>
<ul>
<li>If <span class="math inline">\(\epsilon_i = 0\)</span>, then the <span class="math inline">\(i\)</span>th observation falls on the correct side of the margin.</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 0\)</span>, then the <span class="math inline">\(i\)</span>th observation falls on the wrong side of the margin.</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 1\)</span>, then the <span class="math inline">\(i\)</span>th observation falls on the wrong side of the hyperplane.</li>
</ul>
<p><span class="math inline">\(C\)</span> defines precisely how much error we are willing to tolerate in the resulting separating hyperplane. The sum of the errors for all training observations cannot exceed <span class="math inline">\(C\)</span>. Larger values of <span class="math inline">\(C\)</span> permit more overall error in the separating hyperplane and lead to larger margins, and smaller values of <span class="math inline">\(C\)</span> tolerate less error and produce smaller margins. If <span class="math inline">\(C = 0\)</span> then we do not tolerate any error in the separating hyperplane, in which case <span class="math inline">\(\epsilon_1, \dots, \epsilon_n = 0\)</span> and we estimate the maximal margin classifier (of course this is only possible if the classes are perfectly separable). Once we solve the optimization problem, we generate predictions the same way as for maximal margin classifiers, based on <span class="math inline">\(f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*\)</span>.</p>
<p>Selecting a value for <span class="math inline">\(C\)</span> is tricky and generally determined through a cross-validation approach to compare support vector classifiers under different values for <span class="math inline">\(C\)</span>. When <span class="math inline">\(C\)</span> is small, we generate a model with low-bias (it fits the data well) but high-variance (small changes in the training observations can generate substantial changes in the support vector classifier). If <span class="math inline">\(C\)</span> is large, we generate a model with more bias but less variance.</p>
<p>The important thing to realize is that the support vector classifier is robust, like the maximal margin classifier, to changes in observations outside of the margin. Observations that lie directly on the margin or inside the margin but on the correct side of the hyperplane are <strong>support vectors</strong>. The support vector classifier will only change if those observations are adjusted. When <span class="math inline">\(C\)</span> is large, the number of observations falling inside the margin increases and therefore the number of support vectors also increases. In fact, observations well inside the class boundary close to the hyperplane actually do not play a big role in shaping the boundary. It is instead defined most strongly by the observations closer to the margin.</p>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-c-1.png" width="672" /></p>
</div>
<div id="support-vector-machines" class="section level1">
<h1><span class="header-section-number">3</span> Support vector machines</h1>
<div id="non-linear-decision-boundaries" class="section level2">
<h2><span class="header-section-number">3.1</span> Non-linear decision boundaries</h2>
<p>So far we have only demonstrated the support vector classifier with a <strong>linear decision boundary</strong>. But as with linear regression, we also know there are <a href="/notes/global-methods/">methods of extending the linear framework to account for non-linear relationships</a>. Consider the following relationship:</p>
<p><img src="/notes/support-vector-machines_files/figure-html/sim-nonlinear-1.png" width="672" /></p>
<p>A support vector classifier with a linear decision boundary would perform very poorly on this data.</p>
<p>We could go the route we discussed before and relax the linearity assumption by adding quadratic or cubic terms to address the non-linearity. For instance, adding a quadratic term would change the optimization problem to using <span class="math inline">\(2p\)</span> features:</p>
<p><span class="math display">\[X_1, X_1^2, X_2, X_2^2, \dots, X_p, X_p^2\]</span></p>
<p>And therefore the optimization problem becomes:</p>
<p><span class="math display">\[\begin{aligned}
&amp; \underset{\beta_0, \beta_{11}, \beta_{12}, \dots, \beta_{p1}, \beta_{p2}, \epsilon_1, \dots, \epsilon_n}{\max} &amp; &amp; M \\
&amp; \text{subject to} &amp; &amp; y_i \left( \beta_0 + \sum_{j = 1}^p \beta_{j1} x_{ij} + \sum_{j = 1}^p \beta_{j2} x_{ij}^2 \right) \geq M(1 - \epsilon_i), \\
&amp; &amp; &amp; \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C, \sum_{j = 1}^p \sum_{k = 1}^2 \beta_{jk}^2 = 1 \\
\end{aligned}\]</span></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/3liCbRZPrZA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>The problem with this approach is that as you add polynomial terms (or interactions or splines) you increase the <strong>feature space</strong> used to generate the decision boundary and the separating hyperplane (i.e. the total number of predictors increases). Maximizing this optimization problem is already computationally intensive: if you continue to increase the number of features, computing the support vector classifier becomes much more difficult and inefficient, and may even become impossible.</p>
</div>
<div id="support-vector-machines-1" class="section level2">
<h2><span class="header-section-number">3.2</span> Support vector machines</h2>
<p>The <strong>support vector machine</strong> is an extension of the support vector classifier that enlarges the feature space by using <strong>kernels</strong>. Kernels are a computationally efficient method for extending the feature space to accomodate a non-linear decision boundary.</p>
<blockquote>
<p>Do not confuse this kernel function with <a href="/notes/local-regression/#kernel-functions">kernel smoothers</a>. Same word, two different definitions.</p>
</blockquote>
<p>Computing the support vector classifier involves the <strong>inner products</strong> of the observations, rather than the observations themselves.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> The inner product of two <span class="math inline">\(r\)</span>-length vectors <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is defined as <span class="math inline">\(\langle a,b \rangle = \sum_{i = 1}^r a_i b_i\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)</code></pre></div>
<pre><code>## [1] 1 2 3 4 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(y &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)</code></pre></div>
<pre><code>## [1] 1 2 3 4 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x <span class="op">%*%</span><span class="st"> </span>y</code></pre></div>
<pre><code>##      [,1]
## [1,]   55</code></pre>
<p>So the inner product of two observations is:</p>
<p><span class="math display">\[\langle x_i, x_{i&#39;} \rangle = \sum_{j = 1}^p x_{ij} x_{i&#39;j}\]</span></p>
<p>The linear support vector can be written as:</p>
<p><span class="math display">\[f(x) = \beta_0 + \sum_{i = 1}^n \alpha_i \langle x, x_i \rangle\]</span></p>
<p>where there are <span class="math inline">\(n\)</span> parameters <span class="math inline">\(\alpha_i, i = 1, \dots, n\)</span>, one per training observation. To estimate the parameters <span class="math inline">\(\alpha_1, \dots, \alpha_n, \beta_0\)</span>, we just need to calculate the inner products between all pairs of training observations. However for observations which are not also support vectors, <span class="math inline">\(\alpha_i\)</span> is actually zero. So in fact, we only need to calculate the inner products for support vectors <span class="math inline">\(\mathbb{S}\)</span> which reduces the complexity of this task:</p>
<p><span class="math display">\[f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i \langle x, x_i \rangle\]</span></p>
<div id="kernels" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Kernels</h3>
<p>Imagine we had to calculate the inner product of two vectors which are the output of a function <span class="math inline">\(f(\mathbf{x})\)</span>:<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display">\[
\begin{align}
\mathbf{x} &amp;= (1, 2, 3) \\
\mathbf{y} &amp;= (4, 5, 6) \\
f(x_1, x_2, x_3) &amp;= x_1x_1 + x_1 x_2 + x_1x_3 + x_2x_1 + \\
&amp;\qquad x_2x_2 + x_2x_3 + x_3x_1 + x_3x_2 + x_3x_3
\end{align}
\]</span></p>
<p>If we have to calculate the inner product by first transforming these three-dimensional vectors into nine dimensions, the calculation is:</p>
<p><span class="math display">\[
\begin{align}
f(\mathbf{x}) &amp;= 1, 2, 3, 2, 4, 6, 3, 6, 9 \\
f(\mathbf{y}) &amp;= 16, 20, 24, 20, 25, 30, 24, 30, 36 \\
\langle f(\mathbf{x}), f(\mathbf{y}) \rangle &amp;= (1, 2, 3, 2, 4, 6, 3, 6, 9) \cdot (16, 20, 24, 20, 25, 30, 24, 30, 36) \\
&amp;= 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 \\
&amp;= 1024
\end{align}
\]</span></p>
<p>That was a lot of algebra because <span class="math inline">\(f\)</span> required the full mapping prior to calculating the inner product.</p>
<p>Consider instead if we used a kernel function</p>
<p><span class="math display">\[K(\mathbf{x}, \mathbf{y}) = \langle \mathbf{x}, \mathbf{y} \rangle^2\]</span></p>
<p>Now our operation is</p>
<p><span class="math display">\[
\begin{align}
K(\mathbf{x}, \mathbf{y}) &amp;= \langle (1, 2, 3, 2, 4, 6, 3, 6, 9) \cdot 16, 20, 24, 20, 25, 30, 24, 30, 36 \rangle ^2 \\
 &amp;= (4 + 10 + 18)^2 \\
&amp;= 32^2 \\
&amp;= 1024
\end{align}
\]</span></p>
<p>Same result, but the calculation is far less difficult. Now consider how this computation scales on a computer.</p>
<p><img src="/notes/support-vector-machines_files/figure-html/inner-prod-compare-1.png" width="672" /></p>
</div>
<div id="kernels-for-svms" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Kernels for SVMs</h3>
<p>Now rather than using the actual inner product,</p>
<p><span class="math display">\[\langle x_i, x_{i&#39;} \rangle = \sum_{j = 1}^p x_{ij} x_{i&#39;j}\]</span></p>
<p>instead we can use a <strong>generalization</strong> of the inner product following some functional form <span class="math inline">\(K\)</span> which we will call a <strong>kernel</strong>:</p>
<p><span class="math display">\[K(x_i, x_{i&#39;})\]</span></p>
<p>In this context, a kernel calculates the similarity of two observations. For example,</p>
<p><span class="math display">\[K(x_i, x_{i&#39;}) = \sum_{j = 1}^p x_{ij} x_{i&#39;j}\]</span></p>
<p>generates the support vector classifier, also known as the <strong>linear kernel</strong>. Alternatively, we could use a different kernel function such as:</p>
<p><span class="math display">\[K(x_i, x_{i&#39;}) = (1 + \sum_{j = 1}^p x_{ij} x_{i&#39;j})^d\]</span></p>
<p>This is called the <strong>polynomial kernel</strong> of degree <span class="math inline">\(d\)</span> where <span class="math inline">\(d\)</span> is some positive integer. This will generate a much more flexible decision boundary, similar to how using a spline in linear regression generates a flexible, non-linear functional form. To use this kernel in a support vector classifier, the functional form becomes:</p>
<p><span class="math display">\[f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i K(x,x_i)\]</span></p>
<p><img src="/notes/support-vector-machines_files/figure-html/svm-poly-1.png" width="672" /><img src="/notes/support-vector-machines_files/figure-html/svm-poly-2.png" width="672" /></p>
<p>Another choice is the <strong>radial kernel</strong>:</p>
<p><span class="math display">\[K(x_i, x_{i&#39;}) = \exp(- \gamma \sum_{j=1}^p (x_{ij} - x_{i&#39;j})^2)\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is some positive constant. Radial kernels work by localizing predictions for test observations based on their Euclidian distance to nearby training observations.</p>
<p><img src="/notes/support-vector-machines_files/figure-html/svm-radial-1.png" width="672" /><img src="/notes/support-vector-machines_files/figure-html/svm-radial-2.png" width="672" /></p>
<p>Kernels are better to use for support vector machines than other non-linear approachs because they do not enlarge the feature space. That is, you need to compute <span class="math inline">\(K(x_i, x_{i&#39;})\)</span> for all <span class="math inline">\(\binom{n}{2}\)</span> distinct pairs <span class="math inline">\(i, i&#39;\)</span>, but <span class="math inline">\(p\)</span> itself remains the same. <strong>You do not need to explicitly enlarge the feature space to accomplish this task</strong>. The total number of features/predictors/independent variables in the model remains the same, so you can more easily compute the SVM.</p>
</div>
</div>
</div>
<div id="applying-and-interpreting-svms-using-voter-turnout" class="section level1">
<h1><span class="header-section-number">4</span> Applying and interpreting SVMs using voter turnout</h1>
<p>SVMs are generally used for <strong>prediction models</strong>. They generate predicted classes for test observations and we can assess confidence in the model and overall model fit using standard metric. However SVMs are not good for conducting inference, since there are no easy methods for interpreting the relative importance and influence of individual predictors on the separating hyperplane. Regression coefficients are generally easy to interpret, and even tree-based methods have visual and statistical interpretations (variable importance plots) of the individual predictors. Generally SVMs are interpreted by assessing overall model fit and error rates, using a combination of cross-validation methods and visuals such as ROC curves.</p>
<p>Let’s test the SVM method on our voter turnout data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(mh &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;mental_health.csv&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">mutate</span>(<span class="dt">vote96 =</span> <span class="kw">factor</span>(vote96, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span>na.omit)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   vote96 = col_double(),
##   mhealth_sum = col_double(),
##   age = col_double(),
##   educ = col_double(),
##   black = col_double(),
##   female = col_double(),
##   married = col_double(),
##   inc10 = col_double()
## )</code></pre>
<pre><code>## # A tibble: 1,165 x 8
##    vote96 mhealth_sum   age  educ black female married inc10
##    &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1 Yes              0    60    12     0      0       0  4.81
##  2 Yes              1    36    12     0      0       1  8.83
##  3 No               7    21    13     0      0       0  1.74
##  4 No               6    29    13     0      0       0 10.7 
##  5 Yes              1    41    15     1      1       1  8.83
##  6 Yes              2    48    20     0      0       1  8.83
##  7 No               9    20    12     0      1       0  7.22
##  8 No              12    27    11     0      1       0  1.20
##  9 Yes              2    28    16     0      0       1  7.22
## 10 Yes              0    72    14     0      0       1  4.01
## # … with 1,155 more rows</code></pre>
<div id="linear-kernel" class="section level2">
<h2><span class="header-section-number">4.1</span> Linear kernel</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                        <span class="dt">number =</span> <span class="dv">10</span>,
                        <span class="dt">savePredictions =</span> <span class="st">&quot;final&quot;</span>,
                        <span class="dt">classProbs =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doParallel)</code></pre></div>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cl &lt;-<span class="st"> </span><span class="kw">makePSOCKcluster</span>(<span class="dv">3</span>)
<span class="kw">registerDoParallel</span>(cl)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
svm_linear &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;svmLinear&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl,
  <span class="dt">tuneLength =</span> <span class="dv">10</span>
)
svm_linear<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Linear (vanilla) kernel function. 
## 
## Number of Support Vectors : 727 
## 
## Objective Function Value : -722.8561 
## Training error : 0.278112 
## Probability model included.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw the ROC curve</span>
svm_linear_roc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="dt">predictor =</span> svm_linear<span class="op">$</span>pred<span class="op">$</span>Yes,
                      <span class="dt">response =</span> svm_linear<span class="op">$</span>pred<span class="op">$</span>obs,
                      <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">levels</span>(mh<span class="op">$</span>vote96)))

<span class="kw">plot</span>(svm_linear_roc)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/vote96-svm-line-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">auc</span>(svm_linear_roc)</code></pre></div>
<pre><code>## Area under the curve: 0.7499</code></pre>
</div>
<div id="polynomial-kernel" class="section level2">
<h2><span class="header-section-number">4.2</span> Polynomial kernel</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
svm_poly &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;svmPoly&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl,
  <span class="dt">tuneLength =</span> <span class="dv">3</span>
)
svm_poly<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 0.25 
## 
## Polynomial kernel function. 
##  Hyperparameters : degree =  2  scale =  0.1  offset =  1 
## 
## Number of Support Vectors : 716 
## 
## Objective Function Value : -172.5873 
## Training error : 0.276395 
## Probability model included.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw the ROC curve</span>
svm_poly_roc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="dt">predictor =</span> svm_poly<span class="op">$</span>pred<span class="op">$</span>Yes,
                    <span class="dt">response =</span> svm_poly<span class="op">$</span>pred<span class="op">$</span>obs,
                    <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">levels</span>(mh<span class="op">$</span>vote96)))

<span class="kw">plot</span>(svm_poly_roc)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/vote96-svm-poly-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">auc</span>(svm_poly_roc)</code></pre></div>
<pre><code>## Area under the curve: 0.7484</code></pre>
</div>
<div id="radial-kernel" class="section level2">
<h2><span class="header-section-number">4.3</span> Radial kernel</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
svm_radial &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl,
  <span class="dt">tuneLength =</span> <span class="dv">3</span>
)
svm_radial<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.126861544467025 
## 
## Number of Support Vectors : 725 
## 
## Objective Function Value : -651.2728 
## Training error : 0.253219 
## Probability model included.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># draw the ROC curve</span>
svm_radial_roc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="dt">predictor =</span> svm_radial<span class="op">$</span>pred<span class="op">$</span>Yes,
                      <span class="dt">response =</span> svm_radial<span class="op">$</span>pred<span class="op">$</span>obs,
                      <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">levels</span>(mh<span class="op">$</span>vote96)))

<span class="kw">plot</span>(svm_radial_roc)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/vote96-svm-radial-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">auc</span>(svm_radial_roc)</code></pre></div>
<pre><code>## Area under the curve: 0.736</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ROC</span>
<span class="kw">bind_rows</span>(
  <span class="dt">Linear =</span> svm_linear<span class="op">$</span>pred,
  <span class="dt">Polynomial =</span> svm_poly<span class="op">$</span>pred,
  <span class="dt">Radial =</span> svm_radial<span class="op">$</span>pred,
  <span class="dt">.id =</span> <span class="st">&quot;kernel&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(kernel) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">roc_curve</span>(<span class="dt">truth =</span> obs, <span class="dt">estimate =</span> Yes) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity, <span class="dt">y =</span> sensitivity, <span class="dt">color =</span> kernel)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_path</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">lty =</span> <span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/mh-roc-compare-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># AUC</span>
<span class="kw">bind_rows</span>(
  <span class="dt">Linear =</span> svm_linear<span class="op">$</span>pred,
  <span class="dt">Polynomial =</span> svm_poly<span class="op">$</span>pred,
  <span class="dt">Radial =</span> svm_radial<span class="op">$</span>pred,
  <span class="dt">.id =</span> <span class="st">&quot;kernel&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(kernel) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">roc_auc</span>(<span class="dt">truth =</span> obs, Yes) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(kernel) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">.estimate =</span> <span class="kw">mean</span>(.estimate)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">fct_reorder</span>(kernel, .estimate), .estimate)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>percent) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparison of area under the curve&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;10-fold CV&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Algorithm&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Area under the curve&quot;</span>)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/mh-roc-compare-2.png" width="672" /></p>
<p>This time the SVM with the highest AUC is the linear model, followed by the radial and then the polynomial SVM. Interestingly, the linear SVM had the highest training error rate (cross-validated), followed by radial, and then polynomial with the lowest error rate. These are cross-validated measures, so it’s not as if they should be heavily biased. However they are all within 1 percentage point of each other, so the differences may not actually be that substantial. Further exploration could be warranted here.</p>
</div>
<div id="comparison-to-other-models" class="section level2">
<h2><span class="header-section-number">4.4</span> Comparison to other models</h2>
<p>We could tinker with the parameters for the polynomial and radial kernel SVMs, adjusting the number of degrees in the polynomial SVM and testing different constants <span class="math inline">\(\gamma\)</span> for the radial SVM, again using 10-fold CV to select the optimal values. Instead though, let’s see how the SVM with the highest AUC (linear) stacks up with some of the other statistical learning methods we could apply.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
vote96_glm &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
vote96_tree &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl,
  <span class="dt">tuneLength =</span> <span class="dv">10</span>
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
vote96_bag &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;treebag&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
vote96_rf &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl,
  <span class="dt">tuneLength =</span> <span class="dv">10</span>
)</code></pre></div>
<pre><code>## note: only 6 unique complexity parameters in default grid. Truncating the grid to 6 .</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit model</span>
vote96_boost &lt;-<span class="st"> </span><span class="kw">train</span>(
  vote96 <span class="op">~</span><span class="st"> </span>., 
  <span class="dt">data =</span> mh, 
  <span class="dt">method =</span> <span class="st">&quot;xgbTree&quot;</span>,
  <span class="dt">trControl =</span> cv_ctrl,
  <span class="dt">tuneLength =</span> <span class="dv">3</span>
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stopCluster</span>(cl)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ROC</span>
<span class="kw">bind_rows</span>(
  <span class="st">`</span><span class="dt">SVM (linear)</span><span class="st">`</span> =<span class="st"> </span>svm_linear<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">SVM (polynomial)</span><span class="st">`</span> =<span class="st"> </span>svm_poly<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">SVM (radial)</span><span class="st">`</span> =<span class="st"> </span>svm_radial<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Logistic regression</span><span class="st">`</span> =<span class="st"> </span>vote96_glm<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Decision tree</span><span class="st">`</span> =<span class="st"> </span>vote96_tree<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Bagging</span><span class="st">`</span> =<span class="st"> </span>vote96_bag<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Random forest</span><span class="st">`</span> =<span class="st"> </span>vote96_rf<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Boosting</span><span class="st">`</span> =<span class="st"> </span>vote96_boost<span class="op">$</span>pred,
  <span class="dt">.id =</span> <span class="st">&quot;kernel&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(kernel) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">roc_curve</span>(<span class="dt">truth =</span> obs, <span class="dt">estimate =</span> Yes) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity, <span class="dt">y =</span> sensitivity, <span class="dt">color =</span> kernel)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_path</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">lty =</span> <span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="ot">NULL</span>)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/vote96-compare-roc-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># AUC</span>
<span class="kw">bind_rows</span>(
  <span class="st">`</span><span class="dt">SVM (linear)</span><span class="st">`</span> =<span class="st"> </span>svm_linear<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">SVM (polynomial)</span><span class="st">`</span> =<span class="st"> </span>svm_poly<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">SVM (radial)</span><span class="st">`</span> =<span class="st"> </span>svm_radial<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Logistic regression</span><span class="st">`</span> =<span class="st"> </span>vote96_glm<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Decision tree</span><span class="st">`</span> =<span class="st"> </span>vote96_tree<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Bagging</span><span class="st">`</span> =<span class="st"> </span>vote96_bag<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Random forest</span><span class="st">`</span> =<span class="st"> </span>vote96_rf<span class="op">$</span>pred,
  <span class="st">`</span><span class="dt">Boosting</span><span class="st">`</span> =<span class="st"> </span>vote96_boost<span class="op">$</span>pred,
  <span class="dt">.id =</span> <span class="st">&quot;kernel&quot;</span>
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(kernel) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">roc_auc</span>(<span class="dt">truth =</span> obs, Yes) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(kernel) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">.estimate =</span> <span class="kw">mean</span>(.estimate)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">fct_reorder</span>(kernel, .estimate), .estimate)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span>percent) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparison of area under the curve&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;10-fold CV&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Algorithm&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Area under the curve&quot;</span>)</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/vote96-compare-roc-2.png" width="672" /></p>
</div>
</div>
<div id="support-vector-regression" class="section level1">
<h1><span class="header-section-number">5</span> Support vector regression</h1>
<p>Support vector machines can also be extended to regression problems with continuous outcomes of interest. The major difference from classification SVM is that the response variable falls on the real number line. The most common form of SVM regression is <strong>epsilon-insensitive SVM</strong> (<span class="math inline">\(\epsilon\)</span>-SVM) regression. The goal is to find a function <span class="math inline">\(f(x)\)</span> that deviates from <span class="math inline">\(y_i\)</span> by a value no greater than <span class="math inline">\(\epsilon\)</span> for each training point <span class="math inline">\(x\)</span>, and at the same time is as flat as possible.</p>
<p>Suppose we have a training set of data where <span class="math inline">\(x_i\)</span> is a multivariate set of <span class="math inline">\(N\)</span> observations with observed response values <span class="math inline">\(y_i\)</span>. The optimization problem is</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n}{\max} &amp; &amp; M \\
&amp; \text{subject to} &amp; &amp;  \sum_{j=1}^p \beta_j^2 = 1, \\
&amp; &amp; &amp;  y_i - (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip})  \geq M(1 - \epsilon_i) + \xi_i, \\
&amp; &amp; &amp;  (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) - y_i  \geq M(1 - \epsilon_i) + \xi_i^*, \\
&amp; &amp; &amp; \epsilon_i \geq 0, \\
&amp; &amp; &amp; \sum_{i = 1}^n \epsilon_i \leq C, \\
&amp; &amp; &amp; \xi_i \geq 0, \\
&amp; &amp; &amp; \xi_i^* \geq 0 \\
\end{aligned}
\]</span></p>
<p>This means to estimate parameter values <span class="math inline">\(B_p\)</span> that ensure the flatest possible hyperplane subject to the residuals having a value less than <span class="math inline">\(\epsilon_i\)</span>. Because, like with SVMs, we will not be able to do this in most situations, we add <strong>slack variables</strong> <span class="math inline">\(\xi_i, \xi_i^*\)</span> to allow regression errors up to the value of <span class="math inline">\(\xi_i\)</span> and <span class="math inline">\(\xi_i^*\)</span>, yet still satisfy the other conditions. This is equivalent to imposing soft margins for SVMs. The major tuning parameters are <span class="math inline">\(C\)</span> (the cost budget) and <span class="math inline">\(\epsilon\)</span>, the maximum allowable error for which the algorithm ignores the observation. That is, rather than establishing support vectors which are closest to the hyperplane, SVM regression establishes support vectors as observations sufficiently far away from the hyperplane. All observations within the range of <span class="math inline">\(\epsilon\)</span> (as measured by <span class="math inline">\(L_1\)</span> distance) are treated as having residuals of 0.</p>
<div id="linear-svr" class="section level2">
<h2><span class="header-section-number">5.1</span> Linear SVR</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi, <span class="dt">length =</span> <span class="dv">500</span>)
y &lt;-<span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">sd =</span> <span class="fl">0.3</span>)
line_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> x,
  <span class="dt">y =</span> y
)

<span class="kw">ggplot</span>(line_data, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x) x) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated linear data&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y))</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/line-data-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to estimate model</span>
sine_svm &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">epsilon =</span> <span class="fl">0.1</span>, <span class="dt">kernel =</span> <span class="st">&#39;rbfdot&#39;</span>, df, <span class="dt">C =</span> <span class="dv">1</span>){
  <span class="kw">ksvm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> df, <span class="dt">type =</span> <span class="st">&quot;eps-svr&quot;</span>, <span class="dt">kernel =</span> kernel, <span class="dt">C =</span> C, <span class="dt">scaled =</span> <span class="kw">c</span>(),
       <span class="dt">epsilon =</span> epsilon)
}

<span class="co"># linear kernel</span>
<span class="kw">tibble</span>(
  <span class="dt">epsilon =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(epsilon, sine_svm, <span class="dt">df =</span> line_data, <span class="dt">kernel =</span> <span class="st">&quot;vanilladot&quot;</span>),
         <span class="dt">pred =</span> <span class="kw">map</span>(model, predict)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(epsilon) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> line_data<span class="op">$</span>x,
         <span class="dt">epsilon_lab =</span> <span class="kw">str_c</span>(<span class="st">&quot;epsilon == &quot;</span>, epsilon)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> line_data, <span class="kw">aes</span>(x, y), <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x) x) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(epsilon))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>, <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>epsilon_lab, <span class="dt">labeller =</span> label_parsed) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Support vector regression&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Linear kernel, C = 1&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y),
       <span class="dt">color =</span> <span class="kw">expression</span>(epsilon))</code></pre></div>
<pre><code>##  Setting default kernel parameters  
##  Setting default kernel parameters  
##  Setting default kernel parameters  
##  Setting default kernel parameters</code></pre>
<p><img src="/notes/support-vector-machines_files/figure-html/line-svm-1.png" width="672" /></p>
</div>
<div id="non-linear-svr" class="section level2">
<h2><span class="header-section-number">5.2</span> Non-linear SVR</h2>
<p>As with SVM, we can use kernel functions to map <span class="math inline">\(x_i\)</span> into a higher-dimensional space in the context of calculating similarity, thus transforming a non-linear problem into a linear problem.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Simulate some sine wave data</span>
<span class="kw">set.seed</span>(<span class="dv">101</span>)
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi, <span class="dt">length =</span> <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">sin</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">sd =</span> <span class="fl">0.3</span>)
sine_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> x,
  <span class="dt">y =</span> y
)

<span class="kw">ggplot</span>(sine_data, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> sin) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated sine wave data&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y))</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/sine-data-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># radial basis kernel</span>
<span class="kw">tibble</span>(
  <span class="dt">epsilon =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(epsilon, sine_svm, <span class="dt">df =</span> sine_data, <span class="dt">C =</span> <span class="dv">1</span>),
         <span class="dt">pred =</span> <span class="kw">map</span>(model, predict)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(epsilon) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> sine_data<span class="op">$</span>x,
         <span class="dt">epsilon_lab =</span> <span class="kw">str_c</span>(<span class="st">&quot;epsilon == &quot;</span>, epsilon)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sine_data, <span class="kw">aes</span>(x, y), <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> sin) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(epsilon))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>, <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>epsilon_lab, <span class="dt">labeller =</span> label_parsed) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Support vector regression&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Radial basis kernel, C = 1&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y),
       <span class="dt">color =</span> <span class="kw">expression</span>(epsilon))</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/svm-sine-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(
  <span class="dt">epsilon =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(epsilon, sine_svm, <span class="dt">df =</span> sine_data, <span class="dt">C =</span> <span class="dv">5</span>),
         <span class="dt">pred =</span> <span class="kw">map</span>(model, predict)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unnest</span>(pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(epsilon) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> sine_data<span class="op">$</span>x,
         <span class="dt">epsilon_lab =</span> <span class="kw">str_c</span>(<span class="st">&quot;epsilon == &quot;</span>, epsilon)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sine_data, <span class="kw">aes</span>(x, y), <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> sin) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">factor</span>(epsilon))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>, <span class="dt">palette =</span> <span class="st">&quot;Dark2&quot;</span>, <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>epsilon_lab, <span class="dt">labeller =</span> label_parsed) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Support vector regression&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Radial basis kernel, C = 5&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y),
       <span class="dt">color =</span> <span class="kw">expression</span>(epsilon))</code></pre></div>
<p><img src="/notes/support-vector-machines_files/figure-html/svm-sine-2.png" width="672" /></p>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.3        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-03-04                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package        * version    date       lib
##  assertthat       0.2.0      2017-04-11 [2]
##  backports        1.1.3      2018-12-14 [2]
##  base64enc        0.1-3      2015-07-28 [2]
##  bayesplot        1.6.0      2018-08-02 [2]
##  blogdown         0.10       2019-01-09 [1]
##  bookdown         0.9        2018-12-21 [1]
##  broom          * 0.5.1      2018-12-05 [2]
##  callr            3.1.1      2018-12-21 [2]
##  caret          * 6.0-81     2018-11-20 [1]
##  cellranger       1.1.0      2016-07-27 [2]
##  checkmate        1.9.1      2019-01-15 [2]
##  class            7.3-15     2019-01-01 [2]
##  cli              1.0.1      2018-09-25 [1]
##  codetools        0.2-16     2018-12-24 [2]
##  colorspace       1.4-0      2019-01-13 [2]
##  colourpicker     1.0        2017-09-27 [2]
##  crayon           1.3.4      2017-09-16 [2]
##  crosstalk        1.0.0      2016-12-21 [2]
##  data.table       1.12.0     2019-01-13 [2]
##  desc             1.2.0      2018-05-01 [2]
##  devtools         2.0.1      2018-10-26 [1]
##  dials          * 0.0.2      2018-12-09 [1]
##  digest           0.6.18     2018-10-10 [1]
##  doParallel     * 1.0.14     2018-09-24 [1]
##  dplyr          * 0.8.0.1    2019-02-15 [1]
##  DT               0.5        2018-11-05 [2]
##  dygraphs         1.1.1.6    2018-07-11 [2]
##  e1071          * 1.7-0.1    2019-01-21 [1]
##  ellipsis         0.1.0      2019-02-19 [2]
##  evaluate         0.13       2019-02-12 [2]
##  forcats        * 0.4.0      2019-02-17 [2]
##  foreach        * 1.4.4      2017-12-12 [2]
##  Formula          1.2-3      2018-05-03 [2]
##  fs               1.2.6      2018-08-23 [1]
##  gbm            * 2.1.5      2019-01-14 [2]
##  generics         0.0.2      2018-11-29 [1]
##  ggplot2        * 3.1.0      2018-10-25 [1]
##  ggridges         0.5.1      2018-09-27 [2]
##  glmnet           2.0-16     2018-04-02 [1]
##  glue             1.3.0      2018-07-17 [2]
##  gower            0.1.2      2017-02-23 [2]
##  gridExtra      * 2.3        2017-09-09 [2]
##  gtable           0.2.0      2016-02-26 [2]
##  gtools           3.8.1      2018-06-26 [2]
##  haven            2.1.0      2019-02-19 [2]
##  here           * 0.1        2017-05-28 [2]
##  hms              0.4.2      2018-03-10 [2]
##  htmltools        0.3.6      2017-04-28 [1]
##  htmlwidgets      1.3        2018-09-30 [2]
##  httpuv           1.4.5.1    2018-12-18 [2]
##  httr             1.4.0      2018-12-11 [2]
##  igraph           1.2.4      2019-02-13 [2]
##  iml            * 0.9.0      2019-02-05 [1]
##  infer          * 0.4.0      2018-11-15 [1]
##  inline           0.3.15     2018-05-18 [2]
##  inum             1.0-0      2017-12-12 [1]
##  ipred            0.9-8      2018-11-05 [1]
##  ISLR           * 1.2        2017-10-20 [2]
##  iterators      * 1.0.10     2018-07-13 [2]
##  janeaustenr      0.1.5      2017-06-10 [2]
##  jsonlite         1.6        2018-12-07 [2]
##  kernlab        * 0.9-27     2018-08-10 [2]
##  knitr            1.21       2018-12-10 [2]
##  labeling         0.3        2014-08-23 [2]
##  later            0.8.0      2019-02-11 [2]
##  lattice        * 0.20-38    2018-11-04 [2]
##  lava             1.6.5      2019-02-12 [2]
##  lazyeval         0.2.1      2017-10-29 [2]
##  libcoin          1.0-3      2019-02-18 [1]
##  lme4             1.1-20     2019-02-04 [2]
##  loo              2.0.0      2018-04-11 [2]
##  lubridate        1.7.4      2018-04-11 [2]
##  magrittr         1.5        2014-11-22 [2]
##  markdown         0.9        2018-12-07 [2]
##  MASS             7.3-51.1   2018-11-01 [2]
##  Matrix           1.2-15     2018-11-01 [2]
##  matrixStats      0.54.0     2018-07-23 [2]
##  memoise          1.1.0      2017-04-21 [2]
##  Metrics          0.1.4      2018-07-09 [1]
##  microbenchmark * 1.4-6      2018-10-18 [2]
##  mime             0.6        2018-10-05 [1]
##  miniUI           0.1.1.1    2018-05-18 [2]
##  minqa            1.2.4      2014-10-09 [2]
##  ModelMetrics     1.2.2      2018-11-03 [2]
##  modelr         * 0.1.4      2019-02-18 [2]
##  munsell          0.5.0      2018-06-12 [2]
##  mvtnorm          1.0-8      2018-05-31 [2]
##  nlme             3.1-137    2018-04-07 [2]
##  nloptr           1.2.1      2018-10-03 [2]
##  nnet             7.3-12     2016-02-02 [2]
##  parsnip        * 0.0.1      2018-11-12 [1]
##  partykit         1.2-3      2019-01-31 [1]
##  patchwork      * 0.0.1      2018-09-06 [1]
##  pillar           1.3.1      2018-12-15 [2]
##  pkgbuild         1.0.2      2018-10-16 [1]
##  pkgconfig        2.0.2      2018-08-16 [2]
##  pkgload          1.0.2      2018-10-29 [1]
##  plyr             1.8.4      2016-06-08 [2]
##  prediction       0.3.6.2    2019-01-31 [2]
##  prettyunits      1.0.2      2015-07-13 [2]
##  pROC           * 1.13.0     2018-09-24 [1]
##  processx         3.2.1      2018-12-05 [2]
##  prodlim          2018.04.18 2018-04-18 [2]
##  promises         1.0.1      2018-04-13 [2]
##  ps               1.3.0      2018-12-21 [2]
##  purrr          * 0.3.0      2019-01-27 [2]
##  R6               2.4.0      2019-02-14 [1]
##  randomForest   * 4.6-14     2018-03-25 [2]
##  rcfss          * 0.1.5      2019-01-24 [1]
##  RColorBrewer     1.1-2      2014-12-07 [2]
##  Rcpp             1.0.0      2018-11-07 [1]
##  readr          * 1.3.1      2018-12-21 [2]
##  readxl           1.3.0      2019-02-15 [2]
##  recipes        * 0.1.4      2018-11-19 [1]
##  remotes          2.0.2      2018-10-30 [1]
##  reshape2         1.4.3      2017-12-11 [2]
##  rlang            0.3.1      2019-01-08 [1]
##  rmarkdown        1.11       2018-12-08 [2]
##  rpart            4.1-13     2018-02-23 [1]
##  rprojroot        1.3-2      2018-01-03 [2]
##  rsample        * 0.0.4      2019-01-07 [1]
##  rsconnect        0.8.13     2019-01-10 [2]
##  rstan            2.18.2     2018-11-07 [2]
##  rstanarm         2.18.2     2018-11-10 [2]
##  rstantools       1.5.1      2018-08-22 [2]
##  rstudioapi       0.9.0      2019-01-09 [1]
##  rvest            0.3.2      2016-06-17 [2]
##  scales         * 1.0.0      2018-08-09 [1]
##  sessioninfo      1.1.1      2018-11-05 [1]
##  shiny            1.2.0      2018-11-02 [2]
##  shinyjs          1.0        2018-01-08 [2]
##  shinystan        2.5.0      2018-05-01 [2]
##  shinythemes      1.1.2      2018-11-06 [2]
##  SnowballC        0.6.0      2019-01-15 [2]
##  StanHeaders      2.18.1     2019-01-28 [2]
##  stringi          1.3.1      2019-02-13 [1]
##  stringr        * 1.4.0      2019-02-10 [1]
##  survival         2.43-3     2018-11-26 [2]
##  testthat         2.0.1      2018-10-13 [2]
##  threejs          0.3.1      2017-08-13 [2]
##  tibble         * 2.0.1      2019-01-12 [2]
##  tidymodels     * 0.0.2      2018-11-27 [1]
##  tidyposterior    0.0.2      2018-11-15 [1]
##  tidypredict      0.3.0      2019-01-10 [1]
##  tidyr          * 0.8.2.9000 2019-02-11 [1]
##  tidyselect       0.2.5      2018-10-11 [1]
##  tidytext         0.2.0      2018-10-17 [1]
##  tidyverse      * 1.2.1      2017-11-14 [2]
##  timeDate         3043.102   2018-02-21 [2]
##  titanic        * 0.1.0      2015-08-31 [2]
##  tokenizers       0.2.1      2018-03-29 [2]
##  tree           * 1.0-39     2018-03-17 [2]
##  usethis          1.4.0      2018-08-14 [1]
##  withr            2.1.2      2018-03-15 [2]
##  xfun             0.5        2019-02-20 [1]
##  xgboost          0.81.0.1   2019-01-31 [1]
##  xml2             1.2.0      2018-01-24 [2]
##  xtable           1.8-3      2018-08-29 [2]
##  xts              0.11-2     2018-11-05 [2]
##  yaml             2.2.0      2018-07-25 [2]
##  yardstick      * 0.0.2      2018-11-05 [1]
##  zoo              1.8-4      2018-09-19 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.2)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  local                               
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (tidyverse/tidyr@0b27690)    
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">7</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Though they can also be applied to regression on continuous response variables.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Remember that we can use the perpendicular distance from the hyperplane as a measure of confidence in our predictions, so the new training observation diminishes our confidence for quite a few of the red training observations.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Like how boosting uses the residuals of the response variable <span class="math inline">\(Y\)</span>, rather than <span class="math inline">\(Y\)</span> itself.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Example drawn from <a href="https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them/answer/Lili-Jiang?srid=oOgT">What are kernels in machine learning and SVM and why do we need them?</a><a href="#fnref4">↩</a></p></li>
</ol>
</div>
