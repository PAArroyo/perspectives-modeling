---
title: Clustering
date: 2019-03-06T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Clustering
    weight: 1
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#k-means-clustering"><span class="toc-section-number">1</span> <span class="math inline">\(K\)</span>-means clustering</a></li>
<li><a href="#hierarchical-clustering"><span class="toc-section-number">2</span> Hierarchical clustering</a></li>
<li><a href="#interpreting-dendrograms"><span class="toc-section-number">3</span> Interpreting dendrograms</a></li>
<li><a href="#estimating-hierarchical-clusters"><span class="toc-section-number">4</span> Estimating hierarchical clusters</a></li>
<li><a href="#session-info"><span class="toc-section-number">5</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">6</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(here)
<span class="kw">library</span>(tictoc)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(ggdendro)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<p><strong>Clustering</strong> refers to a set of techniques for finding subgroups within a dataset, called <strong>clusters</strong>. The goal is to partition the dataset into similar and distinct groups so that observations in each group are similar to one another, while each group is distinctive and dissimilar to the other groups.</p>
<div id="k-means-clustering" class="section level1">
<h1><span class="header-section-number">1</span> <span class="math inline">\(K\)</span>-means clustering</h1>
<p><span class="math inline">\(K\)</span>-means clustering is one approach to identifying distinct clusters within data. First we specify the number of <span class="math inline">\(K\)</span> clusters we want to estimate in the data, then assign each observation to precisely one of those <span class="math inline">\(K\)</span> clusters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate data</span>
x &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x1 =</span> <span class="kw">rnorm</span>(<span class="dv">150</span>) <span class="op">+</span><span class="st"> </span><span class="dv">3</span>,
                <span class="dt">x2 =</span> <span class="kw">rnorm</span>(<span class="dv">150</span>) <span class="op">-</span><span class="st"> </span><span class="dv">4</span>)</code></pre></div>
<pre><code>## Warning: `data_frame()` is deprecated, use `tibble()`.
## This warning is displayed once per session.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate k clusters</span>
x.out &lt;-<span class="st"> </span>x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">k2 =</span> <span class="kw">kmeans</span>(x, <span class="dv">2</span>, <span class="dt">nstart =</span> <span class="dv">20</span>)<span class="op">$</span>cluster,
         <span class="dt">k3 =</span> <span class="kw">kmeans</span>(x, <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">20</span>)<span class="op">$</span>cluster,
         <span class="dt">k4 =</span> <span class="kw">kmeans</span>(x, <span class="dv">4</span>, <span class="dt">nstart =</span> <span class="dv">20</span>)<span class="op">$</span>cluster)

<span class="co"># plot clusters</span>
x.out <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(K, pred, k2<span class="op">:</span>k4) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">K =</span> <span class="kw">parse_number</span>(K),
         <span class="dt">pred =</span> <span class="kw">factor</span>(pred)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(. <span class="op">~</span><span class="st"> </span>K, <span class="dt">labeller =</span> label_both) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/kmeans-1.png" width="672" /></p>
<p>Let <span class="math inline">\(C_1, C_2, \dots, C_K\)</span> denote sets containing the indicies of the observations in each cluster. <span class="math inline">\(K\)</span>-means clustering defines a good cluster as one for which within-cluster variation is as small as possible. So we want to minimize the within-cluster variation defined by some function <span class="math inline">\(W(C_K)\)</span> that identifies variation:</p>
<p><span class="math display">\[\min_{C_1, C_2, \dots, C_K} \left\{ \sum_{k = 1}^K W(C_k) \right\}\]</span></p>
<p>so that the overall amount of within-cluster variation across all the clusters is as small as possible. We can define within-cluster variation in several different ways, but a standard approach uses <strong>squared Euclidean distance</strong>:</p>
<p><span class="math display">\[W(C_k) = \frac{1}{|C_k|} \sum_{i,i&#39; \in C_k} \sum_{j = 1}^p (x_{ij} - x_{i&#39;j})^2\]</span></p>
<p>where the within-cluster variation is the sum of all of the pairwise squared Euclidean distances between the observations in the <span class="math inline">\(k\)</span>th cluster, divided by the number of observations in the <span class="math inline">\(k\)</span>th cluster. Unfortunately we cannot evaluate every possible cluster combination because there are almost <span class="math inline">\(K^n\)</span> ways to partition <span class="math inline">\(n\)</span> observations into <span class="math inline">\(K\)</span> clusters. Instead, we will settle for a <strong>good enough</strong> approach; that is, rather than finding the global optimum for the optimization problem we will instead estimate the local optimum.</p>
<p>To do this we employ an iterative process. First we randomly assign each observation to one of the <span class="math inline">\(K\)</span> clusters. This will be the initial cluster assignment for each observation. Then we iterate over the cluster assignments:</p>
<ol style="list-style-type: decimal">
<li>For each of the <span class="math inline">\(K\)</span> clusters, compute the cluster <strong>centroid</strong>, or the vector of <span class="math inline">\(p\)</span> feature means for the observations in the <span class="math inline">\(k\)</span>th cluster.</li>
<li>Assign each observation to the cluster whose centroid is closest as defined by Euclidean distance.</li>
</ol>
<p>Each time we do this observations will move around and join different clusters because the initial assignments were made entirely at random. As we iterate over this process, the cluster assignments will become more stable and eventually stop entirely. This is when we reach the local optimum. Since the local optimum is based on the initial (random) assignments, we run this algorithm multiple times from different random starting configurations and select the best solution (the one with the lowest total within-cluster variation).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">kmean.out &lt;-<span class="st"> </span><span class="kw">rerun</span>(<span class="dv">6</span>, <span class="kw">kmeans</span>(x, <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">1</span>))

kmean.out <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">map_df</span>(<span class="op">~</span><span class="st"> </span><span class="kw">as_tibble</span>(.<span class="op">$</span>cluster), <span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">bind_rows</span>(x,x,x,x,x,x)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">withinss =</span> <span class="kw">rep</span>(<span class="kw">map_chr</span>(kmean.out, <span class="op">~</span><span class="st"> </span>.<span class="op">$</span>tot.withinss), <span class="dt">each =</span> <span class="kw">nrow</span>(x)),
         <span class="dt">value =</span> <span class="kw">factor</span>(value)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> value)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>id <span class="op">+</span><span class="st"> </span>withinss, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">labeller =</span> <span class="kw">label_wrap_gen</span>(<span class="dt">multi_line =</span> <span class="ot">FALSE</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<pre><code>## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead.
## This warning is displayed once per session.</code></pre>
<p><img src="/notes/clustering_files/figure-html/kmeans-sim-start-1.png" width="672" /></p>
<p>This is basically like starting the algorithm with a different random seed each time. In the above example I ran <span class="math inline">\(K\)</span>-mean clustering with <span class="math inline">\(K=4\)</span> six times with different starting seed values. In four of the iterations, the algorithm converged on the same local optimum solution, while the other two times the algorithm converged on a local optimum with a larger sum of within-cluster variation.</p>
</div>
<div id="hierarchical-clustering" class="section level1">
<h1><span class="header-section-number">2</span> Hierarchical clustering</h1>
<p>A drawback to <span class="math inline">\(K\)</span>-means clustering is that it requires you to specify in advance the number of clusters in the data. Since this is unsupervised learning, you don’t really know the actual number of clusters. Depending on the major features of the data, different values of <span class="math inline">\(K\)</span> could produce equally meaningful results. Imagine if your data contains observations on individuals, split between males and females as well as split between Americans, Canadians, and South Africans. <span class="math inline">\(K=2\)</span> would potentially cluster the observations based on gender, whereas <span class="math inline">\(K=3\)</span> could cluster based on nationality. Which is “right”? Well, both of them. It depends on the features of the data in which you are most interested.</p>
<p><strong>Hierarchical clustering</strong> is an alternative approach that does not require us to fix the number of clusters <em>a priori</em>. It also produces a visual interpretation of the clusters using tree-based representations called <strong>dendrograms</strong>. Here let’s review how to interpret dendrograms generated from <strong>bottom-up</strong> clustering.</p>
</div>
<div id="interpreting-dendrograms" class="section level1">
<h1><span class="header-section-number">3</span> Interpreting dendrograms</h1>
<p>Here we plot a dendrogram using simulated data, consisting of 150 observations in two-dimensional space. We simulate three natural classes in the data, but in the real-world you would not know that.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate data</span>
x &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x1 =</span> <span class="kw">rnorm</span>(<span class="dv">50</span>) <span class="op">+</span><span class="st"> </span><span class="dv">3</span>,
                <span class="dt">x2 =</span> <span class="kw">rnorm</span>(<span class="dv">50</span>) <span class="op">-</span><span class="st"> </span><span class="dv">4</span>,
                <span class="dt">y =</span> <span class="kw">ifelse</span>(x1 <span class="op">&lt;</span><span class="st"> </span><span class="dv">3</span>, <span class="st">&quot;1&quot;</span>,
                           <span class="kw">ifelse</span>(x2 <span class="op">&gt;</span><span class="st"> </span><span class="op">-</span><span class="dv">4</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;3&quot;</span>)))

<span class="kw">ggplot</span>(x, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated data&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X[<span class="dv">1</span>]),
       <span class="dt">y =</span> <span class="kw">expression</span>(X[<span class="dv">2</span>])) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-sim-1.png" width="672" /></p>
<p>Suppose that we observe the data without class labels and want to perform hierarchical clustering on the data. The result is plotted below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate hierarchical cluster</span>
hc.complete &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>)

<span class="co"># plot</span>
<span class="kw">ggdendrogram</span>(hc.complete)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-cluster-1.png" width="672" /></p>
<p>Like with decision trees, we have <strong>leafs</strong> and <strong>branches</strong>. Each leaf is labeled with the observation id number. Rather than reading the dendrogram from the top-down, we read it from the bottom-up. Each observation is represented by a leaf. As we move up the tree, leafs <strong>fuse</strong> into branches. These are observations that are similar to one another, similarity generally being defined by Euclidean distance. Observations that fuse together near the bottom of the tree are generally similar to one another, whereas observations that fuse near the top of the tree are dissimilar. The height on the graph where the fusion occurs defines how similar or dissimilar any two observations are. The larger the value, the more dissimilar they are. Rather than paying attention to the proximity of observations along the horizontal axis, we should instead focus on the location of observations relative to the vertical axis.</p>
<p>From this dendrogram we can assign observations to clusters. To generate clusters, we make a horizontal cut somewhere on the dendrogram, severing the tree into multiple subtrees. The height of the cut will dictate how many clusters are formed. For instance, cutting the tree at a height of 4 splits the dendrogram into two subtrees, and therefore two clusters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="dv">4</span>
<span class="co"># extract dendro data</span>
hcdata &lt;-<span class="st"> </span><span class="kw">dendro_data</span>(hc.complete)
hclabs &lt;-<span class="st"> </span><span class="kw">label</span>(hcdata) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(<span class="kw">data_frame</span>(<span class="dt">label =</span> <span class="kw">as.factor</span>(<span class="kw">seq.int</span>(<span class="kw">nrow</span>(x))),
                       <span class="dt">cl =</span> <span class="kw">as.factor</span>(<span class="kw">cutree</span>(hc.complete, <span class="dt">h =</span> h))))</code></pre></div>
<pre><code>## Joining, by = &quot;label&quot;</code></pre>
<pre><code>## Warning: Column `label` joining factors with different levels, coercing to
## character vector</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot dendrogram</span>
<span class="kw">ggdendrogram</span>(hc.complete, <span class="dt">labels =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">data =</span> hclabs,
            <span class="kw">aes</span>(<span class="dt">label =</span> label, <span class="dt">x =</span> x, <span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">color =</span> cl),
            <span class="dt">vjust =</span> .<span class="dv">5</span>, <span class="dt">angle =</span> <span class="dv">90</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> h, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_blank</span>(),
        <span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-cut-4-1.png" width="672" /></p>
<p>Alternatively we could split it lower, for instance at 3:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="dv">3</span>
<span class="co"># extract dendro data</span>
hcdata &lt;-<span class="st"> </span><span class="kw">dendro_data</span>(hc.complete)
hclabs &lt;-<span class="st"> </span><span class="kw">label</span>(hcdata) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(<span class="kw">data_frame</span>(<span class="dt">label =</span> <span class="kw">as.factor</span>(<span class="kw">seq.int</span>(<span class="kw">nrow</span>(x))),
                       <span class="dt">cl =</span> <span class="kw">as.factor</span>(<span class="kw">cutree</span>(hc.complete, <span class="dt">h =</span> h))))</code></pre></div>
<pre><code>## Joining, by = &quot;label&quot;</code></pre>
<pre><code>## Warning: Column `label` joining factors with different levels, coercing to
## character vector</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot dendrogram</span>
<span class="kw">ggdendrogram</span>(hc.complete, <span class="dt">labels =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_text</span>(<span class="dt">data =</span> hclabs,
            <span class="kw">aes</span>(<span class="dt">label =</span> label, <span class="dt">x =</span> x, <span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">color =</span> cl),
            <span class="dt">vjust =</span> .<span class="dv">5</span>, <span class="dt">angle =</span> <span class="dv">90</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> h, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_blank</span>(),
        <span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-cut-3-1.png" width="672" /></p>
<p>Generating a larger number of clusters. Determining the optimal number of clusters is generally left to the discretion of the researcher based on the height of the fusions and desired number of clusters. Again, this is unsupervised learning <strong>so there is no single correct number of clusters</strong>.</p>
</div>
<div id="estimating-hierarchical-clusters" class="section level1">
<h1><span class="header-section-number">4</span> Estimating hierarchical clusters</h1>
<p>The general procedure for estimating hierarchical clusters is relatively straightforward:</p>
<ol style="list-style-type: decimal">
<li>Assume each <span class="math inline">\(n\)</span> observation is its own cluster. Calculate the <span class="math inline">\(\binom{n}{2} = \frac{n(n-1)}{2}\)</span> pairwise dissimilarities between each observation.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>For <span class="math inline">\(i=n, n-1, \dots, 2\)</span>:
<ol style="list-style-type: decimal">
<li>Compare all pairwise inter-cluster dissimilarities among the <span class="math inline">\(i\)</span> clusters and identify the pair of clusters that are least dissimilar (i.e. most dissimilar). Fuse these two clusters. The dissimilarity between these two clusters determines the height in the dendrogram where the fusion should be placed.</li>
<li>Compute the new pairwise inter-cluster dissimilarities among the <span class="math inline">\(i-1\)</span> clusters</li>
</ol></li>
</ol>
<p>This process is continued until there is only a single cluster remaining. The only complication is how to measure dissimilarities between clusters once they contain more than one observation. Previously we used pairwise dissimilarities of the observations, but how do we proceed with multiple observations? There are four major approaches to defining dissimilarity between clusters, also called <strong>linkage</strong>:</p>
<ol style="list-style-type: decimal">
<li><strong>Complete</strong> - compute all pairwise dissimilarities between observations in cluster A and cluster B and record the largest of these dissimilarities.</li>
<li><strong>Single</strong> - compute all pairwise dissimilarities between observations in cluster A and cluster B and record the smallest of these dissimilarities.</li>
<li><strong>Average</strong> - compute all pairwise dissimilarities between observations in cluster A and cluster B and record the average of these dissimilarities.</li>
<li><strong>Centroid</strong> - compute the dissimilarity between the centroid (a mean vector of length <span class="math inline">\(p\)</span>) for cluster A and cluster B.</li>
</ol>
<p>Each linkage approach leads to different hierarchical clusters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hc.complete &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>)
hc.single &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;single&quot;</span>)
hc.average &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(x), <span class="dt">method =</span> <span class="st">&quot;average&quot;</span>)

<span class="co"># plot</span>
<span class="kw">ggdendrogram</span>(hc.complete) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Complete linkage&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-compare-linkage-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggdendrogram</span>(hc.single) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Single linkage&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-compare-linkage-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggdendrogram</span>(hc.average) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Average linkage&quot;</span>)</code></pre></div>
<p><img src="/notes/clustering_files/figure-html/dendro-compare-linkage-3.png" width="672" /></p>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">5</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.3        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-03-12                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package       * version    date       lib
##  assertthat      0.2.0      2017-04-11 [2]
##  backports       1.1.3      2018-12-14 [2]
##  base64enc       0.1-3      2015-07-28 [2]
##  bayesplot       1.6.0      2018-08-02 [2]
##  blogdown        0.10       2019-01-09 [1]
##  bookdown        0.9        2018-12-21 [1]
##  broom         * 0.5.1      2018-12-05 [2]
##  callr           3.1.1      2018-12-21 [2]
##  cellranger      1.1.0      2016-07-27 [2]
##  class           7.3-15     2019-01-01 [2]
##  cli             1.0.1      2018-09-25 [1]
##  codetools       0.2-16     2018-12-24 [2]
##  colorspace      1.4-0      2019-01-13 [2]
##  colourpicker    1.0        2017-09-27 [2]
##  crayon          1.3.4      2017-09-16 [2]
##  crosstalk       1.0.0      2016-12-21 [2]
##  desc            1.2.0      2018-05-01 [2]
##  devtools        2.0.1      2018-10-26 [1]
##  dials         * 0.0.2      2018-12-09 [1]
##  digest          0.6.18     2018-10-10 [1]
##  dplyr         * 0.8.0.1    2019-02-15 [1]
##  DT              0.5        2018-11-05 [2]
##  dygraphs        1.1.1.6    2018-07-11 [2]
##  evaluate        0.13       2019-02-12 [2]
##  forcats       * 0.4.0      2019-02-17 [2]
##  fs              1.2.6      2018-08-23 [1]
##  generics        0.0.2      2018-11-29 [1]
##  ggdendro      * 0.1-20     2019-02-21 [1]
##  ggplot2       * 3.1.0      2018-10-25 [1]
##  ggridges        0.5.1      2018-09-27 [2]
##  glue            1.3.0      2018-07-17 [2]
##  gower           0.1.2      2017-02-23 [2]
##  gridExtra       2.3        2017-09-09 [2]
##  gtable          0.2.0      2016-02-26 [2]
##  gtools          3.8.1      2018-06-26 [2]
##  haven           2.1.0      2019-02-19 [2]
##  here          * 0.1        2017-05-28 [2]
##  hms             0.4.2      2018-03-10 [2]
##  htmltools       0.3.6      2017-04-28 [1]
##  htmlwidgets     1.3        2018-09-30 [2]
##  httpuv          1.4.5.1    2018-12-18 [2]
##  httr            1.4.0      2018-12-11 [2]
##  igraph          1.2.4      2019-02-13 [2]
##  infer         * 0.4.0      2018-11-15 [1]
##  inline          0.3.15     2018-05-18 [2]
##  ipred           0.9-8      2018-11-05 [1]
##  janeaustenr     0.1.5      2017-06-10 [2]
##  jsonlite        1.6        2018-12-07 [2]
##  knitr           1.21       2018-12-10 [2]
##  labeling        0.3        2014-08-23 [2]
##  later           0.8.0      2019-02-11 [2]
##  lattice         0.20-38    2018-11-04 [2]
##  lava            1.6.5      2019-02-12 [2]
##  lazyeval        0.2.1      2017-10-29 [2]
##  lme4            1.1-20     2019-02-04 [2]
##  loo             2.0.0      2018-04-11 [2]
##  lubridate       1.7.4      2018-04-11 [2]
##  magrittr        1.5        2014-11-22 [2]
##  markdown        0.9        2018-12-07 [2]
##  MASS            7.3-51.1   2018-11-01 [2]
##  Matrix          1.2-15     2018-11-01 [2]
##  matrixStats     0.54.0     2018-07-23 [2]
##  memoise         1.1.0      2017-04-21 [2]
##  mime            0.6        2018-10-05 [1]
##  miniUI          0.1.1.1    2018-05-18 [2]
##  minqa           1.2.4      2014-10-09 [2]
##  modelr        * 0.1.4      2019-02-18 [2]
##  munsell         0.5.0      2018-06-12 [2]
##  nlme            3.1-137    2018-04-07 [2]
##  nloptr          1.2.1      2018-10-03 [2]
##  nnet            7.3-12     2016-02-02 [2]
##  parsnip       * 0.0.1      2018-11-12 [1]
##  patchwork     * 0.0.1      2018-09-06 [1]
##  pillar          1.3.1      2018-12-15 [2]
##  pkgbuild        1.0.2      2018-10-16 [1]
##  pkgconfig       2.0.2      2018-08-16 [2]
##  pkgload         1.0.2      2018-10-29 [1]
##  plyr            1.8.4      2016-06-08 [2]
##  prettyunits     1.0.2      2015-07-13 [2]
##  pROC            1.13.0     2018-09-24 [1]
##  processx        3.2.1      2018-12-05 [2]
##  prodlim         2018.04.18 2018-04-18 [2]
##  promises        1.0.1      2018-04-13 [2]
##  ps              1.3.0      2018-12-21 [2]
##  purrr         * 0.3.0      2019-01-27 [2]
##  R6              2.4.0      2019-02-14 [1]
##  Rcpp            1.0.0      2018-11-07 [1]
##  readr         * 1.3.1      2018-12-21 [2]
##  readxl          1.3.0      2019-02-15 [2]
##  recipes       * 0.1.4      2018-11-19 [1]
##  remotes         2.0.2      2018-10-30 [1]
##  reshape2        1.4.3      2017-12-11 [2]
##  rlang           0.3.1      2019-01-08 [1]
##  rmarkdown       1.11       2018-12-08 [2]
##  rpart           4.1-13     2018-02-23 [1]
##  rprojroot       1.3-2      2018-01-03 [2]
##  rsample       * 0.0.4      2019-01-07 [1]
##  rsconnect       0.8.13     2019-01-10 [2]
##  rstan           2.18.2     2018-11-07 [2]
##  rstanarm        2.18.2     2018-11-10 [2]
##  rstantools      1.5.1      2018-08-22 [2]
##  rstudioapi      0.9.0      2019-01-09 [1]
##  rvest           0.3.2      2016-06-17 [2]
##  scales        * 1.0.0      2018-08-09 [1]
##  sessioninfo     1.1.1      2018-11-05 [1]
##  shiny           1.2.0      2018-11-02 [2]
##  shinyjs         1.0        2018-01-08 [2]
##  shinystan       2.5.0      2018-05-01 [2]
##  shinythemes     1.1.2      2018-11-06 [2]
##  SnowballC       0.6.0      2019-01-15 [2]
##  StanHeaders     2.18.1     2019-01-28 [2]
##  stringi         1.3.1      2019-02-13 [1]
##  stringr       * 1.4.0      2019-02-10 [1]
##  survival        2.43-3     2018-11-26 [2]
##  testthat        2.0.1      2018-10-13 [2]
##  threejs         0.3.1      2017-08-13 [2]
##  tibble        * 2.0.1      2019-01-12 [2]
##  tictoc        * 1.0        2014-06-17 [1]
##  tidymodels    * 0.0.2      2018-11-27 [1]
##  tidyposterior   0.0.2      2018-11-15 [1]
##  tidypredict     0.3.0      2019-01-10 [1]
##  tidyr         * 0.8.2.9000 2019-02-11 [1]
##  tidyselect      0.2.5      2018-10-11 [1]
##  tidytext        0.2.0      2018-10-17 [1]
##  tidyverse     * 1.2.1      2017-11-14 [2]
##  timeDate        3043.102   2018-02-21 [2]
##  tokenizers      0.2.1      2018-03-29 [2]
##  usethis         1.4.0      2018-08-14 [1]
##  withr           2.1.2      2018-03-15 [2]
##  xfun            0.5        2019-02-20 [1]
##  xml2            1.2.0      2018-01-24 [2]
##  xtable          1.8-3      2018-08-29 [2]
##  xts             0.11-2     2018-11-05 [2]
##  yaml            2.2.0      2018-07-25 [2]
##  yardstick     * 0.0.2      2018-11-05 [1]
##  zoo             1.8-4      2018-09-19 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  Github (bensoltoff/ggdendro@9c9c6e7)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.1)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  Github (tidyverse/tidyr@0b27690)    
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Again, using Euclidean distance.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
