---
title: Support vector machines
date: 2019-03-04T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Separating hyperplanes
    weight: 1
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(tidymodels)
library(patchwork)
library(iml)
library(here)
library(rcfss)
library(e1071)
library(caret)
library(pROC)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}

**Support vector machines** (SVMs) are a popular statistical learning method for classification tasks.^[Though they can also be applied to regression on continuous response variables.] SVMs build on several important concepts, that while related are distinct from one another. We will first discuss the logic of these individual components, then demonstrate how to estimate and interpret SVMs, and compare model results using this method to other statistical learning procedures we have discussed so far.

# Maximal margin classifier

## Hyperplanes

In $p$-dimensional space, a **hyperplane** is a flat subspace of $p - 1$ dimensions that is *affine* (does not need to pass through the origin). In two dimensions, a hyperplane is a flat one-dimensional subspace (also known as a **line**). In three dimensions, a hyper plane is a flat two-dimensional subspace (also known as a **plane**). In higher dimensions it gets harder to visualize this concept, but the definition still holds true.

In two dimensions, the mathematical equation for a hyperplane is:

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$$

Any $X = (X_1, X_2)^T$ for which this equation holds is a point on the hyperplane (line). This functional form generalizes to $p$ dimensions quite easily:

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0$$

Again, for any point $X = (X_1, X_2, \dots, X_p)^T$ in $p$-dimensional space (i.e. a vector of length $p$) that equals 0, then $X$ lies on the hyperplane.

For $X$ that does not meet this condition, then the data point lies on either side of the hyperplane:

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p > 0$$

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p < 0$$

The hyperplane therefore divides the $p$-dimensional space into two halves. To determine on which side of the hyperplane an observation lies, we simply calculate the sign of the corresponding hyperplane equation.

```{r hyperplane, echo = FALSE}
sim_hyper <- tibble(
  x1 = seq(-1.5, 1.5, length.out = 20),
  x2 = seq(-1.5, 1.5, length.out = 20)
) %>%
  expand(x1, x2) %>%
  mutate(y = 1 + 2 * x1 + 3 * x2,
         group = ifelse(y < 0, -1,
                        ifelse(y > 0, 1, 0)),
         group = factor(group))

sim_hyper_line <- tibble(x1 = seq(-1.5, 1.5, length.out = 20),
                         x2 = (-1 - 2 * x1) / 3)

ggplot(sim_hyper, aes(x1, x2, color = group)) +
  geom_point() +
  geom_line(data = sim_hyper_line, aes(color = NULL)) +
  scale_color_brewer(type = "qual") +
  labs(title = "Hyperplane in two dimensions") +
  theme(legend.position = "none")
```

## Classification using a separating hyperplane

Let's represent a hypothetical classification problem as the following: suppose we have an $n \times p$ data matrix $\mathbf{X}$ that consists of $n$ training observations with $p$ predictors in $p$-dimensional space:

$$x_1 = \begin{pmatrix}
  x_{11} \\
  \vdots \\
  x_{1p}
 \end{pmatrix},
 \dots, x_n = \begin{pmatrix}
  x_{n1} \\
  \vdots \\
  x_{np}
 \end{pmatrix}$$

These observations fall into one of two classes $y_1, \dots, y_n \in \{-1, 1 \}$ where $-1$ and $1$ represent two separate classes or categories. We also have a test observation $x^*$ which is a $p$-vector of observed predictors $x^* = (x_1^*, \dots, x_p^*)$. We want to develop a model that classifies the test observation correctly given our knowledge of the training observations. Previously we have used methods such as logistic regression (where the response variable is coded $\{0, 1 \}$) and decision trees to perform this task. Now we want to use a hyperplane to **separate** the training observations into the two possible classes.

A **separating hyperplane** perfectly separates training observations into their class labels. Observations in the blue class are coded as $y_i = 1$ those from the red class as $y_i = -1$. So a separating hyperplane takes on the properties:

$$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} > 0, \text{if } y_i = 1$$
$$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} < 0, \text{if } y_i = -1$$

```{r sim, echo = FALSE}
set.seed(123)

sim <- tibble(
  x1 = runif(20, -2, 2),
  x2 = runif(20, -2, 2),
  y = ifelse(1 + 2 * x1 + 3 * x2 < 0, -1, 1)
) %>%
  mutate_each(funs(ifelse(y == 1, . + 1.5, .)), x2) %>%
  mutate(y = factor(y, levels = c(-1, 1))) %>%
  mutate(line1 = (-1 - 2 * x1) / 3,
         line2 = .5 + (-1 - 1.5 * x1) / 3,
         line3 = .25 - .05 * x1)

ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(aes(y = line1, color = NULL)) +
  geom_line(aes(y = line2, color = NULL)) +
  geom_line(aes(y = line3, color = NULL)) +
  scale_color_brewer(type = "qual") +
  labs(title = "Examples of separating hyperplanes") +
  theme(legend.position = "none")
```

If a separating hyperplane exists, then we can classify test observations based on their location relative to the hyperplane:

```{r sim-decision, dependson = "sim", echo = FALSE}
sim_mod <- svm(y ~ x1 + x2, data = sim, kernel = "linear", cost = 1e05,
               scale = FALSE)
sim_coef <- c(sim_mod$rho, t(sim_mod$coefs) %*% sim_mod$SV)

sim_grid <- tibble(
  x1 = seq(-2, 2, length.out = 100),
  x2 = seq(-2, 3.5, length.out = 100)
) %>%
  expand(x1, x2) %>%
  mutate(y = ifelse(-sim_coef[[1]] + sim_coef[[2]] * x1 + sim_coef[[3]] * x2 > 0, -1, 1),
         y = factor(y, levels = c(-1, 1)))

sim_plane <- tibble(
  x1 = seq(-2, 2, length.out = 100),
  x2 = (sim_coef[[1]] - sim_coef[[2]] * x1) / sim_coef[[3]]
)

ggplot(sim, aes(x1)) +
  geom_point(data = sim_grid, aes(x1, x2, color = y), alpha = .25, size = .25) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sim_plane, aes(x1, x2)) +
  scale_color_brewer(type = "qual") +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

Classifications are based off the sign of $f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*$. If $f(x^*)$ is positive, then we predict the test observation is $1$. If $f(x^*)$ is negative, then we predict the test observation is $-1$. We can also consider the **magnitude** of $f(x^*)$: the farther the magnitude is away from zero, then the farther the test observation falls from the hyperplane. We can be more confident of our predictions for observations far from the hyperplane, and less so for observations near the hyperplane (i.e. $f(x^*)$ close to zero). The classifier resulting from the separating hyperplane $f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*$ is a **linear decision boundary** because the function itself is a linear form.

## Maximal margin classifier

As we saw previously, if the data can be perfectly separated by a hyperplane it is likely true that there are **multiple potential separating hyperplanes**. We need a method for identifying the *optimal* separating hyperplane. This is known as the **maximal margin hyperplane**, which is the separating hyperplane that is farthest from the training observations. The **margin** is the smallest possible (perpendicular) distance between a training observation and the separating hyperplane. This distance is simply $\hat{f}(x_i)$. The maximal margin hyperplane defines the hyperplane that minimizes the marginal distance across all training observations, and can be used to classify the test observation $x^*$ based on which side of the hyperplane it lies. This is known as the **maximal margin classifier**. The expectation is that a classifier with a large margin for the training observations will also have a large margin for the test observations, leading to accurate classifications. As with the other methods we have discussed so far, this is an assumption and it is still possible to overfit the training data using the maximal margin classifier.

```{r sim-margin, dependson = c("sim", "sim-decision"), echo = FALSE}
sim_pred <- predict(sim_mod, sim, decision.values = TRUE)
sim_dist <- attr(sim_pred, "decision.values")

ggplot(sim, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_point(data = sim_grid, aes(x1, x2, color = y), alpha = .1, size = .25) +
  geom_line(data = sim_plane, aes(x1, x2)) +
  geom_line(data = mutate(sim_plane, x2 = x2 - min(abs(sim_dist))),
            aes(x1, x2), linetype = 2) +
  geom_line(data = mutate(sim_plane, x2 = x2 + min(abs(sim_dist))),
            aes(x1, x2), linetype = 2) +
  scale_color_brewer(type = "qual") +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

Two observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These observations are called the **support vectors**. They are vectors in $p$-dimensional space and "support" the maximal margin hyperplane because if the observations shifted at all in their predictor values $X$, then the maximal margin hyperplane would shift as well. In fact, the maximal margin hyperplane is defined entirely by the support vectors; changes to the other observations would not effect the separating hyperplane as long as the changed observations do not cross the boundary set by the margin.

### Constructing the maximal margin hyperplane

Constructing the maximal margin hyperplane is a (relatively) straight forward affair. Consider a set of $n$ training observations with some number of real number predictors $x_1, \dots, x_n \in \mathbb{R}^p$ and associated class labels $y_1, \dots, y_n \in \{-1, 1\}$. We want to solve the optimization problem:

$$\begin{aligned}
& \underset{\beta_0, \beta_1, \dots, \beta_p}{\text{maximize}} & & M \\
& \text{s.t.} & &  \sum_{j=1}^p \beta_j^2 = 1, \\
& & & y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n \\
\end{aligned}$$

This is simpler than it looks. $y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M \; \forall \; i = 1, \dots, n$ requires the maximal margin hyperplane to sort observations on the correct side of the hyperplane with some amount of cushion, provided $M$ is positive. The requirement $\sum_{j=1}^p \beta_j^2 = 1$ means that not only are the observations sorted onto the correct sides of the hyperplane, but that the function $y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip})$ defines the **perpendicular distance** between the observation $y_i$ and the hyperplane. Therefore $M$ defines the margin of the hyperplane (i.e. the amount of cushion between the hyperplane and the closest training observations), so we select values for the parameters $\beta_0, \beta_1, \dots, \beta_p$ to maximize $M$; that is, obtain the largest amount of cushion possible given the training observations.

### Non-separable cases

Unfortunately the maximal margin classifier only works if there exists a separating hyperplane for the data. If the cases cannot be perfectly separated by a hyperplane, then we can never satisfy the conditions of the maximal margin classifier.

```{r sim-nosep, echo = FALSE}
tibble(
  x1 = runif(20, -2, 2),
  x2 = runif(20, -2, 2),
  y = c(rep(-1, 10), rep(1, 10))
) %>%
  mutate(y = factor(y, levels = c(-1, 1))) %>%
  ggplot(aes(x1, x2, color = y)) +
  geom_point() +
  scale_color_brewer(type = "qual") +
  labs(title = "Non-separable data") +
  theme(legend.position = "none")
```

# Support vector classifier

**Support vector classifiers** relax the requirement of the maximal margin classifier by allowing the separating hyperplane to not **perfectly** separate the observations; instead, it can make some errors. This is reasonable when:

1. There exists no perfectly separating hyperplane
1. A perfectly separating hyperplane is too sensitive to individual training observations, generating potentially very small margins or overfitting the training set.^[Remember that we can use the perpendicular distance from the hyperplane as a measure of confidence in our predictions, so the new training observation diminishes our confidence for quite a few of the red training observations.]

```{r sim-sensitive, echo = FALSE}
# original model
set.seed(123)
sensitive <- tibble(
  x1 = runif(20, -2, 2),
  x2 = runif(20, -2, 2),
  y = ifelse(1 + 2 * x1 + 3 * x2 < 0, -1, 1)
) %>%
  mutate_each(funs(ifelse(y == 1, . + .5, .)), x2) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

sens_mod <- svm(y ~ x1 + x2, data = sensitive, kernel = "linear",
                cost = 1e05, scale = FALSE)
sens_coef <- c(sens_mod$rho, t(sens_mod$coefs) %*% sens_mod$SV)
sens_plane <- tibble(x1 = seq(-2, 2, length.out = 100),
                     x2 = (sens_coef[[1]] - sens_coef[[2]] * x1) / sens_coef[[3]])

ggplot(sensitive, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sens_plane, aes(x1, x2)) +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")

# slight tweak
sensitive2 <- tibble(x1 = with(sensitive, x1[which(x2 == max(x2[y == -1]))]),
                     x2 = with(sensitive, max(x2[y == -1])) + .1,
                     y = factor(1, levels = c(-1, 1))) %>%
  bind_rows(sensitive)

sens2_mod <- svm(y ~ x1 + x2, data = sensitive2, kernel = "linear",
                 cost = 1e05, scale = FALSE)
sens2_coef <- c(sens2_mod$rho, t(sens2_mod$coefs) %*% sens2_mod$SV)
sens2_plane <- tibble(x1 = seq(-2, 2, length.out = 100),
                      x2 = (sens2_coef[[1]] - sens2_coef[[2]] * x1) / sens2_coef[[3]])

ggplot(sensitive2, aes(x1)) +
  geom_point(aes(y = x2, color = y)) +
  geom_line(data = sens2_plane, aes(x1, x2)) +
  geom_line(data = sens_plane, aes(x1, x2), linetype = 2) +
  scale_color_brewer(type = "qual") +
  labs(title = "Maximal margin classification") +
  theme(legend.position = "none")
```

Instead, we want a separating hyperplane that does not perfectly separate the two classes but provides greater robustness to individual observations and better classification of **most** training observations. We are willing to sacrifice accuracy on a few observations if the resulting hyperplane performs better across the remaining observations.

This approach is called the **support vector classifier**. It allows observations to not only exist on the wrong side of the margin (i.e. inside the cushion defined by $M$), but also exist on the wrong side of the hyperplane.

The approach is the same as the maximal margin classifier but the optimization problem is slightly different:

$$\begin{aligned}
& \underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n}{\text{maximize}} & & M \\
& \text{s.t.} & &  \sum_{j=1}^p \beta_j^2 = 1, \\
& & & y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) \geq M(1 - \epsilon_i), \\
& & & \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C \\
\end{aligned}$$

As in the maximal margin classifier, we attempt to optimize $M$ to generate the largest possible margin. However now we allow some error $\epsilon_i$ for each observation so that they can fall on the wrong side of the margin or hyperplane.

* If $\epsilon_i = 0$, then the $i$th observation falls on the correct side of the margin.
* If $\epsilon_i > 0$, then the $i$th observation falls on the wrong side of the margin.
* If $\epsilon_i > 1$, then the $i$th observation falls on the wrong side of the hyperplane.

$C$ defines precisely how much error we are willing to tolerate in the resulting separating hyperplane. The sum of the errors for all training observations cannot exceed $C$. Larger values of $C$ permit more overall error in the separating hyperplane and lead to larger margins, and smaller values of $C$ tolerate less error and produce smaller margins. If $C = 0$ then we do not tolerate any error in the separating hyperplane, in which case $\epsilon_1, \dots, \epsilon_n = 0$ and we estimate the maximal margin classifier (of course this is only possible if the classes are perfectly separable). Once we solve the optimization problem, we generate predictions the same way as for maximal margin classifiers, based on $f(x^*) = \beta_0 + \beta_1 x_1^* + \dots + \beta_p x_p^*$.

Selecting a value for $C$ is tricky and generally determined through a cross-validation approach to compare support vector classifiers under different values for $C$. When $C$ is small, we generate a model with low-bias (it fits the data well) but high-variance (small changes in the training observations can generate substantial changes in the support vector classifier). If $C$ is large, we generate a model with more bias but less variance.

The important thing to realize is that the support vector classifier is robust, like the maximal margin classifier, to changes in observations outside of the margin. Observations that lie directly on the margin or inside the margin but on the correct side of the hyperplane are **support vectors**. The support vector classifier will only change if those observations are adjusted. When $C$ is large, the number of observations falling inside the margin increases and therefore the number of support vectors also increases.

```{r sim-c, fig.asp=1, echo = FALSE}
set.seed(123)
sim_c <- tibble(
  x1 = rnorm(20),
  x2 = rnorm(20),
  y = ifelse(2 * x1 + x2 + rnorm(20, 0, .25) < 0, -1, 1)
) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

plot_svm <- function(df, cost = 1){
  # estimate model
  sim_mod <- svm(y ~ x1 + x2, data = df, kernel = "linear",
                 cost = cost,
                 scale = FALSE)
  
  # extract separating hyperplane
  sim_coef <- c(sim_mod$rho, t(sim_mod$coefs) %*% sim_mod$SV)
  sim_plane <- tibble(x1 = seq(min(df$x1), max(df$x1), length.out = 100),
                      x2 = (-sim_coef[[1]] - sim_coef[[2]] * x1) / sim_coef[[3]])
  
  # extract properties to draw margins
  sim_pred <- predict(sim_mod, df, decision.values = TRUE)
  sim_dist <- attr(sim_pred, "decision.values")
  
  ggplot(df, aes(x1)) +
    geom_point(aes(y = x2, color = y)) +
    geom_line(data = sim_plane, aes(x1, x2)) +
    geom_line(data = mutate(sim_plane, x2 = x2 - min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    geom_line(data = mutate(sim_plane, x2 = x2 + min(abs(sim_dist))),
              aes(x1, x2), linetype = 2) +
    scale_color_brewer(type = "qual") +
    labs(subtitle = str_c("Cost = ", cost)) +
    coord_equal(xlim = range(df$x1),
                ylim = range(df$x2)) +
    theme(legend.position = "none")
}

plot_svm(sim_c, cost = 1) +
  plot_svm(sim_c, cost = 10) +
  plot_svm(sim_c, cost = 100) +
  plot_svm(sim_c, cost = 200) +
  plot_layout(ncol = 2)
```

# Support vector machines

## Non-linear decision boundaries

So far we have only demonstrated the support vector classifier with a **linear decision boundary**. But as with linear regression, we also know there are [methods of extending the linear framework to account for non-linear relationships](/notes/global-methods/). Consider the following relationship:

```{r sim-nonlinear, echo = FALSE}
set.seed(123)

x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150), rep(2, 50))

sim_nonlm <- tibble(
  x1 = x[, 1],
  x2 = x[, 2],
  y = as.factor(y)
)

radial_p <- ggplot(sim_nonlm, aes(x1, x2, color = y)) +
  geom_point() +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "none")
radial_p
```

A support vector classifier with a linear decision boundary would perform very poorly on this data.

We could go the route we discussed before and relax the linearity assumption by adding quadratic or cubic terms to address the non-linearity. For instance, adding a quadratic term would change the optimization problem to using $2p$ features:

$$X_1, X_1^2, X_2, X_2^2, \dots, X_p, X_p^2$$

And therefore the optimization problem becomes:

$$\begin{aligned}
& \underset{\beta_0, \beta_{11}, \beta_{12}, \dots, \beta_{p1}, \beta_{p2}, \epsilon_1, \dots, \epsilon_n}{\text{maximize}} & & M \\
& \text{s.t.} & & y_i \left( \beta_0 + \sum_{j = 1}^p \beta_{j1} x_{ij} + \sum_{j = 1}^p \beta_{j2} x_{ij}^2 \right) \geq M(1 - \epsilon_i), \\
& & & \epsilon_i \geq 0, \sum_{i = 1}^n \epsilon_i \leq C, \sum_{j = 1}^p \sum_{k = 1}^2 \beta_{jk}^2 = 1 \\
\end{aligned}$$

<iframe width="560" height="315" src="https://www.youtube.com/embed/3liCbRZPrZA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The problem with this approach is that as you add polynomial terms (or interactions or splines) you increase the **feature space** used to generate the decision boundary and the separating hyperplane (i.e. the total number of predictors increases). Maximizing this optimization problem is already computationally intensive: if you continue to increase the number of features, computing the support vector classifier becomes much more difficult and inefficient, and may even become impossible.

## Support vector machines

The **support vector machine** is an extension of the support vector classifier that enlarges the feature space by using **kernels**. Kernels are a computationally efficient method for extending the feature space to accomodate a non-linear decision boundary.

> Do not confuse this kernel function with [kernel smoothers](/notes/local-regression/#kernel-functions). Same word, two different definitions.

Computing the support vector classifier involves the **inner products** of the observations, rather than the observations themselves.^[Like how boosting uses the residuals of the response variable $Y$, rather than $Y$ itself.] The inner product of two $r$-length vectors $a$ and $b$ is defined as $\langle a,b \rangle = \sum_{i = 1}^r a_i b_i$.

```{r inner-prod}
(x <- 1:5)
(y <- 1:5)

x %*% y
```

So the inner product of two observations is:

$$\langle x_i, x_{i'} \rangle = \sum_{j = 1}^p x_{ij} x_{i'j}$$

The linear support vector can be written as:

$$f(x) = \beta_0 + \sum_{i = 1}^n \alpha_i \langle x, x_i \rangle$$

where there are $n$ parameters $\alpha_i, i = 1, \dots, n$, one per training observation. To estimate the parameters $\alpha_1, \dots, \alpha_n, \beta_0$, we just need to calculate the inner products between all pairs of training observations. However for observations which are not also support vectors, $\alpha_i$ is actually zero. So in fact, we only need to calculate the inner products for support vectors $\mathbb{S}$ which reduces the complexity of this task:

$$f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i \langle x, x_i \rangle$$

### Kernels

Imagine we had to calculate the inner product of two vectors which are the output of a function $f(\mathbf{x})$:^[Example drawn from [What are kernels in machine learning and SVM and why do we need them?](https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them/answer/Lili-Jiang?srid=oOgT)]

```{r inner-prod-func, include = FALSE}
x <- c(1, 2, 3)
y <- c(4, 5, 6)

f_vec <- function(x){
  results <- vector(mode = "list", length = length(x))
  for(i in seq_along(x)){
    results[[i]] <- x[[i]] * x
  }
  unlist(results)
}
```

$$
\begin{align}
\mathbf{x} &= (`r str_c(x, collapse = ", ")`) \\
\mathbf{y} &= (`r str_c(y, collapse = ", ")`) \\
f(x_1, x_2, x_3) &= x_1x_1 + x_1 x_2 + x_1x_3 + x_2x_1 + \\
&\qquad x_2x_2 + x_2x_3 + x_3x_1 + x_3x_2 + x_3x_3
\end{align}
$$

If we have to calculate the inner product by first transforming these three-dimensional vectors into nine dimensions, the calculation is:

$$
\begin{align}
f(\mathbf{x}) &= `r str_c(f_vec(x), collapse = ", ")` \\
f(\mathbf{y}) &= `r str_c(f_vec(y), collapse = ", ")` \\
\langle f(\mathbf{x}), f(\mathbf{y}) \rangle &= (`r str_c(f_vec(x), collapse = ", ")`) \cdot (`r str_c(f_vec(y), collapse = ", ")`) \\
&= `r str_c(f_vec(x) * f_vec(y), collapse = " + ")` \\
&= `r (f_vec(x) %*% f_vec(y))[1,1]`
\end{align}
$$

That was a lot of algebra because $f$ required the full mapping prior to calculating the inner product.

Consider instead if we used a kernel function

$$K(\mathbf{x}, \mathbf{y}) = \langle \mathbf{x}, \mathbf{y} \rangle^2$$

Now our operation is

$$
\begin{align}
K(\mathbf{x}, \mathbf{y}) &= \langle (`r str_c(f_vec(x), collapse = ", ")`) \cdot `r str_c(f_vec(y), collapse = ", ")` \rangle ^2 \\
 &= (`r str_c(x * y, collapse = " + ")`)^2 \\
&= `r sum(x * y)`^2 \\
&= `r sum(x * y)^2`
\end{align}
$$

Same result, but the calculation is far less difficult. Now consider how this computation scales on a computer.

```{r inner-prod-compare, dependson = "inner-prod-func", echo = FALSE, message = FALSE}
library(microbenchmark)

# create practice vectors
big_x <- big_y <- seq(from = 1, to = 100)

big_results <- microbenchmark(
  `Full feature space` = f_vec(big_x) %*% f_vec(big_y),
  `Kernel function` = sum(big_x %*% big_y)^2
)

autoplot(big_results) +
  ggtitle("Inner product of vectors length 100")
```

### Kernels for SVMs

Now rather than using the actual inner product,

$$\langle x_i, x_{i'} \rangle = \sum_{j = 1}^p x_{ij} x_{i'j}$$

instead we can use a **generalization** of the inner product following some functional form $K$ which we will call a **kernel**:

$$K(x_i, x_{i'})$$

In this context, a kernel calculates the similarity of two observations. For example,

$$K(x_i, x_{i'}) = \sum_{j = 1}^p x_{ij} x_{i'j}$$

generates the support vector classifier, also known as the **linear kernel**. Alternatively, we could use a different kernel function such as:

$$K(x_i, x_{i'}) = (1 + \sum_{j = 1}^p x_{ij} x_{i'j})^d$$

This is called the **polynomial kernel** of degree $d$ where $d$ is some positive integer. This will generate a much more flexible decision boundary, similar to how using a spline in linear regression generates a flexible, non-linear functional form. To use this kernel in a support vector classifier, the functional form becomes:

$$f(x) = \beta_0 + \sum_{i \in \mathbb{S}} \alpha_i K(x,x_i)$$

```{r svm-poly, echo = FALSE}
set.seed(123)

sim_nonlm <- tibble(
  x1 = runif(100, -2, 2),
  x2 = runif(100, -2, 2),
  y = ifelse(x1 + x1^2 + x1^3 - x2 < 0 +
               rnorm(100, 0, 1), -1, 1)
) %>%
  mutate(y = factor(y, levels = c(-1, 1)))

ggplot(sim_nonlm, aes(x1, x2, color = y)) +
  geom_point() +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "none")

svm(y ~ x1 + x2,
    data = sim_nonlm,
    kernel = "polynomial",
    scale = FALSE,
    cost = 1) %>%
  plot(sim_nonlm, x2 ~ x1)
```

Another choice is the **radial kernel**:

$$K(x_i, x_{i'}) = \exp(- \gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2)$$

where $\gamma$ is some positive constant. Radial kernels work by localizing predictions for test observations based on their Euclidian distance to nearby training observations.

```{r svm-radial, dependson = "svm-poly", echo = FALSE}
sim_rad_mod <- svm(y ~ x1 + x2,
                   data = sim_nonlm,
                   kernel = "radial",
                   cost = 5,
                   scale = FALSE)

radial_p
plot(sim_rad_mod, sim_nonlm, x2 ~ x1)
```

Kernels are better to use for support vector machines than other non-linear approachs because they do not enlarge the feature space. That is, you need to compute $K(x_i, x_{i'})$ for all $\binom{n}{2}$ distinct pairs $i, i'$, but $p$ itself remains the same. **You do not need to explicitly enlarge the feature space to accomplish this task**. The total number of features/predictors/independent variables in the model remains the same, so you can more easily compute the SVM.

# Applying and interpreting SVMs using voter turnout

SVMs are generally used for **prediction models**. They generate predicted classes for test observations and we can assess confidence in the model and overall model fit using standard metric. However SVMs are not good for conducting inference, since there are no easy methods for interpreting the relative importance and influence of individual predictors on the separating hyperplane. Regression coefficients are generally easy to interpret, and even tree-based methods have visual and statistical interpretations (variable importance plots) of the individual predictors. Generally SVMs are interpreted by assessing overall model fit and error rates, using a combination of cross-validation methods and visuals such as ROC curves.

Let's test the SVM method on our voter turnout data.

```{r vote96}
(mh <- read_csv(here("static", "data", "mental_health.csv")) %>%
   mutate(vote96 = factor(vote96, levels = c(0, 1), labels = c("No", "Yes"))) %>%
  na.omit)
```

### SVM

Next let's compare a few different SVM models. Again we'll use 10-fold CV on the training set to determine the optimal cost parameter.

#### Linear kernel

```{r caret-trainControl}
cv_ctrl <- trainControl(method = "cv",
                        number = 10,
                        savePredictions = "final",
                        classProbs = TRUE)
```

```{r vote96-svm-line, dependson="vote96"}
# fit model
svm_linear <- train(
  vote96 ~ ., 
  data = mh, 
  method = "svmLinear",
  trControl = cv_ctrl,
  tuneLength = 10
)
svm_linear$finalModel


# draw the ROC curve
svm_linear_roc <- roc(predictor = svm_linear$pred$Yes,
                      response = svm_linear$pred$obs,
                      levels = rev(levels(mh$vote96)))

plot(svm_linear_roc)
auc(svm_linear_roc)
```

### Polynomial kernel

```{r vote96-svm-poly, dependson="vote96"}
# fit model
svm_poly <- train(
  vote96 ~ ., 
  data = mh, 
  method = "svmPoly",
  trControl = cv_ctrl,
  tuneLength = 3
)
svm_poly$finalModel


# draw the ROC curve
svm_poly_roc <- roc(predictor = svm_poly$pred$Yes,
                      response = svm_poly$pred$obs,
                      levels = rev(levels(mh$vote96)))

plot(svm_poly_roc)
auc(svm_poly_roc)
```

### Radial kernel

```{r vote96-svm-radial, dependson="vote96"}
# fit model
svm_radial <- train(
  vote96 ~ ., 
  data = mh, 
  method = "svmRadial",
  trControl = cv_ctrl,
  tuneLength = 3
)
svm_radial$finalModel


# draw the ROC curve
svm_radial_roc <- roc(predictor = svm_radial$pred$Yes,
                      response = svm_radial$pred$obs,
                      levels = rev(levels(mh$vote96)))

plot(svm_radial_roc)
auc(svm_radial_roc)
```

```{r mh-roc-compare, dependson=c("vote96-svm-line","vote96-svm-poly","vote96-svm-radial")}
# ROC
bind_rows(
  Linear = svm_linear$pred,
  Polynomial = svm_poly$pred,
  Radial = svm_radial$pred,
  .id = "kernel"
) %>%
  group_by(kernel) %>%
  roc_curve(truth = obs, estimate = Yes) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = kernel)) +
  geom_path() +
  geom_abline(lty = 3) +
  scale_color_brewer(type = "qual") +
  coord_equal() +
  labs(color = NULL) +
  theme(legend.position = "bottom")

# AUC
bind_rows(
  Linear = svm_linear$pred,
  Polynomial = svm_poly$pred,
  Radial = svm_radial$pred,
  .id = "kernel"
) %>%
  group_by(kernel) %>%
  roc_auc(truth = obs, Yes) %>%
  group_by(kernel) %>%
  summarize(.estimate = mean(.estimate)) %>%
  ggplot(aes(fct_reorder(kernel, .estimate), .estimate)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  coord_flip() +
  labs(title = "Comparison of area under the curve",
       subtitle = "10-fold CV",
       x = "Algorithm",
       y = "Area under the curve")
```

This time the SVM with the highest AUC is the linear model, followed by the radial and then the polynomial SVM. Interestingly, the linear SVM had the highest training error rate (cross-validated), followed by radial, and then polynomial with the lowest error rate. These are cross-validated measures, so it's not as if they should be heavily biased. However they are all within 1 percentage point of each other, so the differences may not actually be that substantial. Further exploration could be warranted here.

We could tinker with the parameters for the polynomial and radial kernel SVMs, adjusting the number of degrees in the polynomial SVM and testing different constants $\gamma$ for the radial SVM, again using 10-fold CV to select the optimal values. Instead though, let's see how the SVM with the highest AUC (linear) stacks up with some of the other statistical learning methods we could apply.

```{r vote96-logit, dependson = "vote96"}
# fit model
vote96_glm <- train(
  vote96 ~ ., 
  data = mh, 
  method = "glm",
  family = "binomial",
  trControl = cv_ctrl
)
```

```{r vote96-tree, dependson = "vote96"}
# fit model
vote96_tree <- train(
  vote96 ~ ., 
  data = mh, 
  method = "rpart",
  trControl = cv_ctrl,
  tuneLength = 10
)
```

```{r parallel-start, cache = FALSE}
library(doParallel)
cl <- makePSOCKcluster(3)
registerDoParallel(cl)
```

```{r vote96-bag, dependson = "vote96"}
# fit model
vote96_bag <- train(
  vote96 ~ ., 
  data = mh, 
  method = "treebag",
  trControl = cv_ctrl
)
```

```{r vote96-rf, dependson = "vote96"}
# fit model
vote96_rf <- train(
  vote96 ~ ., 
  data = mh, 
  method = "ranger",
  trControl = cv_ctrl,
  tuneLength = 10
)
```

```{r vote96-boost, dependson = "vote96"}
# fit model
vote96_boost <- train(
  vote96 ~ ., 
  data = mh, 
  method = "xgbTree",
  trControl = cv_ctrl,
  tuneLength = 3
)
```

```{r parallel-finish, cache = FALSE}
stopCluster(cl)
```

```{r vote96-compare-roc, dependson = c("vote96-svm-line","vote96-svm-poly","vote96-svm-radial", "vote96-logit", "vote96-tree", "vote96-bag", "vote96-rf", "vote96-boost")}
# ROC
bind_rows(
  `SVM (linear)` = svm_linear$pred,
  `SVM (polynomial)` = svm_poly$pred,
  `SVM (radial)` = svm_radial$pred,
  `Logistic regression` = vote96_glm$pred,
  `Decision tree` = vote96_tree$pred,
  `Bagging` = vote96_bag$pred,
  `Random forest` = vote96_rf$pred,
  `Boosting` = vote96_boost$pred,
  .id = "kernel"
) %>%
  group_by(kernel) %>%
  roc_curve(truth = obs, estimate = Yes) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = kernel)) +
  geom_path() +
  geom_abline(lty = 3) +
  scale_color_brewer(type = "qual") +
  coord_equal() +
  labs(color = NULL)

# AUC
bind_rows(
  `SVM (linear)` = svm_linear$pred,
  `SVM (polynomial)` = svm_poly$pred,
  `SVM (radial)` = svm_radial$pred,
  `Logistic regression` = vote96_glm$pred,
  `Decision tree` = vote96_tree$pred,
  `Bagging` = vote96_bag$pred,
  `Random forest` = vote96_rf$pred,
  `Boosting` = vote96_boost$pred,
  .id = "kernel"
) %>%
  group_by(kernel) %>%
  roc_auc(truth = obs, Yes) %>%
  group_by(kernel) %>%
  summarize(.estimate = mean(.estimate)) %>%
  ggplot(aes(fct_reorder(kernel, .estimate), .estimate)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  coord_flip() +
  labs(title = "Comparison of area under the curve",
       subtitle = "10-fold CV",
       x = "Algorithm",
       y = "Area under the curve")
```

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* @james2013introduction
* @friedman2001elements
