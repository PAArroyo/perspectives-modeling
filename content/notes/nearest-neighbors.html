---
title: Nearest Neighbors
date: 2019-01-23T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Classification
    weight: 4
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#naive-non-parametric-regression"><span class="toc-section-number">1</span> Naive non-parametric regression</a></li>
<li><a href="#k-nearest-neighbors-regression"><span class="toc-section-number">2</span> <span class="math inline">\(K\)</span>-nearest neighbors regression</a><ul>
<li><a href="#linear-regression-vs.-k-nearest-neighbors"><span class="toc-section-number">2.1</span> Linear regression vs. <span class="math inline">\(K\)</span>-nearest neighbors</a></li>
<li><a href="#weighted-k-nearest-neighbors"><span class="toc-section-number">2.2</span> Weighted <span class="math inline">\(K\)</span>-nearest neighbors</a></li>
<li><a href="#estimating-knn-on-simulated-wage-data"><span class="toc-section-number">2.3</span> Estimating KNN on simulated wage data</a></li>
<li><a href="#knn-on-biden"><span class="toc-section-number">2.4</span> KNN on Biden</a></li>
</ul></li>
<li><a href="#bayes-decision-rule"><span class="toc-section-number">3</span> Bayes decision rule</a></li>
<li><a href="#k-nearest-neighbors-classification"><span class="toc-section-number">4</span> <span class="math inline">\(K\)</span>-nearest neighbors classification</a><ul>
<li><a href="#applying-knn-to-titanic"><span class="toc-section-number">4.1</span> Applying KNN to Titanic</a></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">5</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">6</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(rsample)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(patchwork)
<span class="kw">library</span>(FNN)
<span class="kw">library</span>(kknn)
<span class="kw">library</span>(titanic)
<span class="kw">library</span>(rcfss)
<span class="kw">library</span>(pROC)
<span class="kw">library</span>(here)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<div id="naive-non-parametric-regression" class="section level1">
<h1><span class="header-section-number">1</span> Naive non-parametric regression</h1>
<p>Suppose we have detailed information wages and education. We donâ€™t have data for the entire population, but we do have observations for one million employed Americans:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="fl">1e06</span>
wage &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">educ =</span> <span class="kw">rpois</span>(n, <span class="dt">lambda =</span> <span class="dv">12</span>),
               <span class="dt">age =</span> <span class="kw">rpois</span>(n, <span class="dt">lambda =</span> <span class="dv">40</span>),
               <span class="dt">prestige =</span> <span class="kw">rpois</span>(n, <span class="dt">lambda =</span> <span class="dv">3</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">educ =</span> <span class="kw">ifelse</span>(educ <span class="op">&gt;</span><span class="st"> </span><span class="dv">25</span>, <span class="dv">25</span>, educ),
         <span class="dt">wage =</span> <span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>educ <span class="op">+</span><span class="st"> </span>.<span class="dv">5</span> <span class="op">*</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span>prestige <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">3</span>))

<span class="kw">ggplot</span>(wage, <span class="kw">aes</span>(wage)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Histogram of simulated income data&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Binwidth = 5&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Income, in thousands of dollars&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Frequency count&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-data-1.png" width="672" /></p>
<p>If we want to estimate the income for an individual given their education level <span class="math inline">\((0, 1, 2, \dots, 25)\)</span>, we could estimate the conditional distribution of income for each of these values:</p>
<p><span class="math display">\[\mu = {\mathrm{E}}(\text{Income}|\text{Education}) = f(\text{Education})\]</span></p>
<p>For each level of education, the conditional (or expected) income would be the mean or median of all individuals in the sample with the same level of education.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wage <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(educ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(wage),
            <span class="dt">sd =</span> <span class="kw">sd</span>(wage)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(educ, mean, <span class="dt">ymin =</span> mean <span class="op">-</span><span class="st"> </span>sd, <span class="dt">ymax =</span> mean <span class="op">+</span><span class="st"> </span>sd)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Conditional income, by education level&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Plus/minus SD&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Education level&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Income, in thousands of dollars&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-wage-cond-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wage <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(educ <span class="op">==</span><span class="st"> </span><span class="dv">12</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(wage)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">mean</span>(wage<span class="op">$</span>wage[wage<span class="op">$</span>educ <span class="op">==</span><span class="st"> </span><span class="dv">12</span>]), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Conditional distribution of income for education = 12&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">str_c</span>(<span class="st">&quot;Mean income = &quot;</span>, <span class="kw">formatC</span>(<span class="kw">mean</span>(wage<span class="op">$</span>wage[wage<span class="op">$</span>educ <span class="op">==</span><span class="st"> </span><span class="dv">12</span>]), <span class="dt">digits =</span> <span class="dv">3</span>)),
       <span class="dt">x =</span> <span class="st">&quot;Income, in thousands of dollars&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Frequency count&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-wage-cond-2.png" width="672" /></p>
<p>Imagine instead that we we have <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, two continuous variables from a sample of a population, and we want to understand the relationship between the variables. Specifically, we want to use our knowledge of <span class="math inline">\(X\)</span> to predict <span class="math inline">\(Y\)</span>. Therefore what we want to know is the mean value of <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span> in the population of individuals from whom the sample was drawn:</p>
<p><span class="math display">\[\mu = {\mathrm{E}}(Y|x) = f(x)\]</span></p>
<p>Unfortunately because <span class="math inline">\(X\)</span> is continuous, it is unlikely that we would draw precisely the same values of <span class="math inline">\(X\)</span> for more than a single observation. Therefore we cannot directly calculate the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and therefore cannot calculate conditional means. Instead, we can divide <span class="math inline">\(X\)</span> into many narrow intervals (or <strong>bins</strong>), just like we would for a histogram. Within each bin we can estimate the conditional distribution of <span class="math inline">\(Y\)</span> and estimate the conditional mean of <span class="math inline">\(Y\)</span> with great precision.</p>
<p>If we have fewer observations, then we have to settle for fewer bins and less precision in our estimates. Here we use data on the average income of 102 different occupations in Canada and their relationship to occupational prestige (measured continuously):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get data</span>
prestige &lt;-<span class="st"> </span><span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;prestige.csv&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">read_csv</span>()</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   occupation = col_character(),
##   education = col_double(),
##   income = col_double(),
##   women = col_double(),
##   prestige = col_double(),
##   census = col_double(),
##   type = col_character()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bin into 5 and get means</span>
prestige_bin &lt;-<span class="st"> </span>prestige <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">bin =</span> <span class="kw">cut_number</span>(income, <span class="dv">6</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(bin) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prestige =</span> <span class="kw">mean</span>(prestige),
            <span class="dt">income =</span> <span class="kw">mean</span>(income))

<span class="co"># get cutpoints</span>
labs &lt;-<span class="st"> </span><span class="kw">levels</span>(prestige_bin<span class="op">$</span>bin)
cutpoints &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">as.numeric</span>( <span class="kw">sub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">((.+),.*&quot;</span>, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">1&quot;</span>, labs) ),
               <span class="kw">as.numeric</span>( <span class="kw">sub</span>(<span class="st">&quot;[^,]*,([^]]*)</span><span class="ch">\\</span><span class="st">]&quot;</span>, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">1&quot;</span>, labs) )) <span class="op">%&gt;%</span>
<span class="st">  </span>unique <span class="op">%&gt;%</span>
<span class="st">  </span>sort <span class="op">%&gt;%</span>
<span class="st">  </span>.[<span class="dv">2</span><span class="op">:</span>(<span class="kw">length</span>(.)<span class="op">-</span><span class="dv">1</span>)] <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">enframe</span>(<span class="dt">name =</span> <span class="ot">NULL</span>)</code></pre></div>
<pre><code>## Warning in eval(lhs, parent, parent): NAs introduced by coercion</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(prestige, <span class="kw">aes</span>(income, prestige)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> prestige_bin) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> prestige_bin) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">data =</span> cutpoints, <span class="kw">aes</span>(<span class="dt">xintercept =</span> value), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Naive nonparametric regression&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Bins = 5&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Average income (in dollars)&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Occupational prestige&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/prestige-5bins-1.png" width="672" /></p>
<p>The <span class="math inline">\(X\)</span>-axis is carved into 5 bins with roughly 20 observations in each bin. The line is a <strong>naive nonparametric regression line</strong> that is calculated by connecting the points defined by the conditional variable means <span class="math inline">\(\bar{Y}\)</span> and the explanatory variable means <span class="math inline">\(\bar{X}\)</span> in the five intervals.</p>
<p>Just like ordinary least squares regression (OLS), this regression line also suffers from <strong>bias</strong> and <strong>variance</strong>. If the actual relationship between prestige and income is non-linear <strong>within a bin</strong>, then our estimate of the conditional mean <span class="math inline">\(\bar{Y}\)</span> will be biased towards a linear relationship. We can minimize bias by making the bins as numerous and narrow as possible:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bin into 50 and get means</span>
prestige_bin &lt;-<span class="st"> </span>prestige <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">bin =</span> <span class="kw">cut_number</span>(income, <span class="dv">51</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(bin) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prestige =</span> <span class="kw">mean</span>(prestige),
            <span class="dt">income =</span> <span class="kw">mean</span>(income))

<span class="co"># get cutpoints</span>
labs &lt;-<span class="st"> </span><span class="kw">levels</span>(prestige_bin<span class="op">$</span>bin)
cutpoints &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">as.numeric</span>( <span class="kw">sub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">((.+),.*&quot;</span>, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">1&quot;</span>, labs) ),
               <span class="kw">as.numeric</span>( <span class="kw">sub</span>(<span class="st">&quot;[^,]*,([^]]*)</span><span class="ch">\\</span><span class="st">]&quot;</span>, <span class="st">&quot;</span><span class="ch">\\</span><span class="st">1&quot;</span>, labs) )) <span class="op">%&gt;%</span>
<span class="st">  </span>unique <span class="op">%&gt;%</span>
<span class="st">  </span>sort <span class="op">%&gt;%</span>
<span class="st">  </span>.[<span class="dv">2</span><span class="op">:</span>(<span class="kw">length</span>(.)<span class="op">-</span><span class="dv">1</span>)] <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">enframe</span>(<span class="dt">name =</span> <span class="ot">NULL</span>)</code></pre></div>
<pre><code>## Warning in eval(lhs, parent, parent): NAs introduced by coercion</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(prestige, <span class="kw">aes</span>(income, prestige)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> prestige_bin) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> prestige_bin) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">data =</span> cutpoints, <span class="kw">aes</span>(<span class="dt">xintercept =</span> value), <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">alpha =</span> .<span class="dv">25</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Naive nonparametric regression&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Bins = 50&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Average income (in dollars)&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Occupational prestige&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/prestige-50bins-1.png" width="672" /></p>
<p>But now we have introduced overfitting into the nonparametric regression estimates. In addition, we substantially increased our variance of the estimated conditional sample means <span class="math inline">\(\bar{Y}\)</span>. If we were to draw a new sample, the estimated conditional sample means <span class="math inline">\(\bar{Y}\)</span> could be widely different from the original model and our resulting estimates of the conditional sample means will be highly variable.</p>
<p>Naive nonparametric regression is a consistent estimator of the population regression curve as the sample size increases. As <span class="math inline">\(n \rightarrow \infty\)</span>, we can shrink the size of the individual intervals and still have sizeable numbers of observations in each interval. In the limit, we have an infinite number of intervals and infinite number of observations in each interval, so the naive nonparametric regression line and the population regression line are identical.</p>
<p>As a practical consideration, if our sample size <span class="math inline">\(n\)</span> is truly large, then naive nonparametric regression could be a good estimation procedure. However as we introduce multiple explanatory variables into the model, the problem starts to blow up. Assume we have three discrete explanatory variables each with 10 possible values:</p>
<p><span class="math display">\[X_1 \in \{1, 2, \dots ,10 \}\]</span> <span class="math display">\[X_2 \in \{1, 2, \dots ,10 \}\]</span> <span class="math display">\[X_3 \in \{1, 2, \dots ,10 \}\]</span></p>
<p>There are then <span class="math inline">\(10^3 = 1000\)</span> possible combinations of the explanatory variables and <span class="math inline">\(1000\)</span> conditional expectations of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\mu = {\mathrm{E}}(Y|x_1, x_2, x_3) = f(x_1, x_2, x_3)\]</span></p>
<p>In order to accurate estimate conditional expectations for each category, we would need substantial numbers of observations <strong>for every combination of <span class="math inline">\(X\)</span></strong>. This would require a sample size far greater than most social scientists have the resources to collect.</p>
<p>Letâ€™s return to our simulated wage data. Our dataset contains information on education, age, and job prestige:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(wage, <span class="kw">aes</span>(educ)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of simulated wage data&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Education&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Frequency count&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wage-sim-describe-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(wage, <span class="kw">aes</span>(age)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of simulated wage data&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Frequency count&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wage-sim-describe-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(wage, <span class="kw">aes</span>(prestige)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of simulated wage data&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Job prestige&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Frequency count&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wage-sim-describe-3.png" width="672" /></p>
<p>Can we estimate naive nonparametric regression on this dataset with <span class="math inline">\(N = 1,000,000\)</span>?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wage_np &lt;-<span class="st"> </span>wage <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(educ, age, prestige) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">wage_mean =</span> <span class="kw">mean</span>(wage),
            <span class="dt">wage_sd =</span> <span class="kw">sd</span>(wage),
            <span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">complete</span>(educ, age, prestige, <span class="dt">fill =</span> <span class="kw">list</span>(<span class="dt">wage_mean =</span> <span class="ot">NA</span>,
                                            <span class="dt">wage_sd =</span> <span class="ot">NA</span>,
                                            <span class="dt">n =</span> <span class="dv">0</span>))

<span class="co"># number of unique combos </span>
wage_unique &lt;-<span class="st"> </span><span class="kw">nrow</span>(wage_np)

<span class="co"># n for each unique combo</span>
<span class="kw">ggplot</span>(wage_np, <span class="kw">aes</span>(n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Naive nonparametric regression of simulated wage data&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Number of observations for each unique combination&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wage-sim-np-1.png" width="672" /></p>
<p>Even on a dataset with <span class="math inline">\(1,000,000\)</span> observations, for the vast majority of the potential combinations of variables we have zero observations from which to generate expected values. What if we instead drew <span class="math inline">\(10,000,000\)</span> observations from the same data generating process?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="fl">1e07</span>
wage10 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">educ =</span> <span class="kw">rpois</span>(n, <span class="dt">lambda =</span> <span class="dv">12</span>),
                 <span class="dt">age =</span> <span class="kw">rpois</span>(n, <span class="dt">lambda =</span> <span class="dv">40</span>),
                 <span class="dt">prestige =</span> <span class="kw">rpois</span>(n, <span class="dt">lambda =</span> <span class="dv">3</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">educ =</span> <span class="kw">ifelse</span>(educ <span class="op">&gt;</span><span class="st"> </span><span class="dv">25</span>, <span class="dv">25</span>, educ),
         <span class="dt">wage =</span> <span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>educ <span class="op">+</span><span class="st"> </span>.<span class="dv">5</span> <span class="op">*</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span>prestige <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">3</span>))

wage10_np &lt;-<span class="st"> </span>wage10 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(educ, age, prestige) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">wage_mean =</span> <span class="kw">mean</span>(wage),
            <span class="dt">wage_sd =</span> <span class="kw">sd</span>(wage),
            <span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">complete</span>(educ, age, prestige, <span class="dt">fill =</span> <span class="kw">list</span>(<span class="dt">wage_mean =</span> <span class="ot">NA</span>,
                                            <span class="dt">wage_sd =</span> <span class="ot">NA</span>,
                                            <span class="dt">n =</span> <span class="dv">0</span>))

<span class="co"># number of unique combos </span>
wage10_unique &lt;-<span class="st"> </span><span class="kw">nrow</span>(wage10_np)

<span class="co"># n for each unique combo</span>
<span class="kw">ggplot</span>(wage10_np, <span class="kw">aes</span>(n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Naive nonparametric regression of simulated wage data&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Number of observations for each unique combination&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wage-sim-np-ten-1.png" width="672" /></p>
<p>Unless your dataset is extremely large or you have a small handful of variables with a low number of unique values, naive nonparametric estimation will not be effective.</p>
</div>
<div id="k-nearest-neighbors-regression" class="section level1">
<h1><span class="header-section-number">2</span> <span class="math inline">\(K\)</span>-nearest neighbors regression</h1>
<p>An alternative, but related, method is called <strong><span class="math inline">\(K\)</span>-nearest neighbors regression</strong> (KNN regression). Rather than binning the data into discrete and fixed intervals, KNN regression uses a moving average to generate the regression line. Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression identifies the <span class="math inline">\(K\)</span> training observations nearest to the prediction point <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span> and estimates <span class="math inline">\(f(x_0)\)</span> as the average of all the training responses in <span class="math inline">\(N_0\)</span>:</p>
<p><span class="math display">\[\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in N_0} y_i\]</span></p>
<p>With <span class="math inline">\(K=1\)</span>, the resulting KNN regression line will fit the training observations extraordinarily well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prestige_knn1 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(prestige, income), <span class="dt">y =</span> prestige<span class="op">$</span>prestige,
                         <span class="dt">test =</span> <span class="kw">select</span>(prestige, income), <span class="dt">k =</span> <span class="dv">1</span>)

prestige <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> prestige_knn1<span class="op">$</span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(income, prestige)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;1-nearest neighbor regression&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Income (in dollars)&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Occupational prestige&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/prestige-knn-1-1.png" width="672" /></p>
<p>Perhaps a bit too well. Compare this to <span class="math inline">\(K=9\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prestige_knn9 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(prestige, income), <span class="dt">y =</span> prestige<span class="op">$</span>prestige,
                         <span class="dt">test =</span> <span class="kw">select</span>(prestige, income), <span class="dt">k =</span> <span class="dv">9</span>)

prestige <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> prestige_knn9<span class="op">$</span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(income, prestige)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;9-nearest neighbor regression&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Income (in dollars)&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Occupational prestige&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/prestige-knn-9-1.png" width="672" /></p>
<p>This regression line averages over the nine nearest observations; while still a step function, it is smoother than <span class="math inline">\(K=1\)</span>. Small values for <span class="math inline">\(K\)</span> provide low bias estimates of the training observations but high variance. Large values for <span class="math inline">\(K\)</span> provide low variance but higher bias by masking some of the structure of <span class="math inline">\(f(X)\)</span>.</p>
<div id="linear-regression-vs.-k-nearest-neighbors" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear regression vs. <span class="math inline">\(K\)</span>-nearest neighbors</h2>
<p>Parametric methods such as linear regression are superior to non-parametric methods such as KNN regression when the parametric approach accurately assumes the true functional form of <span class="math inline">\(f\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
              <span class="dt">y =</span> <span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>))

sim_knn9 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(sim, x), <span class="dt">y =</span> sim<span class="op">$</span>y,
                    <span class="dt">test =</span> <span class="kw">select</span>(sim, x), <span class="dt">k =</span> <span class="dv">9</span>)

sim <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> sim_knn9<span class="op">$</span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;True&quot;</span>), <span class="dt">intercept =</span> <span class="dv">2</span>, <span class="dt">slope =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;LM&quot;</span>), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">color =</span> <span class="st">&quot;KNN&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="st">&quot;Method&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-p-line-1.png" width="672" /></p>
<p>Here we simulate data from a linear relationship:</p>
<p><span class="math display">\[f(x) = 2 + x + \epsilon_i\]</span></p>
<p>The black line represents the true model, the blue line represents the linear regression model, and the red line represents the 9-nearest neighbor regression line. As we can see, the linear model does a great job approximating the true model because we have defined the relationship to be linear and that is what OLS attempts to estimate. The KNN line is too jumpy and contours too closely to the training data to capture the true relationship.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate test MSE for LM and KNN models</span>
sim_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
                   <span class="dt">y =</span> <span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>))
mse_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> sim) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(sim_test)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,
                  <span class="dt">knn =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(sim, x), <span class="dt">y =</span> sim<span class="op">$</span>y,
                                         <span class="dt">test =</span> <span class="kw">select</span>(sim_test, x), <span class="dt">k =</span> .)),
                  <span class="dt">mse =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((sim_test<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> mse_lm, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-p-line2-1.png" width="672" /></p>
<p>As <span class="math inline">\(k\)</span> increases, KNN regression does a better job approximating the linear relationship. Here we can see as <span class="math inline">\(k\)</span> increases, the test MSE (based on a separate draw of observations from the data generating process) shrinks and approaches the test MSE based on the linear model (the dashed line).</p>
<p>Of course as non-linearity in the true relationship increases, KNN will perform better relative to a parametric model which assumes linearity:</p>
<p><span class="math display">\[f(x) = 2 + x + x^2 + x^3 + \epsilon_i\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_cube &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x<span class="op">^</span><span class="dv">3</span>
}

sim &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
              <span class="dt">y =</span> <span class="kw">x_cube</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>))

sim_knn9 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(sim, x), <span class="dt">y =</span> sim<span class="op">$</span>y,
                    <span class="dt">test =</span> <span class="kw">select</span>(sim, x), <span class="dt">k =</span> <span class="dv">9</span>)

sim <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> sim_knn9<span class="op">$</span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;True&quot;</span>), <span class="dt">fun =</span> x_cube) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;LM&quot;</span>), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">color =</span> <span class="st">&quot;KNN&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="st">&quot;Method&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-p-cubic-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate test MSE for LM and KNN models</span>
sim_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
                   <span class="dt">y =</span> <span class="kw">x_cube</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>))

mse_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> sim) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(sim_test)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,
                  <span class="dt">knn =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(sim, x), <span class="dt">y =</span> sim<span class="op">$</span>y,
                                         <span class="dt">test =</span> <span class="kw">select</span>(sim_test, x), <span class="dt">k =</span> .)),
                  <span class="dt">mse =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((sim_test<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> mse_lm, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/np-p-cubic-2.png" width="672" /></p>
<p>While KNN performs better in the presence of a non-linear relationship, it also performs worse as the number of predictors <span class="math inline">\(p\)</span> increases. Here we use the same data generating process as before:</p>
<p><span class="math display">\[f(x) = 2 + x + x^2 + x^3 + \epsilon_i\]</span></p>
<p>But this time generate additional <strong>noise parameters</strong>, or variables that are not actually included in <span class="math inline">\(f(x)\)</span> but are included in <span class="math inline">\(\hat{f}(x)\)</span>. Linear regression is more robust to the addition of such parameters and the test MSE increases more slowly as a result. Compare this to the change in test MSE for KNN regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_nr &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
                 <span class="dt">y =</span> <span class="kw">x_cube</span>(x1) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>),
                 <span class="dt">x2 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                 <span class="dt">x3 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                 <span class="dt">x4 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                 <span class="dt">x5 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                 <span class="dt">x6 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>))
sim_nr_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
                      <span class="dt">y =</span> <span class="kw">x_cube</span>(x1) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>),
                      <span class="dt">x2 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                      <span class="dt">x3 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                      <span class="dt">x4 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                      <span class="dt">x5 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>),
                      <span class="dt">x6 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>))

sim_pred_knn &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">p =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,
                            <span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lm =</span> <span class="kw">map</span>(p, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(<span class="kw">formula</span>(<span class="kw">str_c</span>(<span class="st">&quot;y ~ &quot;</span>, <span class="kw">str_c</span>(<span class="st">&quot;x&quot;</span>, <span class="kw">seq.int</span>(.), <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>))),
                          <span class="dt">data =</span> sim_nr)),
         <span class="dt">mse_lm =</span> <span class="kw">map_dbl</span>(lm, <span class="op">~</span><span class="st"> </span><span class="kw">mse</span>(., sim_nr_test)),
         <span class="dt">knn =</span> <span class="kw">map2</span>(p, k, <span class="op">~</span><span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select_</span>(sim_nr, <span class="dt">.dots =</span> <span class="kw">str_c</span>(<span class="st">&quot;x&quot;</span>, <span class="kw">seq.int</span>(.x))),
                                    <span class="dt">y =</span> sim_nr<span class="op">$</span>y,
                                    <span class="dt">test =</span> <span class="kw">select_</span>(sim_nr_test, <span class="dt">.dots =</span> <span class="kw">str_c</span>(<span class="st">&quot;x&quot;</span>, <span class="kw">seq.int</span>(.x))),
                                    <span class="dt">k =</span> .y)),
         <span class="dt">mse_knn =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((sim_nr_test<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(sim_pred_knn, <span class="kw">aes</span>(k, mse_knn)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(. <span class="op">~</span><span class="st"> </span>p, <span class="dt">labeller =</span> <span class="kw">labeller</span>(<span class="dt">p =</span> label_both)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept =</span> mse_lm), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Test MSE for linear regression vs. KNN&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-nonrobust-1.png" width="672" /></p>
<p>We should be able to anticipate this problem. It is the same pitfall of naive nonparametric regression: as the number of predictors increases, the number of dimensions also increases. Spreading 100 observations over <span class="math inline">\(p=6\)</span> dimensions results in the problem that for many observations, there are no nearby neighbors. The closest neighbor may be extraordinarily far away in <span class="math inline">\(p\)</span>-dimensional space, so the prediction resulting from averaging across these neighbors is poor.</p>
</div>
<div id="weighted-k-nearest-neighbors" class="section level2">
<h2><span class="header-section-number">2.2</span> Weighted <span class="math inline">\(K\)</span>-nearest neighbors</h2>
<p>One alternative to this conundrum is to use a weighting function to weight our KNN estimates based on the <strong>distance</strong> of the nearby neighbors. For example, the <code>kknn</code> package weights KNN estimates based on the <strong>Minkowski distance</strong> between points:</p>
<p><span class="math display">\[\text{Distance}(x_i, y_i) = \left( \sum_{i = 1}^n |x_i - y_i| ^p \right)^\frac{1}{p}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the order parameter (not the number of predictors). <span class="math inline">\(p=1\)</span> results in <strong>Manhattan distance</strong>, while <span class="math inline">\(p=2\)</span> is known as <strong>Euclidean distance</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
              <span class="dt">y =</span> <span class="kw">x_cube</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>))

sim_wknn &lt;-<span class="st"> </span><span class="kw">kknn</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> sim, <span class="dt">test =</span> sim, <span class="dt">k =</span> <span class="dv">5</span>)

sim <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> sim_wknn[[<span class="st">&quot;fitted.values&quot;</span>]]) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;True&quot;</span>), <span class="dt">fun =</span> x_cube) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;LM&quot;</span>), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">color =</span> <span class="st">&quot;KNN&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;5-nearest neighbor regression&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Euclidean distance weighting&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Method&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-weight-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate test MSE for LM and KNN models</span>
sim_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
                   <span class="dt">y =</span> <span class="kw">x_cube</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, .<span class="dv">2</span>))

mse_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> sim) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(sim_test)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,
                  <span class="dt">knn =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">kknn</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> sim, <span class="dt">test =</span> sim_test, <span class="dt">k =</span> .)),
                  <span class="dt">mse =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((sim_test<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>fitted.values)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> mse_lm, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-weight-2.png" width="672" /></p>
<p>Letâ€™s compare the robustness of KNN vs.Â weighted KNN using the previous example with random noise parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_pred_wknn &lt;-<span class="st"> </span>sim_pred_knn <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">wknn =</span> <span class="kw">map2</span>(p, k, <span class="op">~</span><span class="st"> </span><span class="kw">kknn</span>(<span class="kw">formula</span>(<span class="kw">str_c</span>(<span class="st">&quot;y ~ &quot;</span>,
                                                <span class="kw">str_c</span>(<span class="st">&quot;x&quot;</span>, <span class="kw">seq.int</span>(.x), <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>))),
                                  <span class="dt">train =</span> sim_nr, <span class="dt">test =</span> sim_nr_test, <span class="dt">k =</span> .y)),
         <span class="dt">mse_wknn =</span> <span class="kw">map_dbl</span>(wknn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((sim_nr_test<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>fitted.values)<span class="op">^</span><span class="dv">2</span>)))
sim_pred_lm &lt;-<span class="st"> </span>sim_pred_wknn <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(p, k, mse_lm) <span class="op">%&gt;%</span>
<span class="st">  </span>distinct

sim_pred_wknn <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(p, k, <span class="kw">contains</span>(<span class="st">&quot;mse&quot;</span>), <span class="op">-</span>mse_lm) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(method, mse, <span class="kw">contains</span>(<span class="st">&quot;mse&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="kw">str_replace</span>(method, <span class="st">&quot;mse_&quot;</span>, <span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="kw">factor</span>(method, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;knn&quot;</span>, <span class="st">&quot;wknn&quot;</span>),
                         <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;KNN&quot;</span>, <span class="st">&quot;Weighted KNN&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(k, mse, <span class="dt">color =</span> method)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(. <span class="op">~</span><span class="st"> </span>p, <span class="dt">labeller =</span> <span class="kw">labeller</span>(<span class="dt">p =</span> label_both)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">data =</span> sim_pred_lm, <span class="kw">aes</span>(<span class="dt">yintercept =</span> mse_lm), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Test MSE for linear regression vs. KNN&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Traditional and weighted KNN&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>,
       <span class="dt">method =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wknn-nonrobust-1.png" width="672" /></p>
</div>
<div id="estimating-knn-on-simulated-wage-data" class="section level2">
<h2><span class="header-section-number">2.3</span> Estimating KNN on simulated wage data</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># split into train/test set</span>
wage_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(wage, <span class="dt">prop =</span> <span class="fl">0.5</span>)
wage_train &lt;-<span class="st"> </span><span class="kw">training</span>(wage_split)
wage_test &lt;-<span class="st"> </span><span class="kw">testing</span>(wage_split)

<span class="co"># estimate test MSE for LM and KNN models</span>
mse_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(wage <span class="op">~</span><span class="st"> </span>educ <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>prestige, <span class="dt">data =</span> wage_train) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(wage_test)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">100</span>, <span class="dt">by =</span> <span class="dv">10</span>)),
                  <span class="dt">knn =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(wage_train, <span class="op">-</span>wage), <span class="dt">y =</span> wage_train<span class="op">$</span>wage,
                                         <span class="dt">test =</span> <span class="kw">select</span>(wage_test, <span class="op">-</span>wage), <span class="dt">k =</span> .)),
                  <span class="dt">mse =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((wage_test<span class="op">$</span>wage <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> mse_lm, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;KNN on simulated wage data&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/wage-sim-knn-1.png" width="672" /></p>
</div>
<div id="knn-on-biden" class="section level2">
<h2><span class="header-section-number">2.4</span> KNN on Biden</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">biden &lt;-<span class="st"> </span><span class="kw">here</span>(<span class="st">&quot;static&quot;</span>, <span class="st">&quot;data&quot;</span>, <span class="st">&quot;biden.csv&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">read_csv</span>()</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   biden = col_double(),
##   female = col_double(),
##   age = col_double(),
##   educ = col_double(),
##   dem = col_double(),
##   rep = col_double()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># split into train/test set</span>
biden_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(biden, <span class="dt">p =</span> <span class="fl">0.7</span>)
biden_train &lt;-<span class="st"> </span><span class="kw">training</span>(biden_split)
biden_test &lt;-<span class="st"> </span><span class="kw">testing</span>(biden_split)

<span class="co"># estimate test MSE for LM and KNN models</span>
mse_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(biden <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> biden_train) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(biden_test)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">100</span>, <span class="dt">by =</span> <span class="dv">10</span>)),
                  <span class="dt">knn =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">knn.reg</span>(<span class="kw">select</span>(biden_train, <span class="op">-</span>biden), <span class="dt">y =</span> biden_train<span class="op">$</span>biden,
                                         <span class="dt">test =</span> <span class="kw">select</span>(biden_test, <span class="op">-</span>biden), <span class="dt">k =</span> .)),
                  <span class="dt">mse =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((biden_test<span class="op">$</span>biden <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> mse_lm, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;KNN for Biden&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/biden-knn-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate test MSE for LM and WKNN models</span>
mse_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(biden <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> biden_train) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mse</span>(biden_test)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">100</span>, <span class="dt">by =</span> <span class="dv">10</span>)),
                  <span class="dt">knn =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">kknn</span>(biden <span class="op">~</span><span class="st"> </span>.,
                                      <span class="dt">train =</span> biden_train, <span class="dt">test =</span> biden_test, <span class="dt">k =</span> .)),
                  <span class="dt">mse =</span> <span class="kw">map_dbl</span>(knn, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((sim_test<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>fitted.values)<span class="op">^</span><span class="dv">2</span>)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> mse_lm, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Weighted KNN for Biden&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test mean squared error&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/biden-wknn-1.png" width="672" /></p>
</div>
</div>
<div id="bayes-decision-rule" class="section level1">
<h1><span class="header-section-number">3</span> Bayes decision rule</h1>
<p>For classification problems, the test error rate is minimized by a simple classifier that assigns each observation to the most likely class given its predictor values:</p>
<p><span class="math display">\[\Pr(Y = j | X = x_0)\]</span></p>
<p>where <span class="math inline">\(x_0\)</span> is the test observation and each possible class is represented by <span class="math inline">\(J\)</span>. This is a <strong>conditional probability</strong> that <span class="math inline">\(Y = j\)</span>, given the observed predictor vector<span class="math inline">\(x_0\)</span>. This classifier is known as the <strong>Bayes classifier</strong>. If the response variable is binary (i.e.Â two classes), the Bayes classifier corresponds to predicting class one if <span class="math inline">\(\Pr(Y = 1 | X = x_0) &gt; 0.5\)</span>, and class two otherwise.</p>
<p>In the simulated example below, the blue lines indicate the <strong>Bayes decision boundary</strong>. The Bayes classifierâ€™s prediction is determined by this boundary: observations on the blue side would be assigned to the blue class, and observations on the red side are assigned to the red class.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bayes_rule &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2) {
  x1 <span class="op">+</span><span class="st"> </span>x1<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x2<span class="op">^</span><span class="dv">2</span>
}

bayes_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">x1 =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">by =</span> .<span class="dv">05</span>),
                          <span class="dt">x2 =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">by =</span> .<span class="dv">05</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">logodds =</span> <span class="kw">bayes_rule</span>(x1, x2),
         <span class="dt">y =</span> logodds <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>,
         <span class="dt">prob =</span> <span class="kw">logit2prob</span>(logodds))

bayes_bound &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(<span class="kw">mutate</span>(bayes_grid,
                                <span class="dt">prob =</span> prob,
                                <span class="dt">cls =</span> <span class="ot">TRUE</span>,
                                <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls, <span class="dv">1</span>, <span class="dv">0</span>)),
                         <span class="kw">mutate</span>(bayes_grid,
                                <span class="dt">prob =</span> prob,
                                <span class="dt">cls =</span> <span class="ot">FALSE</span>,
                                <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls, <span class="dv">1</span>, <span class="dv">0</span>)))

sim_bayes &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="kw">runif</span>(<span class="dv">200</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),
                    <span class="dt">x2 =</span> <span class="kw">runif</span>(<span class="dv">200</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),
                    <span class="dt">logodds =</span> <span class="kw">bayes_rule</span>(x1, x2) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, .<span class="dv">5</span>),
                    <span class="dt">y =</span> logodds <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>,
                    <span class="dt">y_actual =</span> <span class="kw">bayes_rule</span>(x1, x2) <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>)
sim_bayes_err &lt;-<span class="st"> </span><span class="kw">mean</span>(sim_bayes<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span>sim_bayes<span class="op">$</span>y_actual)

<span class="kw">ggplot</span>(bayes_bound, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> .<span class="dv">5</span>, <span class="dt">alpha =</span> .<span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sim_bayes) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/bayes-class-1.png" width="672" /></p>
<p>The Bayes classifer produces the lowest possible test error rate, called the <strong>Bayes error rule</strong>, because it will always assign observations based on the maximum conditional probability:</p>
<p><span class="math display">\[1 - {\mathrm{E}}\left( \max_j \Pr(Y = j | X) \right)\]</span></p>
<p>where the expectation averages the probability over all possible values of <span class="math inline">\(X\)</span>. In this simulation the Bayes error rate is <span class="math inline">\(0.165\)</span>. Because in the true population the classes overlap somewhat, the Bayes classifier cannot generate an error rate of zero.</p>
</div>
<div id="k-nearest-neighbors-classification" class="section level1">
<h1><span class="header-section-number">4</span> <span class="math inline">\(K\)</span>-nearest neighbors classification</h1>
<p>Unfortunately we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> in real-world data, so we cannot compute the Bayes classifier. Instead we try to produce <strong>estimates</strong> of the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and then classify a given observation to the class with the highest estimated probability. Logistic regression and other types of GLMs, tree-based methods, and SVMs all operate on this basic principle (or related ones - in the case of SVMs the decision is based on the test observationâ€™s location relative to the separating hyperplane).</p>
<p>However regression-based methods such as GLMs make assumptions about the functional form of <span class="math inline">\(f\)</span>. For a purely non-parametric approach, we could use <strong><span class="math inline">\(K\)</span>-nearest neighbors</strong> (KNN) classification. Similar to KNN regression, given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the KNN classifier identifies the <span class="math inline">\(K\)</span> nearest training observations to <span class="math inline">\(x_0\)</span>, again represented by <span class="math inline">\(N_0\)</span>. The conditional probability for class <span class="math inline">\(j\)</span> is the fraction of points in <span class="math inline">\(N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>:</p>
<p><span class="math inline">\(\Pr(Y = j| X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j)\)</span>$</p>
<p>where <span class="math inline">\(I(y_i = j)\)</span> is an indicator function. Finally KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability. Here is the KNN classifier applied to the example above with <span class="math inline">\(K=1\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1 &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(sim_bayes, x1, x2), <span class="dt">test =</span> <span class="kw">select</span>(bayes_grid, x1, x2),
                   <span class="dt">cl =</span> sim_bayes<span class="op">$</span>y, <span class="dt">k =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="ot">TRUE</span>)
prob1 &lt;-<span class="st"> </span><span class="kw">attr</span>(knn1, <span class="st">&quot;prob&quot;</span>)

bayes_bound1 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(<span class="kw">mutate</span>(bayes_grid,
                                 <span class="dt">prob =</span> <span class="kw">attr</span>(knn1, <span class="st">&quot;prob&quot;</span>),
                                 <span class="dt">y =</span> <span class="kw">as.logical</span>(knn1),
                                 <span class="dt">cls =</span> <span class="ot">TRUE</span>,
                                 <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls,
                                                   <span class="dv">1</span>, <span class="dv">0</span>)),
                          <span class="kw">mutate</span>(bayes_grid,
                                 <span class="dt">prob =</span> <span class="kw">attr</span>(knn1, <span class="st">&quot;prob&quot;</span>),
                                 <span class="dt">y =</span> <span class="kw">as.logical</span>(knn1),
                                 <span class="dt">cls =</span> <span class="ot">FALSE</span>,
                                 <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls,
                                                   <span class="dv">1</span>, <span class="dv">0</span>)))

<span class="kw">ggplot</span>(bayes_bound, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls, <span class="dt">linetype =</span> <span class="st">&quot;True boundary&quot;</span>), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data =</span> bayes_bound1, <span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls, <span class="dt">linetype =</span> <span class="st">&quot;KNN&quot;</span>), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sim_bayes) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;K nearest neighbor classifier&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(K<span class="op">==</span><span class="dv">1</span>),
       <span class="dt">linetype =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-class1-1.png" width="672" /></p>
<p>All this classifier does is look to the single closest neighbor to make a prediction. Compared to the known decision boundary, this strongly overfits the training data. Compare this to <span class="math inline">\(K=5\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn5 &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(sim_bayes, x1, x2), <span class="dt">test =</span> <span class="kw">select</span>(bayes_grid, x1, x2),
                   <span class="dt">cl =</span> sim_bayes<span class="op">$</span>y, <span class="dt">k =</span> <span class="dv">5</span>, <span class="dt">prob =</span> <span class="ot">TRUE</span>)
prob5 &lt;-<span class="st"> </span><span class="kw">attr</span>(knn5, <span class="st">&quot;prob&quot;</span>)

bayes_bound5 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(<span class="kw">mutate</span>(bayes_grid,
                                 <span class="dt">prob =</span> <span class="kw">attr</span>(knn5, <span class="st">&quot;prob&quot;</span>),
                                 <span class="dt">y =</span> <span class="kw">as.logical</span>(knn5),
                                 <span class="dt">cls =</span> <span class="ot">TRUE</span>,
                                 <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls,
                                                   <span class="dv">1</span>, <span class="dv">0</span>)),
                          <span class="kw">mutate</span>(bayes_grid,
                                 <span class="dt">prob =</span> <span class="kw">attr</span>(knn5, <span class="st">&quot;prob&quot;</span>),
                                 <span class="dt">y =</span> <span class="kw">as.logical</span>(knn5),
                                 <span class="dt">cls =</span> <span class="ot">FALSE</span>,
                                 <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls,
                                                   <span class="dv">1</span>, <span class="dv">0</span>)))

<span class="kw">ggplot</span>(bayes_bound, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls, <span class="dt">linetype =</span> <span class="st">&quot;True boundary&quot;</span>), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data =</span> bayes_bound5, <span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls, <span class="dt">linetype =</span> <span class="st">&quot;KNN&quot;</span>), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sim_bayes) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;K nearest neighbor classifier&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(K<span class="op">==</span><span class="dv">5</span>),
       <span class="dt">linetype =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-class5-1.png" width="672" /></p>
<p>The resulting decision boundary is still choppy, but not as much as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn10 &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(sim_bayes, x1, x2), <span class="dt">test =</span> <span class="kw">select</span>(bayes_grid, x1, x2),
                    <span class="dt">cl =</span> sim_bayes<span class="op">$</span>y, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="ot">TRUE</span>)
prob10 &lt;-<span class="st"> </span><span class="kw">attr</span>(knn10, <span class="st">&quot;prob&quot;</span>)

bayes_bound10 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(<span class="kw">mutate</span>(bayes_grid,
                                  <span class="dt">prob =</span> <span class="kw">attr</span>(knn10, <span class="st">&quot;prob&quot;</span>),
                                  <span class="dt">y =</span> <span class="kw">as.logical</span>(knn5),
                                  <span class="dt">cls =</span> <span class="ot">TRUE</span>,
                                  <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls,
                                                    <span class="dv">1</span>, <span class="dv">0</span>)),
                           <span class="kw">mutate</span>(bayes_grid,
                                  <span class="dt">prob =</span> <span class="kw">attr</span>(knn10, <span class="st">&quot;prob&quot;</span>),
                                  <span class="dt">y =</span> <span class="kw">as.logical</span>(knn5),
                                  <span class="dt">cls =</span> <span class="ot">FALSE</span>,
                                  <span class="dt">prob_cls =</span> <span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>cls,
                                                    <span class="dv">1</span>, <span class="dv">0</span>)))

<span class="kw">ggplot</span>(bayes_bound, <span class="kw">aes</span>(x1, x2, <span class="dt">color =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls, <span class="dt">linetype =</span> <span class="st">&quot;True boundary&quot;</span>), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data =</span> bayes_bound10, <span class="kw">aes</span>(<span class="dt">z =</span> prob_cls, <span class="dt">group =</span> cls, <span class="dt">linetype =</span> <span class="st">&quot;KNN&quot;</span>), <span class="dt">bins =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sim_bayes) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;K nearest neighbor classifier&quot;</span>,
       <span class="dt">subtitle =</span> <span class="kw">expression</span>(K<span class="op">==</span><span class="dv">10</span>),
       <span class="dt">linetype =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-class10-1.png" width="672" /></p>
<p>As with KNN regression, we can calculate the test error rate across different values for <span class="math inline">\(k\)</span> to determine an optimal value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate test MSE for KNN models</span>
sim_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x1 =</span> <span class="kw">runif</span>(<span class="dv">5000</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),
                   <span class="dt">x2 =</span> <span class="kw">runif</span>(<span class="dv">5000</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),
                   <span class="dt">logodds =</span> <span class="kw">bayes_rule</span>(x1, x2) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dv">0</span>, .<span class="dv">5</span>),
                   <span class="dt">y =</span> logodds <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>)

mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,
                  <span class="dt">knn_train =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(sim_bayes, x1, x2),
                                                  <span class="dt">test =</span> <span class="kw">select</span>(sim_bayes, x1, x2),
                                                  <span class="dt">cl =</span> sim_bayes<span class="op">$</span>y, <span class="dt">k =</span> .)),
                  <span class="dt">knn_test =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(sim_bayes, x1, x2),
                                                 <span class="dt">test =</span> <span class="kw">select</span>(sim_test, x1, x2),
                                                 <span class="dt">cl =</span> sim_bayes<span class="op">$</span>y, <span class="dt">k =</span> .)),
                  <span class="dt">mse_train =</span> <span class="kw">map_dbl</span>(knn_train, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(sim_bayes<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span><span class="kw">as.logical</span>(.))),
                  <span class="dt">mse_test =</span> <span class="kw">map_dbl</span>(knn_test, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(sim_test<span class="op">$</span>y <span class="op">!=</span><span class="st"> </span><span class="kw">as.logical</span>(.))))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse_test)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> sim_bayes_err, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test error rate&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/knn-class-compare-1.png" width="672" /></p>
<p>And of course we could adapt this to use LOOCV or <span class="math inline">\(k\)</span>-fold CV rather than the validation set approach.</p>
<div id="applying-knn-to-titanic" class="section level2">
<h2><span class="header-section-number">4.1</span> Applying KNN to Titanic</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic &lt;-<span class="st"> </span>titanic<span class="op">::</span>titanic_train <span class="op">%&gt;%</span>
<span class="st">  </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Name, <span class="op">-</span>Ticket, <span class="op">-</span>Cabin, <span class="op">-</span>PassengerId, <span class="op">-</span>Embarked) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Female =</span> <span class="kw">ifelse</span>(Sex <span class="op">==</span><span class="st"> &quot;female&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Sex) <span class="op">%&gt;%</span>
<span class="st">  </span>na.omit

titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic, <span class="dt">p =</span> <span class="kw">c</span>(<span class="st">&quot;test&quot;</span> =<span class="st"> </span>.<span class="dv">3</span>, <span class="st">&quot;train&quot;</span> =<span class="st"> </span>.<span class="dv">7</span>))
titanic_train &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(titanic_split<span class="op">$</span>train)
titanic_test &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(titanic_split<span class="op">$</span>test)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_logit &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> titanic_train, <span class="dt">family =</span> binomial)
titanic_logit_mse &lt;-<span class="st"> </span><span class="kw">mse.glm</span>(titanic_logit, titanic_test)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate test MSE for KNN models</span>
mse_knn &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,
                  <span class="dt">knn_train =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(titanic_train, <span class="op">-</span>Survived),
                                                  <span class="dt">test =</span> <span class="kw">select</span>(titanic_train, <span class="op">-</span>Survived),
                                                  <span class="dt">cl =</span> titanic_train<span class="op">$</span>Survived, <span class="dt">k =</span> .)),
                  <span class="dt">knn_test =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="kw">select</span>(titanic_train, <span class="op">-</span>Survived),
                                                 <span class="dt">test =</span> <span class="kw">select</span>(titanic_test, <span class="op">-</span>Survived),
                                                 <span class="dt">cl =</span> titanic_train<span class="op">$</span>Survived, <span class="dt">k =</span> .)),
                  <span class="dt">mse_train =</span> <span class="kw">map_dbl</span>(knn_train, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(titanic_test<span class="op">$</span>Survived <span class="op">!=</span><span class="st"> </span>.)),
                  <span class="dt">mse_test =</span> <span class="kw">map_dbl</span>(knn_test, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(titanic_test<span class="op">$</span>Survived <span class="op">!=</span><span class="st"> </span>.)))

<span class="kw">ggplot</span>(mse_knn, <span class="kw">aes</span>(k, mse_test)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> titanic_logit_mse, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;K&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Test error rate&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">expand_limits</span>(<span class="dt">y =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="/notes/nearest-neighbors_files/figure-html/titanic-knn-compare-1.png" width="672" /></p>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">5</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-01-22                  
## 
## â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##  package     * version date       lib source                              
##  assertthat    0.2.0   2017-04-11 [2] CRAN (R 3.5.0)                      
##  backports     1.1.3   2018-12-14 [2] CRAN (R 3.5.0)                      
##  bindr         0.1.1   2018-03-13 [2] CRAN (R 3.5.0)                      
##  bindrcpp      0.2.2   2018-03-29 [1] CRAN (R 3.5.0)                      
##  blogdown      0.9.4   2018-11-26 [1] Github (rstudio/blogdown@b2e1ed4)   
##  bookdown      0.9     2018-12-21 [1] CRAN (R 3.5.0)                      
##  broom       * 0.5.1   2018-12-05 [2] CRAN (R 3.5.0)                      
##  callr         3.1.1   2018-12-21 [2] CRAN (R 3.5.0)                      
##  cellranger    1.1.0   2016-07-27 [2] CRAN (R 3.5.0)                      
##  cli           1.0.1   2018-09-25 [1] CRAN (R 3.5.0)                      
##  colorspace    1.3-2   2016-12-14 [2] CRAN (R 3.5.0)                      
##  crayon        1.3.4   2017-09-16 [2] CRAN (R 3.5.0)                      
##  desc          1.2.0   2018-05-01 [2] CRAN (R 3.5.0)                      
##  devtools      2.0.1   2018-10-26 [1] CRAN (R 3.5.1)                      
##  digest        0.6.18  2018-10-10 [1] CRAN (R 3.5.0)                      
##  dplyr       * 0.7.8   2018-11-10 [1] CRAN (R 3.5.0)                      
##  evaluate      0.12    2018-10-09 [2] CRAN (R 3.5.0)                      
##  FNN         * 1.1.2.2 2018-12-10 [2] CRAN (R 3.5.0)                      
##  forcats     * 0.3.0   2018-02-19 [2] CRAN (R 3.5.0)                      
##  fs            1.2.6   2018-08-23 [1] CRAN (R 3.5.0)                      
##  generics      0.0.2   2018-11-29 [1] CRAN (R 3.5.0)                      
##  ggplot2     * 3.1.0   2018-10-25 [1] CRAN (R 3.5.0)                      
##  glue          1.3.0   2018-07-17 [2] CRAN (R 3.5.0)                      
##  gtable        0.2.0   2016-02-26 [2] CRAN (R 3.5.0)                      
##  haven         2.0.0   2018-11-22 [2] CRAN (R 3.5.0)                      
##  here        * 0.1     2017-05-28 [2] CRAN (R 3.5.0)                      
##  hms           0.4.2   2018-03-10 [2] CRAN (R 3.5.0)                      
##  htmltools     0.3.6   2017-04-28 [1] CRAN (R 3.5.0)                      
##  httr          1.4.0   2018-12-11 [2] CRAN (R 3.5.0)                      
##  igraph        1.2.2   2018-07-27 [2] CRAN (R 3.5.0)                      
##  jsonlite      1.6     2018-12-07 [2] CRAN (R 3.5.0)                      
##  kknn        * 1.3.1   2016-03-26 [2] CRAN (R 3.5.0)                      
##  knitr       * 1.21    2018-12-10 [2] CRAN (R 3.5.1)                      
##  lattice       0.20-38 2018-11-04 [2] CRAN (R 3.5.2)                      
##  lazyeval      0.2.1   2017-10-29 [2] CRAN (R 3.5.0)                      
##  lubridate     1.7.4   2018-04-11 [2] CRAN (R 3.5.0)                      
##  magrittr      1.5     2014-11-22 [2] CRAN (R 3.5.0)                      
##  Matrix        1.2-15  2018-11-01 [2] CRAN (R 3.5.2)                      
##  memoise       1.1.0   2017-04-21 [2] CRAN (R 3.5.0)                      
##  modelr        0.1.2   2018-05-11 [2] CRAN (R 3.5.0)                      
##  munsell       0.5.0   2018-06-12 [2] CRAN (R 3.5.0)                      
##  nlme          3.1-137 2018-04-07 [2] CRAN (R 3.5.2)                      
##  patchwork   * 0.0.1   2018-09-06 [1] Github (thomasp85/patchwork@7fb35b1)
##  pillar        1.3.1   2018-12-15 [2] CRAN (R 3.5.0)                      
##  pkgbuild      1.0.2   2018-10-16 [1] CRAN (R 3.5.0)                      
##  pkgconfig     2.0.2   2018-08-16 [2] CRAN (R 3.5.1)                      
##  pkgload       1.0.2   2018-10-29 [1] CRAN (R 3.5.0)                      
##  plyr          1.8.4   2016-06-08 [2] CRAN (R 3.5.0)                      
##  prettyunits   1.0.2   2015-07-13 [2] CRAN (R 3.5.0)                      
##  pROC        * 1.13.0  2018-09-24 [1] CRAN (R 3.5.0)                      
##  processx      3.2.1   2018-12-05 [2] CRAN (R 3.5.0)                      
##  ps            1.3.0   2018-12-21 [2] CRAN (R 3.5.0)                      
##  purrr       * 0.2.5   2018-05-29 [2] CRAN (R 3.5.0)                      
##  R6            2.3.0   2018-10-04 [1] CRAN (R 3.5.0)                      
##  rcfss       * 0.1.5   2018-05-30 [2] local                               
##  Rcpp          1.0.0   2018-11-07 [1] CRAN (R 3.5.0)                      
##  readr       * 1.3.1   2018-12-21 [2] CRAN (R 3.5.0)                      
##  readxl        1.2.0   2018-12-19 [2] CRAN (R 3.5.0)                      
##  remotes       2.0.2   2018-10-30 [1] CRAN (R 3.5.0)                      
##  rlang         0.3.0.1 2018-10-25 [1] CRAN (R 3.5.0)                      
##  rmarkdown     1.11    2018-12-08 [2] CRAN (R 3.5.0)                      
##  rprojroot     1.3-2   2018-01-03 [2] CRAN (R 3.5.0)                      
##  rsample     * 0.0.3   2018-11-20 [1] CRAN (R 3.5.0)                      
##  rstudioapi    0.8     2018-10-02 [1] CRAN (R 3.5.0)                      
##  rvest         0.3.2   2016-06-17 [2] CRAN (R 3.5.0)                      
##  scales        1.0.0   2018-08-09 [1] CRAN (R 3.5.0)                      
##  sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 3.5.0)                      
##  stringi       1.2.4   2018-07-20 [2] CRAN (R 3.5.0)                      
##  stringr     * 1.3.1   2018-05-10 [2] CRAN (R 3.5.0)                      
##  testthat      2.0.1   2018-10-13 [2] CRAN (R 3.5.0)                      
##  tibble      * 2.0.0   2019-01-04 [2] CRAN (R 3.5.2)                      
##  tidyr       * 0.8.2   2018-10-28 [2] CRAN (R 3.5.0)                      
##  tidyselect    0.2.5   2018-10-11 [1] CRAN (R 3.5.0)                      
##  tidyverse   * 1.2.1   2017-11-14 [2] CRAN (R 3.5.0)                      
##  titanic     * 0.1.0   2015-08-31 [2] CRAN (R 3.5.0)                      
##  usethis       1.4.0   2018-08-14 [1] CRAN (R 3.5.0)                      
##  withr         2.1.2   2018-03-15 [2] CRAN (R 3.5.0)                      
##  xfun          0.4     2018-10-23 [1] CRAN (R 3.5.0)                      
##  xml2          1.2.0   2018-01-24 [2] CRAN (R 3.5.0)                      
##  yaml          2.2.0   2018-07-25 [2] CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> References</h1>
<ul>
<li><span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span></li>
<li><span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span></li>
</ul>
<div id="refs" class="references">
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York, NY, USA: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
</div>
