---
title: Assessing model accuracy
date: 2019-01-09T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Selecting and fitting a model
    weight: 2
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#reducible-vs.irreducible-error"><span class="toc-section-number">1</span> Reducible vs. irreducible error</a></li>
<li><a href="#quality-of-fit"><span class="toc-section-number">2</span> Quality of fit</a><ul>
<li><a href="#training-vs.test-error"><span class="toc-section-number">2.1</span> Training vs. test error</a></li>
<li><a href="#optimism-of-training-error"><span class="toc-section-number">2.2</span> Optimism of training error</a></li>
</ul></li>
<li><a href="#bias-variance-trade-off"><span class="toc-section-number">3</span> Bias-variance trade-off</a><ul>
<li><a href="#bias-and-variance-defined"><span class="toc-section-number">3.1</span> Bias and variance defined</a></li>
<li><a href="#bias-variance-decomposition"><span class="toc-section-number">3.2</span> Bias-variance decomposition</a></li>
</ul></li>
<li><a href="#applications-to-classification-models"><span class="toc-section-number">4</span> Applications to classification models</a><ul>
<li><a href="#error-rate"><span class="toc-section-number">4.1</span> Error rate</a></li>
<li><a href="#cross-entropy"><span class="toc-section-number">4.2</span> Cross-entropy</a></li>
</ul></li>
<li><a href="#estimating-the-expected-test-error"><span class="toc-section-number">5</span> Estimating the expected test error</a><ul>
<li><a href="#c_p"><span class="toc-section-number">5.1</span> <span class="math inline">\(C_p\)</span></a></li>
<li><a href="#aic"><span class="toc-section-number">5.2</span> AIC</a></li>
<li><a href="#bic"><span class="toc-section-number">5.3</span> BIC</a></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">6</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">7</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(here)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<p><span class="math display">\[\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\loglik}{\text{logLik}}\]</span></p>
<div id="reducible-vs.irreducible-error" class="section level1">
<h1><span class="header-section-number">1</span> Reducible vs. irreducible error</h1>
<p>Consider a set of inouts <span class="math inline">\(X\)</span> and an outcome <span class="math inline">\(Y\)</span>. We can estimate a generic function</p>
<p><span class="math display">\[\hat{Y} = \hat{f}(X)\]</span></p>
<p>where <span class="math inline">\(\hat{f}\)</span> is our estimate for the true functional form <span class="math inline">\(f\)</span>, and <span class="math inline">\(\hat{Y}\)</span> is the resulting prediction for <span class="math inline">\(Y\)</span>. Different statistical learning algorithms will lead us to different estimates of <span class="math inline">\(\hat{f}\)</span> and therefore different <span class="math inline">\(\hat{Y}\)</span>. The accuracy of <span class="math inline">\(\hat{Y}\)</span> depends on two major components:</p>
<ol style="list-style-type: decimal">
<li>Reducible error</li>
<li>Irreducible error</li>
</ol>
<p><strong>Reducible error</strong> is error generated by using an inappropriate or suboptimal technique to estimate <span class="math inline">\(f\)</span>. If we improve on the technique, the reducible error can be reduced. Even if we perfectly estimated <span class="math inline">\(f\)</span> such that</p>
<p><span class="math display">\[\hat{Y} = f(X)\]</span></p>
<p>our predictions would still not guaranteed to be accurate. That is because the true data-generating process which created <span class="math inline">\(Y\)</span> is actually a function of <span class="math inline">\(\epsilon\)</span>,</p>
<p><span class="math display">\[Y = f(X) + \epsilon\]</span></p>
<p>Therefore variability associated with <span class="math inline">\(\epsilon\)</span> also effects the accuracy of our predictions. No matter how well we estimate <span class="math inline">\(f\)</span>, by definition we cannot estimate <span class="math inline">\(\epsilon\)</span>.</p>
<p><span class="math inline">\(\epsilon\)</span> could be driven by a number of factors, such as unmeasured variables useful in predicting <span class="math inline">\(Y\)</span>, or inherently unmeasurable variation in the subject or observation. Assuming <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(X\)</span> are fixed, we can show that</p>
<p><span class="math display">\[
\begin{align}
\E(Y - \bar{Y})^2 &amp;= \E[f(X) + \epsilon - \hat{f}(X)]^2 \\
&amp;= \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{\Var (\epsilon)}_{\text{Irreducible}}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\E(Y - \bar{Y})^2\)</span> is the expected value of the squared difference between the predicted and actual value of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\Var(\epsilon)\)</span> is the variance associated with the error term <span class="math inline">\(\epsilon\)</span>.</p>
<p>Irreducible error is potentially managed by ensuring we have a well-specified <a href="/notes/build-a-model">model</a> that captures all the necessary variables, but in the end there will always be an unexplainable component to our statistical learning model. Reducible error, on the other hand, is more easily managed by selecting an appropriate statistical learning method for a given problem.</p>
</div>
<div id="quality-of-fit" class="section level1">
<h1><span class="header-section-number">2</span> Quality of fit</h1>
<p>In order to evaluate how well a statistical learning method performs on a given data set, we need to define a measure for how well its predictions actually match the observed data. This is variously known as a <strong>loss</strong> or <strong>cost</strong> function <span class="math inline">\(L(Y, \hat{f}(X))\)</span>, where <span class="math inline">\(Y\)</span> is the outcome of interest, <span class="math inline">\(X\)</span> is the input(s), and a prediction model <span class="math inline">\(\hat{f}(X)\)</span> that has been estimated using a training set <span class="math inline">\(\tau\)</span>. These functions take the generic form</p>
<p><span class="math display">\[
L(Y, \hat{f}(X)) = \left\{
        \begin{array}{ll}
            (Y - \hat{f}(X))^2 &amp; \quad \text{squared error} \\
            \mid Y - \hat{f}(X) \mid &amp; \quad \text{absolute error}
        \end{array}
    \right.
\]</span></p>
<p>Different loss functions are appropriate for different kinds of outcomes of interest. For regression problems with a continuous outcome of interest, the most common loss function is the <strong>mean squared error</strong> (MSE), defined as</p>
<p><span class="math display">\[MSE = \frac{1}{N} \sum_{i = 1}^{N}{(y_i - \hat{f}(x_i))^2}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i =\)</span> the observed response value for the <span class="math inline">\(i\)</span>th observation</li>
<li><span class="math inline">\(\hat{f}(x_i) =\)</span> the predicted response value for the <span class="math inline">\(i\)</span>th observation given by <span class="math inline">\(\hat{f}\)</span></li>
<li><span class="math inline">\(N =\)</span> the total number of observations</li>
</ul>
<p>Our goal is to identify a model that generates the smallest possible MSE. The MSE is an absolute measure of fit because its value depends on the measurement units of the response variable <span class="math inline">\(Y\)</span>. It will be small if predicted values <span class="math inline">\(\hat{Y}\)</span> are close to the actual values <span class="math inline">\(Y\)</span>, and will be large if for some/all observations the predicted and actual values differ significantly.</p>
<blockquote>
<p><strong>Root mean squared error</strong> (RMSE) is the square root of MSE. It is also commonly seen in statistical packages as a model accuracy metric. You can use that as well, however we will use MSE because it is associated with <strong>variance</strong> (<span class="math inline">\(\sigma^2\)</span>), whereas RMSE is associated with <strong>standard deviation</strong> (<span class="math inline">\(\sigma\)</span>). Variance has certain statistical properties that are missing from standard deviation, so we will use MSE.</p>
</blockquote>
<div id="training-vs.test-error" class="section level2">
<h2><span class="header-section-number">2.1</span> Training vs. test error</h2>
<p>The equation above more formally defines the <strong>training error</strong></p>
<p><span class="math display">\[\overline{\text{Err}} = \frac{1}{N} \sum_{i = 1}^{N}{L(y_i, \hat{f}(x_i))}\]</span></p>
<p>which is the average loss over the training sample. However this is not actually the quantity we care about. We already know the values of <span class="math inline">\(Y\)</span> for the training data. Instead, we want to use the model to predict or explain the outcome of interest for an independent <strong>test set</strong> of data. So in truth, we want to know the <strong>test error</strong>, or the error/loss for an independent test set:</p>
<p><span class="math display">\[\text{Err}_\tau = \E[L(Y, \hat{f}(X)) | \tau]\]</span></p>
<p>Since we often do not have a test set (or could generate multiple potential test sets), we can instead frame this as the expected test error</p>
<p><span class="math display">\[\text{Err} = \E[L(Y, \hat{f}(X))] = \E[\text{Err}_\tau]\]</span></p>
<p>The expectation averages over everything that is random, including the randomness in the training set that produced <span class="math inline">\(\hat{f}\)</span>. Our goal is to estimate <span class="math inline">\(\text{Err}_\tau\)</span>, though in practice we tend to estimate the expected error <span class="math inline">\(\text{Err}\)</span> instead.</p>
</div>
<div id="optimism-of-training-error" class="section level2">
<h2><span class="header-section-number">2.2</span> Optimism of training error</h2>
<p>It would be nice if we could use the training error as an estimate for the test/expected error. Alas, we cannot. Most statistical learning methods specifically attempt to minimize the training error (think least squares regression). This produces a relatively low training error, but does not guarantee a correspondingly low test error. In fact, it can have the opposite relationship – as the training error decreases, the test error can actually increase.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate data from ISL figure 2.9</span>
sim_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> <span class="kw">runif</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">100</span>),
  <span class="dt">y =</span> <span class="fl">5.055901</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1848551</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.00748706</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.00005543478</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dt">sd =</span> <span class="fl">0.6</span>)
)

<span class="co"># model fit</span>
<span class="kw">ggplot</span>(sim_mse, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x) <span class="fl">5.055901</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1848551</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.00748706</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.00005543478</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;lm&quot;</span>), <span class="dt">method =</span> lm, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;spline-low&quot;</span>), <span class="dt">method =</span> lm,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>splines<span class="op">::</span><span class="kw">ns</span>(x, <span class="dv">5</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;spline-high&quot;</span>), <span class="dt">method =</span> lm,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>splines<span class="op">::</span><span class="kw">ns</span>(x, <span class="dv">20</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Training data points&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Models estimated on training set&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/model-accuracy_files/figure-html/sim-train-model-1.png" width="672" /></p>
<p>Here I synthesize an example dataset and fit three separate models to it: a linear regression fit, and two <strong>splines</strong> which permit more flexible relationships between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Notice that as the flexibility increases, the models are more likely to fit the data. That is, the training MSE will decrease. But this does not guarantee the test MSE also decreases. Consider the trained models applied to an independent test set generated from the same underlying process:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_mse_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> <span class="kw">runif</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">100</span>),
  <span class="dt">y =</span> <span class="fl">5.055901</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1848551</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.00748706</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.00005543478</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dt">sd =</span> <span class="fl">0.6</span>)
)

<span class="co"># model fit</span>
<span class="kw">ggplot</span>(sim_mse, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> sim_mse_test) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> <span class="cf">function</span>(x) <span class="fl">5.055901</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1848551</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.00748706</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.00005543478</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;lm&quot;</span>), <span class="dt">method =</span> lm, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;spline-low&quot;</span>), <span class="dt">method =</span> lm,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>splines<span class="op">::</span><span class="kw">ns</span>(x, <span class="dv">5</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;spline-high&quot;</span>), <span class="dt">method =</span> lm,
              <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>splines<span class="op">::</span><span class="kw">ns</span>(x, <span class="dv">20</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Test data points&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Models estimated on training set&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="/notes/model-accuracy_files/figure-html/sim-test-model-1.png" width="672" /></p>
<p>It is not entirely clear, but these models no longer perform as well as compared to the training data. This divergence increases as the flexibility of the model increases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_mse_test &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> <span class="kw">runif</span>(<span class="dt">n =</span> <span class="fl">1e04</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">100</span>),
  <span class="dt">y =</span> <span class="fl">5.055901</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1848551</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.00748706</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.00005543478</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e04</span>, <span class="dt">sd =</span> <span class="fl">0.5</span>)
)

<span class="co"># train vs. test MSE</span>
train_test_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">df =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">30</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(df, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>splines<span class="op">::</span><span class="kw">ns</span>(x, .x), <span class="dt">data =</span> sim_mse)),
         <span class="dt">pred =</span> <span class="kw">map</span>(model, augment),
         <span class="dt">mse_train =</span> <span class="kw">map_dbl</span>(pred, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>(.<span class="op">$</span>.resid<span class="op">^</span><span class="dv">2</span>)),
         <span class="dt">pred_test =</span> <span class="kw">map</span>(model, augment, <span class="dt">newdata =</span> sim_mse_test),
         <span class="dt">mse_test =</span> <span class="kw">map_dbl</span>(pred_test, <span class="op">~</span><span class="st"> </span><span class="kw">mean</span>((.<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>.<span class="op">$</span>.fitted)<span class="op">^</span><span class="dv">2</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(mse, value, mse_train, mse_test) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mse =</span> <span class="kw">str_remove</span>(mse, <span class="st">&quot;mse_&quot;</span>),
         <span class="dt">mse =</span> <span class="kw">str_to_title</span>(mse))

train_test_mse <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(df, value, <span class="dt">color =</span> mse)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">type =</span> <span class="st">&quot;qual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_log10</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Flexibility&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean squared error&quot;</span>,
       <span class="dt">color =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/notes/model-accuracy_files/figure-html/sim-test-mse-1.png" width="672" /></p>
<p>This discrepancy holds true for any data set and any statistical learning method. As the model flexibility increases, training MSE will decrease but the test MSE may not. When a given method generates a small training MSE and a large test MSE, we are said to be <strong>overfitting</strong> the data. The model is being tuned too closely to the training set of observations, and is no longer <strong>generalizable</strong>. It is detecting artifacts and random patterns specific to the single set of observations but which don’t really exist in the data-generating process.</p>
</div>
</div>
<div id="bias-variance-trade-off" class="section level1">
<h1><span class="header-section-number">3</span> Bias-variance trade-off</h1>
<div id="bias-and-variance-defined" class="section level2">
<h2><span class="header-section-number">3.1</span> Bias and variance defined</h2>
<p><strong>Bias</strong> defines the error that is introduced by approximating a real-life problem using a simplified model. In the context of statistical learning, this is the amount by which the average of our estimate differs from the true mean:</p>
<p><span class="math display">\[\text{Bias} = \E[\hat{f}(x_o)] - f(x_0)\]</span></p>
<p>Generally speaking, we want to have little-to-no bias in our estimates; otherwise we are consistently estimating an incorrect value for <span class="math inline">\(\hat{f}(x_0)\)</span>.</p>
<p><strong>Variance</strong> refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. Formally, it is the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean:</p>
<p><span class="math display">\[\text{Variance} = \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2\]</span></p>
</div>
<div id="bias-variance-decomposition" class="section level2">
<h2><span class="header-section-number">3.2</span> Bias-variance decomposition</h2>
<p>If we assume that <span class="math inline">\(Y = f(X) + \epsilon\)</span> where <span class="math inline">\(\E[\epsilon] = 0\)</span> and <span class="math inline">\(\Var(\epsilon) = \sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point <span class="math inline">\(X = x_0\)</span> using squared-error loss:</p>
<p><span class="math display">\[
\begin{align}
\text{Err}(x_0) &amp;= \E[(Y - \hat{f}(x_0))^2 | X = x_0] \\
&amp;= \sigma^2_\epsilon + [\E[\hat{f}(x_o)] - f(x_0)]^2 + \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2 \\
&amp;= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_o)) + \Var(\hat{f}(x_0)) \\
&amp;= \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}
\end{align}
\]</span></p>
<p>In order to minimize the expected test error rate, we need a statistical learning method that has both <strong>low bias</strong> and <strong>low variance</strong>. The difficulty is that few, if any, methods satisfy both requirements. As flexibility increases, the bias of a model will decrease but the variance will increase. Consider our sample data from earlier:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sim_mse, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Training data points&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y))</code></pre></div>
<p><img src="/notes/model-accuracy_files/figure-html/sim-train-data-1.png" width="672" /></p>
<p>A low bias, high variance modeling strategy would be to draw a curve that passes through every data point:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate KNN model, k = 1</span>
sim_knn1 &lt;-<span class="st"> </span>FNN<span class="op">::</span><span class="kw">knn.reg</span>(sim_mse,
                         <span class="dt">y =</span> sim_mse<span class="op">$</span>y,
                         <span class="dt">test =</span> sim_mse,
                         <span class="dt">k =</span> <span class="dv">1</span>)

sim_mse <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> sim_knn1<span class="op">$</span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Training data points&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y))</code></pre></div>
<p><img src="/notes/model-accuracy_files/figure-html/sim-data-nn-1.png" width="672" /></p>
<p>It has low bias because the training MSE is 0, but it has high variance because if we generate a new training set the model will be entirely different. Again, this is related to the problem of overfitting our training data set.</p>
<p>Compare this to a method with high bias but low variance: a horizontal line at the mean:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sim_mse, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="kw">mean</span>(sim_mse<span class="op">$</span>y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Training data points&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(Y))</code></pre></div>
<p><img src="/notes/model-accuracy_files/figure-html/sim-data-mean-1.png" width="672" /></p>
<p>The variance of our model <span class="math inline">\(\hat{f}\)</span> should be low since the mean value for any given independent sample from the population should possess the same basic characteristics. But this model has a lot more bias (aka a higher MSE) because its accuracy is not good.</p>
<p>How do we actually account for this trade-off? That is to say, which should be most concerned with if we cannot reduce both components? Bias or variance? That entirely depends on our goals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set number of throws</span>
numDarts &lt;-<span class="st"> </span><span class="dv">10</span>

<span class="co"># throw numDarts number of darts and get the coordinates where they hit</span>
throwDarts &lt;-<span class="st"> </span><span class="cf">function</span>(numDarts, <span class="dt">reliable =</span> <span class="ot">TRUE</span>, <span class="dt">valid =</span> <span class="ot">TRUE</span>) {
  <span class="cf">if</span> (reliable <span class="op">&amp;</span><span class="st"> </span>valid) {
    xvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> .<span class="dv">05</span>)
    yvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> .<span class="dv">05</span>)
  } <span class="cf">else</span> <span class="cf">if</span> (reliable <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span> <span class="op">&amp;</span><span class="st"> </span>valid <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {
    xvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> .<span class="dv">5</span>, <span class="dt">sd =</span> .<span class="dv">05</span>)
    yvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> .<span class="dv">4</span>, <span class="dt">sd =</span> .<span class="dv">05</span>)
  } <span class="cf">else</span> <span class="cf">if</span> (reliable <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span> <span class="op">&amp;</span><span class="st"> </span>valid <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {
    xvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> .<span class="dv">3</span>)
    yvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> .<span class="dv">3</span>)
  } <span class="cf">else</span> <span class="cf">if</span> (reliable <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span> <span class="op">&amp;</span><span class="st"> </span>valid <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) {
    xvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> .<span class="dv">5</span>, <span class="dt">sd =</span> .<span class="dv">3</span>)
    yvals &lt;-<span class="st"> </span><span class="kw">rnorm</span>(numDarts, <span class="dt">mean =</span> <span class="op">-</span>.<span class="dv">4</span>, <span class="dt">sd =</span> .<span class="dv">3</span>)
  }
  
  <span class="kw">tibble</span>(
    <span class="dt">x =</span> xvals,
    <span class="dt">y =</span> yvals,
    <span class="dt">reliable =</span> reliable,
    <span class="dt">valid =</span> valid
  )
}

<span class="co"># get data for each situation</span>
throws &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(
  <span class="kw">throwDarts</span>(numDarts, <span class="dt">reliable =</span> <span class="ot">TRUE</span>, <span class="dt">valid =</span> <span class="ot">TRUE</span>),
  <span class="kw">throwDarts</span>(numDarts, <span class="dt">reliable =</span> <span class="ot">TRUE</span>, <span class="dt">valid =</span> <span class="ot">FALSE</span>),
  <span class="kw">throwDarts</span>(numDarts, <span class="dt">reliable =</span> <span class="ot">FALSE</span>, <span class="dt">valid =</span> <span class="ot">TRUE</span>),
  <span class="kw">throwDarts</span>(numDarts, <span class="dt">reliable =</span> <span class="ot">FALSE</span>, <span class="dt">valid =</span> <span class="ot">FALSE</span>)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">reliable =</span> <span class="kw">ifelse</span>(reliable, <span class="st">&quot;Low Variance&quot;</span>, <span class="st">&quot;High Variance&quot;</span>),
    <span class="dt">valid =</span> <span class="kw">ifelse</span>(valid, <span class="st">&quot;Low Bias&quot;</span>, <span class="st">&quot;High Bias&quot;</span>)
  )

<span class="co"># plot the dart board, facet by each type</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> throws, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(reliable <span class="op">~</span><span class="st"> </span>valid) <span class="op">+</span>
<span class="st">  </span>ggforce<span class="op">::</span><span class="kw">geom_circle</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="ot">NULL</span>, <span class="dt">x0 =</span> <span class="dv">0</span>, <span class="dt">y0 =</span> <span class="dv">0</span>, <span class="dt">r =</span> <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlim</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_fixed</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="ot">NULL</span>,
       <span class="dt">x =</span> <span class="ot">NULL</span>,
       <span class="dt">y =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;point&quot;</span>, <span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">size =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="/notes/model-accuracy_files/figure-html/darts-1.png" width="672" /></p>
<p>Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. Imagine we can repeat our entire model building process to get a number of separate hits on the target. Each hit represents an individual realization of our model, given the chance variability in the training data we gather. Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions. These different realizations result in a scatter of hits on the target.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Traditionally in statistics we are concerned with reducing the bias of our estimates. Consider one of the desirable properties of the ordinary least squares estimator: it is the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">best linear unbiased estimator (BLUE)</a>. However linear regression models could be estimated with alternative estimators, such as ridge regression (see chapter 6 in [ISL]). These methods are not unbiased, but have lower variance. What is the value to such an estimator? One desirable quality is simplicity. As we will learn, ridge regression artificially biases the parameters in the regression model towards zero. This enables us to do things we could otherwise not do, such as estimate a regression model where the number of predictors is greater than the number of observations<span class="math inline">\(p &gt; n\)</span>.</p>
<p>What if our ultimate goal is prediction? Then our primary concern is reducing the test error. Increasing bias slightly to more dramatically decrease variance could lead to a lower test error rate. In that situation, it could make sense to intentionally increase our bias.</p>
</div>
</div>
<div id="applications-to-classification-models" class="section level1">
<h1><span class="header-section-number">4</span> Applications to classification models</h1>
<p>The same basic principles apply to classification problems. The major difference is how we specify the loss function for a qualitative outcome.</p>
<div id="error-rate" class="section level2">
<h2><span class="header-section-number">4.1</span> Error rate</h2>
<p>The most common approach is the <strong>error rate</strong>, or the proportion of mistakes that are made if we apply our estimate <span class="math inline">\(\hat{f}\)</span> to the observations:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{n = 1}^{n} I(y_i \neq \hat{y}_i)\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the predicted classification label for the <span class="math inline">\(i\)</span>th observation using some estimated function <span class="math inline">\(\hat{f}\)</span>, and <span class="math inline">\(I(y_i \neq \hat{y}_i)\)</span> is an <strong>indicator</strong> function that equals 1 if <span class="math inline">\(y_i \neq \hat{y}_i\)</span> and 0 if <span class="math inline">\(y_i = \hat{y}_i\)</span> (i.e. if the observation was correctly classified).</p>
<p>The equation above refers to the <strong>training error rate</strong> because, as with the training MSE, it is computed using the training data set. The <strong>test error rate</strong> associated with a test set of observations of the form <span class="math inline">\((x_0, y_0)\)</span> is given by</p>
<p><span class="math display">\[\overline{I(y_0 \neq \hat{y}_0)}\]</span></p>
<p>where <span class="math inline">\(\hat{y}_0\)</span> is the predicted class label that results from applying the classifier to the test observation with predictor <span class="math inline">\(x_0\)</span>.</p>
</div>
<div id="cross-entropy" class="section level2">
<h2><span class="header-section-number">4.2</span> Cross-entropy</h2>
<p>Another commonly employed loss function for deep learning models is <strong>cross-entropy</strong>, or <strong>log loss</strong>. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of <span class="math inline">\(0.75\)</span> when the actual observation label is 1 is not as bad as predicting a probability of <span class="math inline">\(0.1\)</span> when the actual observation label is 1. A perfect model would have a log loss of 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(
  <span class="dt">prob =</span> <span class="kw">seq</span>(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">0.001</span>),
  <span class="dt">log_loss =</span> <span class="op">-</span><span class="kw">log</span>(prob)
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(prob, log_loss)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Log loss when true label = 1&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Predicted probability&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Log loss&quot;</span>)</code></pre></div>
<p><img src="/notes/model-accuracy_files/figure-html/cross-entropy-1.png" width="672" /></p>
<p>Log loss penalizes both false-positives and false-negatives, but more strongly penalizes predictions that are confident and wrong. In binary classification where the number of classes <span class="math inline">\(M = 2\)</span>, cross-entropy can be calculated as:</p>
<p><span class="math display">\[-(y \log(p) + (1 - y) \log(1 - p))\]</span></p>
<p>If <span class="math inline">\(M &gt; 2\)</span>, we calculate a separate loss for each class label per observation and sum the result:</p>
<p><span class="math display">\[- \sum_{c=1}^M y_{o,c} \log(p_{o,c})\]</span></p>
</div>
</div>
<div id="estimating-the-expected-test-error" class="section level1">
<h1><span class="header-section-number">5</span> Estimating the expected test error</h1>
<p>How can we estimate the expected test error rate when all we are given is a single data set? If our data set is sufficiently large, we randomly divide it into three parts:<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<ol style="list-style-type: decimal">
<li>Training set - used to fit the models</li>
<li>Validation set - used to estimate prediction error for model selection</li>
<li>Test set - used for assessment of the generalization error of the final chosen model</li>
</ol>
<p>A general rule of thumb suggests 50% allocated for training, and 25% each for validation and testing.</p>
<p>Commonly though, we do not have enough observations to allocate for each of these purposes. Remember that the fewer observations allocated to the training set, the more noisy the estimated model will be. Computational approaches such as cross-validation and bootstrapping allow us to efficiently use (and re-use) observations to provide reasonable samples for training, validating, and testing. We will cover those separately. Today, we consider a few methods which attempt to use the entire data set for training but still approximate the validation step analytically. These were used historically when computational power was not as prevalent. The examples focus on regression models, but these methods can be extended to classification models as well.</p>
<div id="c_p" class="section level2">
<h2><span class="header-section-number">5.1</span> <span class="math inline">\(C_p\)</span></h2>
<p>The general form of the training set (in-sample) estimates for the error are</p>
<p><span class="math display">\[\widehat{\text{Err}_{in}} = \overline{\text{Err}} + \hat{\omega}\]</span></p>
<p>where <span class="math inline">\(\hat{\omega}\)</span> is an estimate of the average optimism (or bias) of the training error.</p>
<p><strong>Mallow’s <span class="math inline">\(C_p\)</span></strong> estimates the test error as:</p>
<p><span class="math display">\[C_p = \overline{\text{Err}} + 2 \times \frac{d}{N} \hat{\sigma}_\epsilon^2\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(d\)</span> - number of parameters</li>
<li><span class="math inline">\(\hat{\sigma}^2\)</span> - estimate of the variance of the error <span class="math inline">\(\epsilon\)</span> associated with each response measurement</li>
</ul>
<p>In essence, <span class="math inline">\(C_p\)</span> uses the training error as the starting point and adds a penalty of <span class="math inline">\(2 \times \frac{d}{N} \hat{\sigma}_\epsilon^2\)</span> to adjust for the bias in the training error. This approach is reasonable and actually is itself an unbiased estimator of the test error as long as <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>. The smaller the <span class="math inline">\(C_p\)</span> statistic, the lower the test error. This statistic is only appropriate for models fit under squared error loss (such as linear regression).</p>
</div>
<div id="aic" class="section level2">
<h2><span class="header-section-number">5.2</span> AIC</h2>
<p>The <strong>Akaike information criterion</strong> is similar to <span class="math inline">\(C_p\)</span> but generalizes to any situation where a log-likelihood loss function is used (e.g. maximum likelihood estimation). As <span class="math inline">\(N \leadsto \infty\)</span>,</p>
<p><span class="math display">\[-2 \times \E[\log (\Pr_{\hat{\Theta}}(Y)] \approx - \frac{2}{N} \times \E[\loglik] + 2 \times \frac{d}{N}\]</span></p>
<p>where <span class="math inline">\(\Pr_{\Theta}(Y)\)</span> is a probability density function (PDF) for <span class="math inline">\(Y\)</span>, <span class="math inline">\(\hat{\Theta}\)</span> is the maximum-likelihood estimate of <span class="math inline">\(\Theta\)</span>, and <span class="math inline">\(\loglik\)</span> is the maximized log-likelihood function. For a logistic regression model using the binomial log-likelihood, we have</p>
<p><span class="math display">\[AIC = -\frac{2}{N} \times \loglik + 2 \times \frac{d}{N}\]</span></p>
<p>For the Gaussian model (i.e. linear regression) with variance <span class="math inline">\(\sigma_\epsilon^2 = \hat{\sigma}_\epsilon^2\)</span> assumed known,</p>
<p><span class="math display">\[-2 \times \loglik = \frac{\sum_{i=1}^N (y_i - \hat{f}(x_i))^2}{\sigma_\epsilon^2}\]</span></p>
<p>which is <span class="math inline">\(N \times \overline{\text{Err}} / \sigma_\epsilon^2\)</span> for squared error loss. Plugging this into our AIC equation leads us right back to</p>
<p><span class="math display">\[AIC = \overline{\text{Err}} + 2 \times \frac{d}{N} \hat{\sigma}_\epsilon^2\]</span></p>
<p>To use AIC for model selection, we choose the model giving the smallest AIC over the set of models considered.</p>
</div>
<div id="bic" class="section level2">
<h2><span class="header-section-number">5.3</span> BIC</h2>
<p>The <strong>Bayesian information criterion</strong> is similar to the AIC, but motivated quite differently.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Like the AIC, it is applicable in settings where the fitting is carried out by maximization of a log-likelihood. The generic form is</p>
<p><span class="math display">\[BIC = -2 \times \loglik + \log(N) \times d\]</span></p>
<p>As with AIC, under the Gaussian model we can write this as</p>
<p><span class="math display">\[BIC = \frac{N}{\sigma_\epsilon^2} \left[ \overline{\text{Err}} + \log (N) \times \frac{d}{N} \sigma_\epsilon^2 \right]\]</span></p>
<p>Therefore BIC is proportional to AIC with the factor <span class="math inline">\(2\)</span> replaced by <span class="math inline">\(\log (N)\)</span>. The major difference is that BIC tends to penalize complex models more heavily, giving preference to simpler models in selection.</p>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">6</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-01-09                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package     * version date       lib source                           
##  assertthat    0.2.0   2017-04-11 [2] CRAN (R 3.5.0)                   
##  backports     1.1.3   2018-12-14 [2] CRAN (R 3.5.0)                   
##  bindr         0.1.1   2018-03-13 [2] CRAN (R 3.5.0)                   
##  bindrcpp      0.2.2   2018-03-29 [1] CRAN (R 3.5.0)                   
##  blogdown      0.9.4   2018-11-26 [1] Github (rstudio/blogdown@b2e1ed4)
##  bookdown      0.9     2018-12-21 [1] CRAN (R 3.5.0)                   
##  broom       * 0.5.1   2018-12-05 [2] CRAN (R 3.5.0)                   
##  callr         3.1.1   2018-12-21 [2] CRAN (R 3.5.0)                   
##  cellranger    1.1.0   2016-07-27 [2] CRAN (R 3.5.0)                   
##  cli           1.0.1   2018-09-25 [1] CRAN (R 3.5.0)                   
##  colorspace    1.3-2   2016-12-14 [2] CRAN (R 3.5.0)                   
##  crayon        1.3.4   2017-09-16 [2] CRAN (R 3.5.0)                   
##  desc          1.2.0   2018-05-01 [2] CRAN (R 3.5.0)                   
##  devtools      2.0.1   2018-10-26 [1] CRAN (R 3.5.1)                   
##  digest        0.6.18  2018-10-10 [1] CRAN (R 3.5.0)                   
##  dplyr       * 0.7.8   2018-11-10 [1] CRAN (R 3.5.0)                   
##  evaluate      0.12    2018-10-09 [2] CRAN (R 3.5.0)                   
##  forcats     * 0.3.0   2018-02-19 [2] CRAN (R 3.5.0)                   
##  fs            1.2.6   2018-08-23 [1] CRAN (R 3.5.0)                   
##  generics      0.0.2   2018-11-29 [1] CRAN (R 3.5.0)                   
##  ggplot2     * 3.1.0   2018-10-25 [1] CRAN (R 3.5.0)                   
##  glue          1.3.0   2018-07-17 [2] CRAN (R 3.5.0)                   
##  gtable        0.2.0   2016-02-26 [2] CRAN (R 3.5.0)                   
##  haven         2.0.0   2018-11-22 [2] CRAN (R 3.5.0)                   
##  here        * 0.1     2017-05-28 [2] CRAN (R 3.5.0)                   
##  hms           0.4.2   2018-03-10 [2] CRAN (R 3.5.0)                   
##  htmltools     0.3.6   2017-04-28 [1] CRAN (R 3.5.0)                   
##  httr          1.4.0   2018-12-11 [2] CRAN (R 3.5.0)                   
##  jsonlite      1.6     2018-12-07 [2] CRAN (R 3.5.0)                   
##  knitr         1.21    2018-12-10 [2] CRAN (R 3.5.1)                   
##  lattice       0.20-38 2018-11-04 [2] CRAN (R 3.5.2)                   
##  lazyeval      0.2.1   2017-10-29 [2] CRAN (R 3.5.0)                   
##  lubridate     1.7.4   2018-04-11 [2] CRAN (R 3.5.0)                   
##  magrittr      1.5     2014-11-22 [2] CRAN (R 3.5.0)                   
##  memoise       1.1.0   2017-04-21 [2] CRAN (R 3.5.0)                   
##  modelr        0.1.2   2018-05-11 [2] CRAN (R 3.5.0)                   
##  munsell       0.5.0   2018-06-12 [2] CRAN (R 3.5.0)                   
##  nlme          3.1-137 2018-04-07 [2] CRAN (R 3.5.2)                   
##  pillar        1.3.1   2018-12-15 [2] CRAN (R 3.5.0)                   
##  pkgbuild      1.0.2   2018-10-16 [1] CRAN (R 3.5.0)                   
##  pkgconfig     2.0.2   2018-08-16 [2] CRAN (R 3.5.1)                   
##  pkgload       1.0.2   2018-10-29 [1] CRAN (R 3.5.0)                   
##  plyr          1.8.4   2016-06-08 [2] CRAN (R 3.5.0)                   
##  prettyunits   1.0.2   2015-07-13 [2] CRAN (R 3.5.0)                   
##  processx      3.2.1   2018-12-05 [2] CRAN (R 3.5.0)                   
##  ps            1.3.0   2018-12-21 [2] CRAN (R 3.5.0)                   
##  purrr       * 0.2.5   2018-05-29 [2] CRAN (R 3.5.0)                   
##  R6            2.3.0   2018-10-04 [1] CRAN (R 3.5.0)                   
##  Rcpp          1.0.0   2018-11-07 [1] CRAN (R 3.5.0)                   
##  readr       * 1.3.1   2018-12-21 [2] CRAN (R 3.5.0)                   
##  readxl        1.2.0   2018-12-19 [2] CRAN (R 3.5.0)                   
##  remotes       2.0.2   2018-10-30 [1] CRAN (R 3.5.0)                   
##  rlang         0.3.0.1 2018-10-25 [1] CRAN (R 3.5.0)                   
##  rmarkdown     1.11    2018-12-08 [2] CRAN (R 3.5.0)                   
##  rprojroot     1.3-2   2018-01-03 [2] CRAN (R 3.5.0)                   
##  rstudioapi    0.8     2018-10-02 [1] CRAN (R 3.5.0)                   
##  rvest         0.3.2   2016-06-17 [2] CRAN (R 3.5.0)                   
##  scales        1.0.0   2018-08-09 [1] CRAN (R 3.5.0)                   
##  sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 3.5.0)                   
##  stringi       1.2.4   2018-07-20 [2] CRAN (R 3.5.0)                   
##  stringr     * 1.3.1   2018-05-10 [2] CRAN (R 3.5.0)                   
##  testthat      2.0.1   2018-10-13 [2] CRAN (R 3.5.0)                   
##  tibble      * 2.0.0   2019-01-04 [2] CRAN (R 3.5.2)                   
##  tidyr       * 0.8.2   2018-10-28 [2] CRAN (R 3.5.0)                   
##  tidyselect    0.2.5   2018-10-11 [1] CRAN (R 3.5.0)                   
##  tidyverse   * 1.2.1   2017-11-14 [2] CRAN (R 3.5.0)                   
##  usethis       1.4.0   2018-08-14 [1] CRAN (R 3.5.0)                   
##  withr         2.1.2   2018-03-15 [2] CRAN (R 3.5.0)                   
##  xfun          0.4     2018-10-23 [1] CRAN (R 3.5.0)                   
##  xml2          1.2.0   2018-01-24 [2] CRAN (R 3.5.0)                   
##  yaml          2.2.0   2018-07-25 [2] CRAN (R 3.5.0)                   
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">7</span> References</h1>
<ul>
<li>This page is derived in part from <a href="http://varianceexplained.org/files/loess.html">“Creating a LOESS animation with <code>gganimate</code>”</a> by David Robinson.</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Explanation drawn from <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Understanding the Bias-Variance Tradeoff</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Assuming we have a standard cross-sectional data set.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>See ESL ch 7.7 for more details.<a href="#fnref3">↩</a></p></li>
</ol>
</div>
