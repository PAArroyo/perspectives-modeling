---
title: Assessing model accuracy
date: 2019-01-09T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Selecting and fitting a model
    weight: 2
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(broom)
library(here)
theme_set(theme_minimal())
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}}$$

# Reducible vs. irreducible error

Consider a set of inouts $X$ and an outcome $Y$. We can estimate a generic function

$$\hat{Y} = \hat{f}(X)$$

where $\hat{f}$ is our estimate for the true functional form $f$, and $\hat{Y}$ is the resulting prediction for $Y$. Different statistical learning algorithms will lead us to different estimates of $\hat{f}$ and therefore different $\hat{Y}$. The accuracy of $\hat{Y}$ depends on two major components:

1. Reducible error
1. Irreducible error

**Reducible error** is error generated by using an inappropriate or suboptimal technique to estimate $f$. If we improve on the technique, the reducible error can be reduced. Even if we perfectly estimated $f$ such that

$$\hat{Y} = f(X)$$

our predictions would still not guaranteed to be accurate. That is because the true data-generating process which created $Y$ is actually a function of $\epsilon$,

$$Y = f(X) + \epsilon$$

Therefore variability associated with $\epsilon$ also effects the accuracy of our predictions. No matter how well we estimate $f$, by definition we cannot estimate $\epsilon$.

$\epsilon$ could be driven by a number of factors, such as unmeasured variables useful in predicting $Y$, or inherently unmeasurable variation in the subject or observation. Assuming $\hat{f}$ and $X$ are fixed, we can show that

$$
\begin{align}
\E(Y - \bar{Y})^2 &= \E[f(X) + \epsilon - \hat{f}(X)]^2 \\
&= \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{\Var (\epsilon)}_{\text{Irreducible}}
\end{align}
$$

where $\E(Y - \bar{Y})^2$ is the expected value of the squared difference between the predicted and actual value of $Y$ and $\Var(\epsilon)$ is the variance associated with the error term $\epsilon$.

Irreducible error is potentially managed by ensuring we have a well-specified [model](/notes/build-a-model) that captures all the necessary variables, but in the end there will always be an unexplainable component to our statistical learning model. Reducible error, on the other hand, is more easily managed by selecting an appropriate statistical learning method for a given problem.

# Quality of fit

In order to evaluate how well a statistical learning method performs on a given data set, we need to define a measure for how well its predictions actually match the observed data. This is variously known as a **loss** or **cost** function $L(Y, \hat{f}(X))$, where $Y$ is the outcome of interest, $X$ is the input(s), and a prediction model $\hat{f}(X)$ that has been estimated using a training set $\tau$. These functions take the generic form

$$
L(Y, \hat{f}(X)) = \left\{
        \begin{array}{ll}
            (Y - \hat{f}(X))^2 & \quad \text{squared error} \\
            \mid Y - \hat{f}(X) \mid & \quad \text{absolute error}
        \end{array}
    \right.
$$

Different loss functions are appropriate for different kinds of outcomes of interest. For regression problems with a continuous outcome of interest, the most common loss function is the **mean squared error** (MSE), defined as

$$MSE = \frac{1}{N} \sum_{i = 1}^{N}{(y_i - \hat{f}(x_i))^2}$$

where:

* $y_i =$ the observed response value for the $i$th observation
* $\hat{f}(x_i) =$ the predicted response value for the $i$th observation given by $\hat{f}$
* $N =$ the total number of observations

Our goal is to identify a model that generates the smallest possible MSE. The MSE is an absolute measure of fit because its value depends on the measurement units of the response variable $Y$. It will be small if predicted values $\hat{Y}$ are close to the actual values $Y$, and will be large if for some/all observations the predicted and actual values differ significantly.

> **Root mean squared error** (RMSE) is the square root of MSE. It is also commonly seen in statistical packages as a model accuracy metric. You can use that as well, however we will use MSE because it is associated with **variance** ($\sigma^2$), whereas RMSE is associated with **standard deviation** ($\sigma$). Variance has certain statistical properties that are missing from standard deviation, so we will use MSE.

## Training vs. test error

The equation above more formally defines the **training error**

$$\overline{\text{Err}} = \frac{1}{N} \sum_{i = 1}^{N}{L(y_i, \hat{f}(x_i))}$$

which is the average loss over the training sample. However this is not actually the quantity we care about. We already know the values of $Y$ for the training data. Instead, we want to use the model to predict or explain the outcome of interest for an independent **test set** of data. So in truth, we want to know the **test error**, or the error/loss for an independent test set:

$$\text{Err}_\tau = \E[L(Y, \hat{f}(X)) | \tau]$$

Since we often do not have a test set (or could generate multiple potential test sets), we can instead frame this as the expected test error

$$\text{Err} = \E[L(Y, \hat{f}(X))] = \E[\text{Err}_\tau]$$

The expectation averages over everything that is random, including the randomness in the training set that produced $\hat{f}$. Our goal is to estimate $\text{Err}_\tau$, though in practice we tend to estimate the expected error $\text{Err}$ instead.

## Optimism of training error

It would be nice if we could use the training error as an estimate for the test/expected error. Alas, we cannot. Most statistical learning methods specifically attempt to minimize the training error (think least squares regression). This produces a relatively low training error, but does not guarantee a correspondingly low test error. In fact, it can have the opposite relationship -- as the training error decreases, the test error can actually increase.

```{r sim-train-model}
# simulate data from ISL figure 2.9
sim_mse <- tibble(
  x = runif(n = 1e02, min = 0, max = 100),
  y = 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3 + rnorm(1e02, sd = 0.5)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Training data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

Here I synthesize an example dataset and fit three separate models to it: a linear regression fit, and two **splines** which permit more flexible relationships between $X$ and $Y$. Notice that as the flexibility increases, the models are more likely to fit the data. That is, the training MSE will decrease. But this does not guarantee the test MSE also decreases. Consider the trained models applied to an independent test set generated from the same underlying process:

It is not entirely clear, but these models no longer perform as well as compared to the training data. This divergence increases as the flexibility of the model increases.

```{r sim-test-mse}
sim_mse_test <- tibble(
  x = runif(n = 1e04, min = 0, max = 100),
  y = 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3 + rnorm(1e04, sd = 0.5)
)

# train vs. test MSE
train_test_mse <- tibble(df = 2:30) %>%
  mutate(model = map(df, ~ lm(y ~ splines::ns(x, .x), data = sim_mse)),
         pred = map(model, augment),
         mse_train = map_dbl(pred, ~ mean(.$.resid^2)),
         pred_test = map(model, augment, newdata = sim_mse_test),
         mse_test = map_dbl(pred_test, ~ mean((.$y - .$.fitted)^2))) %>%
  gather(mse, value, mse_train, mse_test) %>%
  mutate(mse = str_remove(mse, "mse_"),
         mse = str_to_title(mse))

train_test_mse %>%
  ggplot(aes(df, value, color = mse)) +
  geom_smooth(se = FALSE) +
  scale_color_brewer(type = "qual") +
  scale_x_log10() +
  labs(x = "Flexibility",
       y = "Mean squared error",
       color = NULL) +
  theme(legend.position = "bottom")
```

This discrepancy holds true for any data set and any statistical learning method. As the model flexibility increases, training MSE will decrease but the test MSE may not. When a given method generates a small training MSE and a large test MSE, we are said to be **overfitting** the data. The model is being tuned too closely to the training set of observations, and is no longer **generalizable**. It is detecting artifacts and random patterns specific to the single set of observations but which don't really exist in the data-generating process.

# Bias-variance trade-off

## Bias-variance decomposition

**Bias** defines the error that is introduced by approximating a real-life problem using a simplified model. In the context of statistical learning, this is the amount by which the average of our estimate differs from the true mean:

$$\text{Bias} = \E[\hat{f}(x_o)] - f(x_0)$$

Generally speaking, we want to have little-to-no bias in our estimates; otherwise we are consistently estimating an incorrect value for $\hat{f}(x_0)$.

**Variance** refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Formally, it is the expected squared deviation of $\hat{f}(x_0)$ around its mean:

$$\text{Variance} = \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2$$

If we assume that $Y = f(X) + \epsilon$ where $\E[\epsilon] = 0$ and $\Var(\epsilon) = \sigma^2_\epsilon$, we can derive an expression for the expected prediction error of a regression fit $\hat{f}(X)$ at an input point $X = x_0$ using squared-error loss:

$$
\begin{align}
\text{Err}(x_0) &= \E[(Y - \hat{f}(x_0))^2 | X = x_0] \\
&= \sigma^2_\epsilon + [\E[\hat{f}(x_o)] - f(x_0)]^2 + \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2 \\
&= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_o)) + \Var(\hat{f}(x_0)) \\
&= \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}
\end{align}
$$

In order to minimize the expected test error rate, we need a statistical learning method that has both **low bias** and **low variance**. The difficulty is that few, if any, methods satisfy both requirements. As flexibility increases, the bias of a model will decrease but the variance will increase. Again, this is related to the problem of overfitting our training data set.

# Applications to classification models

The same basic principles apply to classification problems. The major difference is how we specify the loss function for a qualitative outcome.

## Loss functions

### Error rate

The most common approach is the **error rate**, or the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the observations:

$$\frac{1}{n} \sum_{n = 1}^{n} I(y_i \neq \hat{y}_i)$$

where $\hat{y}_i$ is the predicted classification label for the $i$th observation using some estimated function $\hat{f}$, and $I(y_i \neq \hat{y}_i)$ is an **indicator** function that equals 1 if $y_i \neq \hat{y}_i$ and 0 if $y_i = \hat{y}_i$ (i.e. if the observation was correctly classified).

The equation above refers to the **training error rate** because, as with the training MSE, it is computed using the training data set. The **test error rate** associated with a test set of observations of the form $(x_0, y_0)$ is given by

$$\overline{I(y_0 \neq \hat{y}_0)}$$

where $\hat{y}_0$ is the predicted class label that results from applying the classifier to the test observation with predictor $x_0$.

### Cross-entropy

Another commonly employed loss function for deep learning models is **cross-entropy**, or **log loss**. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of $0.75$ when the actual observation label is 1 is not as bad as predicting a probability of $0.1$ when the actual observation label is 1. A perfect model would have a log loss of 0.

```{r cross-entropy}
tibble(
  prob = seq(0.001, 1, by = 0.001),
  log_loss = -log(prob)
) %>%
  ggplot(aes(prob, log_loss)) +
  geom_line() +
  labs(title = "Log loss when true label = 1",
       x = "Predicted probability",
       y = "Log loss")
```

Log loss penalizes both false-positives and false-negatives, but more strongly penalizes predictions that are confident and wrong. In binary classification where the number of classes $M = 2$, cross-entropy can be calculated as:

$$-(y \log(p) + (1 - y) \log(1 - p))$$

If $M > 2$, we calculate a separate loss for each class label per observation and sum the result:

$$- \sum_{c=1}^M y_{o,c} \log(p_{o,c})$$

## Bayes classifier

# Estimating the expected test error

How can we estimate the expected test error rate when all we are given is a single data set? If our data set is sufficiently large, we randomly divide it into three parts:^[Assuming we have a standard cross-sectional data set.]

1. Training set - used to fit the models
1. Validation set - used to estimate prediction error for model selection
1. Test set - used for assessment of the generalization error of the final chosen model

A general rule of thumb suggests 50% allocated for training, and 25% each for validation and testing.

Commonly though, we do not have enough observations to allocate for each of these purposes. Remember that the fewer observations allocated to the training set, the more noisy the estimated model will be. Computational approaches such as cross-validation and bootstrapping allow us to efficiently use (and re-use) observations to provide reasonable samples for training, validating, and testing. We will cover those separately. Today, we consider a few methods which attempt to use the entire data set for training but still approximate the validation step analytically. These were used historically when computational power was not as prevalent.

## AIC


## BIC


## MDL


## SRM

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* This page is derived in part from ["Creating a LOESS animation with `gganimate`"](http://varianceexplained.org/files/loess.html) by David Robinson.
