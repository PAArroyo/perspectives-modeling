---
title: Assessing model accuracy
date: 2019-01-09T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Selecting and fitting a model
    weight: 2
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(broom)
library(here)
theme_set(theme_minimal())
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}}$$

# Reducible vs. irreducible error

Consider a set of inouts $X$ and an outcome $Y$. We can estimate a generic function

$$\hat{Y} = \hat{f}(X)$$

where $\hat{f}$ is our estimate for the true functional form $f$, and $\hat{Y}$ is the resulting prediction for $Y$. Different statistical learning algorithms will lead us to different estimates of $\hat{f}$ and therefore different $\hat{Y}$. The accuracy of $\hat{Y}$ depends on two major components:

1. Reducible error
1. Irreducible error

**Reducible error** is error generated by using an inappropriate or suboptimal technique to estimate $f$. If we improve on the technique, the reducible error can be reduced. Even if we perfectly estimated $f$ such that

$$\hat{Y} = f(X)$$

our predictions would still not guaranteed to be accurate. That is because the true data-generating process which created $Y$ is actually a function of $\epsilon$,

$$Y = f(X) + \epsilon$$

Therefore variability associated with $\epsilon$ also effects the accuracy of our predictions. No matter how well we estimate $f$, by definition we cannot estimate $\epsilon$.

$\epsilon$ could be driven by a number of factors, such as unmeasured variables useful in predicting $Y$, or inherently unmeasurable variation in the subject or observation. Assuming $\hat{f}$ and $X$ are fixed, we can show that

$$
\begin{align}
\E(Y - \bar{Y})^2 &= \E[f(X) + \epsilon - \hat{f}(X)]^2 \\
&= \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{\Var (\epsilon)}_{\text{Irreducible}}
\end{align}
$$

where $\E(Y - \bar{Y})^2$ is the expected value of the squared difference between the predicted and actual value of $Y$ and $\Var(\epsilon)$ is the variance associated with the error term $\epsilon$.

Irreducible error is potentially managed by ensuring we have a well-specified [model](/notes/build-a-model) that captures all the necessary variables, but in the end there will always be an unexplainable component to our statistical learning model. Reducible error, on the other hand, is more easily managed by selecting an appropriate statistical learning method for a given problem.

# Quality of fit

In order to evaluate how well a statistical learning method performs on a given data set, we need to define a measure for how well its predictions actually match the observed data. This is variously known as a **loss** or **cost** function $L(Y, \hat{f}(X))$, where $Y$ is the outcome of interest, $X$ is the input(s), and a prediction model $\hat{f}(X)$ that has been estimated using a training set $\tau$. These functions take the generic form

$$
L(Y, \hat{f}(X)) = \left\{
        \begin{array}{ll}
            (Y - \hat{f}(X))^2 & \quad \text{squared error} \\
            \mid Y - \hat{f}(X) \mid & \quad \text{absolute error}
        \end{array}
    \right.
$$

Different loss functions are appropriate for different kinds of outcomes of interest. For regression problems with a continuous outcome of interest, the most common loss function is the **mean squared error** (MSE), defined as

$$MSE = \frac{1}{N} \sum_{i = 1}^{N}{(y_i - \hat{f}(x_i))^2}$$

where:

* $y_i =$ the observed response value for the $i$th observation
* $\hat{f}(x_i) =$ the predicted response value for the $i$th observation given by $\hat{f}$
* $N =$ the total number of observations

Our goal is to identify a model that generates the smallest possible MSE. The MSE is an absolute measure of fit because its value depends on the measurement units of the response variable $Y$. It will be small if predicted values $\hat{Y}$ are close to the actual values $Y$, and will be large if for some/all observations the predicted and actual values differ significantly.

> **Root mean squared error** (RMSE) is the square root of MSE. It is also commonly seen in statistical packages as a model accuracy metric. You can use that as well, however we will use MSE because it is associated with **variance** ($\sigma^2$), whereas RMSE is associated with **standard deviation** ($\sigma$). Variance has certain statistical properties that are missing from standard deviation, so we will use MSE.

## Training vs. test error

The equation above more formally defines the **training error**

$$\overline{\text{Err}} = \frac{1}{N} \sum_{i = 1}^{N}{L(y_i, \hat{f}(x_i))}$$

which is the average loss over the training sample. However this is not actually the quantity we care about. We already know the values of $Y$ for the training data. Instead, we want to use the model to predict or explain the outcome of interest for an independent **test set** of data. So in truth, we want to know the **test error**, or the error/loss for an independent test set:

$$\text{Err}_\tau = \E[L(Y, \hat{f}(X)) | \tau]$$

Since we often do not have a test set (or could generate multiple potential test sets), we can instead frame this as the expected test error

$$\text{Err} = \E[L(Y, \hat{f}(X))] = \E[\text{Err}_\tau]$$

The expectation averages over everything that is random, including the randomness in the training set that produced $\hat{f}$. Our goal is to estimate $\text{Err}_\tau$, though in practice we tend to estimate the expected error $\text{Err}$ instead.

## Optimism of training error

It would be nice if we could use the training error as an estimate for the test/expected error. Alas, we cannot. Most statistical learning methods specifically attempt to minimize the training error (think least squares regression). This produces a relatively low training error, but does not guarantee a correspondingly low test error. In fact, it can have the opposite relationship -- as the training error decreases, the test error can actually increase.

```{r sim-train-model}
# simulate data from ISL figure 2.9
sim_mse <- tibble(
  x = runif(n = 50, min = 0, max = 100),
  y = 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3 + rnorm(50, sd = 0.6)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Training data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

Here I synthesize an example dataset and fit three separate models to it: a linear regression fit, and two **splines** which permit more flexible relationships between $X$ and $Y$. Notice that as the flexibility increases, the models are more likely to fit the data. That is, the training MSE will decrease. But this does not guarantee the test MSE also decreases. Consider the trained models applied to an independent test set generated from the same underlying process:

```{r sim-test-model, dependson = "sim-train-mse"}
sim_mse_test <- tibble(
  x = runif(n = 50, min = 0, max = 100),
  y = 5.055901 - 0.1848551 * x + 0.00748706 * x^2 - 0.00005543478 * x^3 + rnorm(50, sd = 0.6)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point(data = sim_mse_test) +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Test data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

It is not entirely clear, but these models no longer perform as well as compared to the training data. This divergence increases as the flexibility of the model increases.

```{r sim-test-mse, dependson = "sim-tran-model"}
sim_mse_test <- tibble(
  x = runif(n = 1e04, min = 0, max = 100),
  y = 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3 + rnorm(1e04, sd = 0.5)
)

# train vs. test MSE
train_test_mse <- tibble(df = 2:30) %>%
  mutate(model = map(df, ~ lm(y ~ splines::ns(x, .x), data = sim_mse)),
         pred = map(model, augment),
         mse_train = map_dbl(pred, ~ mean(.$.resid^2)),
         pred_test = map(model, augment, newdata = sim_mse_test),
         mse_test = map_dbl(pred_test, ~ mean((.$y - .$.fitted)^2))) %>%
  gather(mse, value, mse_train, mse_test) %>%
  mutate(mse = str_remove(mse, "mse_"),
         mse = str_to_title(mse))

train_test_mse %>%
  ggplot(aes(df, value, color = mse)) +
  geom_smooth(se = FALSE) +
  scale_color_brewer(type = "qual") +
  scale_x_log10() +
  labs(x = "Flexibility",
       y = "Mean squared error",
       color = NULL) +
  theme(legend.position = "bottom")
```

This discrepancy holds true for any data set and any statistical learning method. As the model flexibility increases, training MSE will decrease but the test MSE may not. When a given method generates a small training MSE and a large test MSE, we are said to be **overfitting** the data. The model is being tuned too closely to the training set of observations, and is no longer **generalizable**. It is detecting artifacts and random patterns specific to the single set of observations but which don't really exist in the data-generating process.

# Bias-variance trade-off

## Bias and variance defined

**Bias** defines the error that is introduced by approximating a real-life problem using a simplified model. In the context of statistical learning, this is the amount by which the average of our estimate differs from the true mean:

$$\text{Bias} = \E[\hat{f}(x_o)] - f(x_0)$$

Generally speaking, we want to have little-to-no bias in our estimates; otherwise we are consistently estimating an incorrect value for $\hat{f}(x_0)$.

**Variance** refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Formally, it is the expected squared deviation of $\hat{f}(x_0)$ around its mean:

$$\text{Variance} = \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2$$

## Bias-variance decomposition

If we assume that $Y = f(X) + \epsilon$ where $\E[\epsilon] = 0$ and $\Var(\epsilon) = \sigma^2_\epsilon$, we can derive an expression for the expected prediction error of a regression fit $\hat{f}(X)$ at an input point $X = x_0$ using squared-error loss:

$$
\begin{align}
\text{Err}(x_0) &= \E[(Y - \hat{f}(x_0))^2 | X = x_0] \\
&= \sigma^2_\epsilon + [\E[\hat{f}(x_o)] - f(x_0)]^2 + \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2 \\
&= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_o)) + \Var(\hat{f}(x_0)) \\
&= \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}
\end{align}
$$

In order to minimize the expected test error rate, we need a statistical learning method that has both **low bias** and **low variance**. The difficulty is that few, if any, methods satisfy both requirements. As flexibility increases, the bias of a model will decrease but the variance will increase. Consider our sample data from earlier:

```{r sim-train-data, dependson = "sim-train-mse"}
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  labs(title = "Training data points",
       x = expression(X),
       y = expression(Y))
```

A low bias, high variance modeling strategy would be to draw a curve that passes through every data point:

```{r sim-data-nn, dependson = "sim-train-mse"}
# estimate KNN model, k = 1
sim_knn1 <- FNN::knn.reg(sim_mse,
                         y = sim_mse$y,
                         test = sim_mse,
                         k = 1)

sim_mse %>%
  mutate(pred = sim_knn1$pred) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "Training data points",
       x = expression(X),
       y = expression(Y))
```

It has low bias because the training MSE is 0, but it has high variance because if we generate a new training set the model will be entirely different. Again, this is related to the problem of overfitting our training data set.

Compare this to a method with high bias but low variance: a horizontal line at the mean:

```{r sim-data-mean, dependson = "sim-train-mse"}
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  geom_hline(yintercept = mean(sim_mse$y)) +
  labs(title = "Training data points",
       x = expression(X),
       y = expression(Y))
```

The variance of our model $\hat{f}$ should be low since the mean value for any given independent sample from the population should possess the same basic characteristics. But this model has a lot more bias (aka a higher MSE) because its accuracy is not good.

How do we actually account for this trade-off? That is to say, which should be most concerned with if we cannot reduce both components? Bias or variance? That entirely depends on our goals.

```{r darts}
# set number of throws
numDarts <- 10

# throw numDarts number of darts and get the coordinates where they hit
throwDarts <- function(numDarts, reliable = TRUE, valid = TRUE) {
  if (reliable & valid) {
    xvals <- rnorm(numDarts, mean = 0, sd = .05)
    yvals <- rnorm(numDarts, mean = 0, sd = .05)
  } else if (reliable == TRUE & valid == FALSE) {
    xvals <- rnorm(numDarts, mean = .5, sd = .05)
    yvals <- rnorm(numDarts, mean = .4, sd = .05)
  } else if (reliable == FALSE & valid == TRUE) {
    xvals <- rnorm(numDarts, mean = 0, sd = .3)
    yvals <- rnorm(numDarts, mean = 0, sd = .3)
  } else if (reliable == FALSE & valid == FALSE) {
    xvals <- rnorm(numDarts, mean = .5, sd = .3)
    yvals <- rnorm(numDarts, mean = -.4, sd = .3)
  }
  
  tibble(
    x = xvals,
    y = yvals,
    reliable = reliable,
    valid = valid
  )
}

# get data for each situation
throws <- bind_rows(
  throwDarts(numDarts, reliable = TRUE, valid = TRUE),
  throwDarts(numDarts, reliable = TRUE, valid = FALSE),
  throwDarts(numDarts, reliable = FALSE, valid = TRUE),
  throwDarts(numDarts, reliable = FALSE, valid = FALSE)
) %>%
  mutate(
    reliable = ifelse(reliable, "Low Variance", "High Variance"),
    valid = ifelse(valid, "Low Bias", "High Bias")
  )

# plot the dart board, facet by each type
ggplot(data = throws, aes(x, y)) +
  facet_grid(reliable ~ valid) +
  ggforce::geom_circle(aes(x = NULL, y = NULL, x0 = 0, y0 = 0, r = 1)) +
  geom_point(alpha = 0.5) +
  xlim(-1, 1) +
  ylim(-1, 1) +
  coord_fixed() +
  labs(title = NULL,
       x = NULL,
       y = NULL) +
  annotate("point", x = 0, y = 0, size = 3)
```

Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. Imagine we can repeat our entire model building process to get a number of separate hits on the target. Each hit represents an individual realization of our model, given the chance variability in the training data we gather. Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions. These different realizations result in a scatter of hits on the target.^[Explanation drawn from [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html).]

Traditionally in statistics we are concerned with reducing the bias of our estimates. Consider one of the desirable properties of the ordinary least squares estimator: it is the [best linear unbiased estimator (BLUE)](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem). However linear regression models could be estimated with alternative estimators, such as ridge regression (see chapter 6 in [ISL]). These methods are not unbiased, but have lower variance. What is the value to such an estimator? One desirable quality is simplicity. As we will learn, ridge regression artificially biases the parameters in the regression model towards zero. This enables us to do things we could otherwise not do, such as estimate a regression model where the number of predictors is greater than the number of observations$p > n$.

What if our ultimate goal is prediction? Then our primary concern is reducing the test error. Increasing bias slightly to more dramatically decrease variance could lead to a lower test error rate. In that situation, it could make sense to intentionally increase our bias.

# Applications to classification models

The same basic principles apply to classification problems. The major difference is how we specify the loss function for a qualitative outcome.

## Loss functions

### Error rate

The most common approach is the **error rate**, or the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the observations:

$$\frac{1}{n} \sum_{n = 1}^{n} I(y_i \neq \hat{y}_i)$$

where $\hat{y}_i$ is the predicted classification label for the $i$th observation using some estimated function $\hat{f}$, and $I(y_i \neq \hat{y}_i)$ is an **indicator** function that equals 1 if $y_i \neq \hat{y}_i$ and 0 if $y_i = \hat{y}_i$ (i.e. if the observation was correctly classified).

The equation above refers to the **training error rate** because, as with the training MSE, it is computed using the training data set. The **test error rate** associated with a test set of observations of the form $(x_0, y_0)$ is given by

$$\overline{I(y_0 \neq \hat{y}_0)}$$

where $\hat{y}_0$ is the predicted class label that results from applying the classifier to the test observation with predictor $x_0$.

### Cross-entropy

Another commonly employed loss function for deep learning models is **cross-entropy**, or **log loss**. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of $0.75$ when the actual observation label is 1 is not as bad as predicting a probability of $0.1$ when the actual observation label is 1. A perfect model would have a log loss of 0.

```{r cross-entropy}
tibble(
  prob = seq(0.001, 1, by = 0.001),
  log_loss = -log(prob)
) %>%
  ggplot(aes(prob, log_loss)) +
  geom_line() +
  labs(title = "Log loss when true label = 1",
       x = "Predicted probability",
       y = "Log loss")
```

Log loss penalizes both false-positives and false-negatives, but more strongly penalizes predictions that are confident and wrong. In binary classification where the number of classes $M = 2$, cross-entropy can be calculated as:

$$-(y \log(p) + (1 - y) \log(1 - p))$$

If $M > 2$, we calculate a separate loss for each class label per observation and sum the result:

$$- \sum_{c=1}^M y_{o,c} \log(p_{o,c})$$

## Bayes classifier

# Estimating the expected test error

How can we estimate the expected test error rate when all we are given is a single data set? If our data set is sufficiently large, we randomly divide it into three parts:^[Assuming we have a standard cross-sectional data set.]

1. Training set - used to fit the models
1. Validation set - used to estimate prediction error for model selection
1. Test set - used for assessment of the generalization error of the final chosen model

A general rule of thumb suggests 50% allocated for training, and 25% each for validation and testing.

Commonly though, we do not have enough observations to allocate for each of these purposes. Remember that the fewer observations allocated to the training set, the more noisy the estimated model will be. Computational approaches such as cross-validation and bootstrapping allow us to efficiently use (and re-use) observations to provide reasonable samples for training, validating, and testing. We will cover those separately. Today, we consider a few methods which attempt to use the entire data set for training but still approximate the validation step analytically. These were used historically when computational power was not as prevalent.

## AIC


## BIC


## MDL


## SRM

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* This page is derived in part from ["Creating a LOESS animation with `gganimate`"](http://varianceexplained.org/files/loess.html) by David Robinson.
