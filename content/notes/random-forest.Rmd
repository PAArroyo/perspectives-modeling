---
title: Random forests
date: 2019-02-27T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Tree-based inference
    weight: 3
---

```{r setup, include = FALSE}
# set default chunk options
knitr::opts_chunk$set(cache = TRUE)
```

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(tidymodels)
library(randomForest)
library(patchwork)
library(rcfss)
library(rpart)
library(rpart.plot)
library(ranger)
library(iml)

set.seed(1234)
theme_set(theme_minimal())
```

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}} \newcommand{\Lagr}{\mathcal{L}} \newcommand{\lagr}{\mathcal{l}}

# Limitations of bagging tree aggregation

The problem with [bagging](/notes/bagging/) is that if there is a single dominant predictor in the dataset, most trees will use the same predictor for the first split and ensure correlation and similarity among the trees. Consider a set of six decision trees grown using the `ISLR::Auto` dataset to predict fuel efficiency (`mpg`), where each decision tree is constructed from a bootstrapped sample:

```{r mpg-bag-trees, dependson = "ames"}
# generate bootstrapped samples
bag_trees <- bootstraps(ISLR::Auto, times = 6) %>%
  mutate(tree = map(splits, ~ rpart(mpg ~ .,
                                    data = analysis(.x) %>%
                                      select(-name),
                                    method = "anova",
                                    model = TRUE)))

par(mfrow = c(2, 3))
for(i in 1:nrow(bag_trees)) {
  rpart.plot(bag_trees$tree[[i]],
             main = str_c("Decision Tree", i, sep = " "))
}
```

Almost every tree splits on the same initial predictor (`displacement`). The resulting downstream branches also appear quite similar across trees. Because bagged trees are grown sufficiently deep without pruning, they will have low bias. The tree aggregation introduces a random component into the tree-building process (i.e. bootstrap sampling) which reduces the variance compared to a single tree's prediction, but each tree is not independent from each other: all of the trees are grown from essentially the same underlying data structure using the same growth and pruning algorithms. We should expect the individual trees to look similar to one another.

This means the predictions from each $B$ bootstrap sample are **identically distributed** (**i.d.**) random variables, each with variance $\sigma^2$. If they were independent from one another (**independently and identically distributed** or **i.i.d.**), the variance of their average over $B$ random variables would simply be

$$\frac{1}{B} \sigma^2$$

However the variance for an average of i.d. variables with positive pairwise correlation $\rho$ is

$$\rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2$$

As $B$ increases, the second term shrinks towards zero, but the first always remains. Therefore the size of the correlation of pairs of bagged trees limits the benefits of averaging. While bagging reduces the variance of our estimates, we will always be constrained by $\rho \sigma^2$.

# Random forests

To achieve better variance reduction, we need to minimize the correlation between the trees. If we can make the predictions from the $B$ samples i.i.d., then their average prediction will have smaller variance compared to bagging. That is, we need more randomness in the tree-growing process. **Random forests** do this in two ways:

1. Bootstrap sampling - like bagging, random forests generate $B$ bootstrap resampled data sets, which makes them different and somewhat decorrelated
1. Split-variable randomization - each time a split is to be performed, rather than considering all possible variables on which to perform a split, the search for the split variable is limited to a random subset of $m \leq p$ variables.
    * For regression trees, the default value for $m$ is $\frac{m}{3}$ and the minimum node size is five.
    * For classification trees, the default value for $m$ is $\sqrt{p}$ and the minimum node size is one.
    * Note these are only default values - $m$ and the minimum node size are tuning parameters in the model and can be tuned to the specific dataset.
    * When $m = p$, the random forests are equivalent to bagging.

The basic algorithm for a random forest is:

1. For $b=1$ to $B$:
    a. Draw a bootstrap sample $\mathbf{Z}^*$ of size $N$ from the training data
    a. Grow a decision tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree until the minimum node size $n_\text{min}$ is reached:
        a. Select $m$ variables at random from the $p$ predictors
        a. Pick the best variable/split-point among the $m$
        a. Split the node into two child nodes
1. Output the ensemble of trees $\{ T_b \}_1^B$

To make a prediction at a new point $x$:

* Regression - take the average prediction of all the decision trees
* Classification - take the most commonly predicted class of all the decision trees

Because random forests are another tree-aggregation method, we can still use [out-of-bag estimates](/notes/bagging/#out-of-bag-estimates) to estimate the error rate of the model. Likewise, we can use measures of [feature importance, partial dependence plots, and feature interactions](/notes/global-interpretation/) to substantively interpret how different predictors are associated with the outcome of interest.

# Tuning a random forest

## Default values

Random forests are convenient because they do not require much tuning to achieve optimal results. Even using default hyperparameter settings, models can perform fairly well. Let's use a random forest model to predict house sale price in the Ames housing data set. Here I don't modify any of the default values for the `randomForest()` function.

```{r ames}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

```{r ames-rf, dependson = "ames"}
# for reproduciblity
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)

m1
```

We can first consider how many trees we needed to grow to minimize our test error.

```{r ames-rf-plot, dependson = "ames-rf"}
tibble(
  mse = m1$mse
) %>%
  mutate(
    ntrees = row_number()
  ) %>%
  ggplot(aes(ntrees, mse)) +
  geom_line() +
  geom_vline(xintercept = which.min(m1$mse), linetype = 2) +
  labs(x = "Number of trees grown",
       y = "OOB MSE")
```

As the number of trees grown increases, OOB MSE decreases rapidly until about 50 then slows down. The optimal random forest with these hyperparameter settings only requires `r which.min(m1$mse)` trees.

How does the OOB estimate compare to an actual validation set estimate?

```{r ames-rf-oob-val, dependson = "ames-rf"}
# create training and validation data 
set.seed(123)
valid_split <- initial_split(ames_train, .8)

# training data
ames_train_v2 <- analysis(valid_split)

# validation data
ames_valid <- assessment(valid_split)
x_test <- select(ames_valid, -Sale_Price)
y_test <- ames_valid$Sale_Price

rf_oob_comp <- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train_v2,
  xtest = x_test,
  ytest = y_test
)
```

```{r ames-rf-oob-val-plot, dependson = "ames-rf-oob-val"}
# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation
) %>%
  mutate(ntrees = row_number()) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual") +
  xlab("Number of trees")
```

While they are not precisely the same value, they definitely have similar trends. With an RMSE of less than \$30K, this model performs better than a fully tuned bagging model or even a regularized regression model (i.e. elastic net).

## Tuning parameter options

The most common tuning parameters for random forest models are:

* Number of trees - we want enough trees to stabilize our estimates of the error, but using too many trees is computationally inefficient
* Number of variables to randomly sample at each split - if $m=p$, the model equates to bagging. If $m=1$, the split variable is completely random. A common approach starts with 5 values evenly spaced across the range from $2$ to $p$
* Sample size for each bootstrapped sample - if we sample with replacement, our sample size is $N$. Instead, we could sample **without replacement** and only sample a fraction of the original sample. Decreasing the sample size can reduce training time but increase the bias of our estimates. Increasing the sample size can increase performance but at the risk of overfitting since it introduces more variance.
* Minimum number of observations within the terminal nodes - this controls the complexity of the trees. Small node size allows for deeper, more complex trees (lower bias, higher variance), whereas larger node results in shallower trees (higher bias, lower variance).
* Maximum number of terminal nodes - another method for controling the complexity of the tree. More nodes leads to deeper trees, fewer nodes leads to shallower trees

We can perform a grid search over several hyperparameters, but if you do this make sure to choose an efficient function that implements a random forest model. Otherwise if each random forest takes a minute to grow, hyperparameter tuning over dozens of combinations of hyperparameters will take close to an hour. In R, the `ranger` package implements an efficient random forest algorithm.

```{r ames-hypergrid}
# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)
```

```{r ames-rf-tune, dependson = "ames"}
for(i in 1:nrow(hyper_grid)) {
  # train model
  model <- ranger(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    num.trees = 500,
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
```

```{r ames-rf-tune-results, dependson = "ames-rf-tune"}
hyper_grid <- hyper_grid %>% 
  arrange(OOB_RMSE)
hyper_grid %>%
  head(10)
```

These results indicate the best random forest model uses $m=`r hyper_grid$mtry[[1]]`$, terminal node size of `r hyper_grid$node_size[[1]]` observations, and a sample size of `r hyper_grid$sampe_size[[1]]`. Let's repeat this model to get a better expectation of our error rate.

```{r ames-rf-tune-final, dependson = "ames-rf-tune-results"}
OOB_RMSE <- vector(mode = "numeric", length = 100)

set.seed(123)
for(i in seq_along(OOB_RMSE)) {
  optimal_ranger <- ranger(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    num.trees = 500,
    mtry = hyper_grid$mtry[[1]],
    min.node.size = hyper_grid$node_size[[1]],
    sample.fraction = hyper_grid$sampe_size[[1]]
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}
```

```{r ames-rf-tune-final-plot, dependson = "ames-rf-tune-final"}
tibble(RMSE = OOB_RMSE) %>%
  ggplot(aes(RMSE)) +
  geom_histogram() +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Distribution of OOB RMSE",
       subtitle = "Repeats = 100",
       x = "OOB RMSE",
       y = "Frequency")
```

# Interpreting a random forest

## Feature importance

```{r ames-rf-predictor, dependson = "ames-rf-tune-final"}
pred_ranger <- function(model, newdata){
  results <- predict(model, newdata)
  return(results$prediction)
}

predictor_rf <- Predictor$new(
  model = optimal_ranger,
  data = select(ames_train, - Sale_Price),
  y = ames_train$Sale_Price,
  predict.fun = pred_ranger
)
```

```{r iml-parallel-begin, cache = FALSE}
library(doParallel)

cl <- makePSOCKcluster(3)
registerDoParallel(cl)
```

```{r ames-rf-feat-imp, dependson = "ames-rf-predictor"}
feat_imp_rf <- FeatureImp$new(predictor_rf, loss = "mse", parallel = TRUE)
```

```{r ames-rf-feat-imp-plot, dependson = "ames-rf-feat-imp", fig.asp = 1.4}
plot(feat_imp_rf) +
  ggtitle("Random forest")
```

## Partial dependence

```{r ames-rf-pdp, dependson = "ames-rf-predictor", fig.asp = 1}
pdp_Overall_Qual <- FeatureEffect$new(predictor_rf,
                                      "Overall_Qual",
                                      method = "pdp+ice")
pdp_Gr_Liv_Area <- FeatureEffect$new(predictor_rf,
                                     "Gr_Liv_Area",
                                     method = "pdp+ice",
                                     center.at = min(ames_train$Gr_Liv_Area),
                                     grid.size = 50)
pdp_Year_Built <- FeatureEffect$new(predictor_rf,
                                    "Year_Built",
                                    method = "pdp+ice",
                                    center.at = min(ames_train$Year_Built),
                                    grid.size = 50)

p1 <- plot(pdp_Overall_Qual)
p2 <- plot(pdp_Gr_Liv_Area)
p3 <- plot(pdp_Year_Built)

p1 + p2 + p3 +
  plot_layout(ncol = 1)
```

## Feature interaction

```{r ames-rf-int, dependson = "ames-rf-predictor", eval = FALSE}
int_rf <- Interaction$new(predictor_rf, parallel = TRUE)
```

```{r ames-rf-int-plot, dependson = "ames-rf-int", eval = FALSE}
plot(int_rf)
```

```{r iml-parallel-end, cache = FALSE}
stopCluster(cl)
```

# Comparison to bagging

```{r bag-compare, dependson = "ames"}
bag_oob <- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train,
  num.trees = 500,
  mtry = ncol(ames_train) - 1,
  nodesize = hyper_grid$node_size[[1]],
  replace = FALSE,
  sampsize = ceiling(hyper_grid$sampe_size[[1]] * nrow(ames_train))
)
```

```{r rf-compare, dependson = "ames-rf-tune-results"}
rf_oob <- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train,
  num.trees = 500,
  mtry = hyper_grid$mtry[[1]],
  nodesize = hyper_grid$node_size[[1]],
  replace = FALSE,
  sampsize = ceiling(hyper_grid$sampe_size[[1]] * nrow(ames_train))
)
```

```{r rf-bag-compare, dependson = c("bag-compare", "rf-compare")}
tibble(
  Bagging = sqrt(bag_oob$mse),
  `Random Forest` = sqrt(rf_oob$mse)
) %>%
  mutate(ntrees = row_number()) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  scale_color_brewer(type = "qual") +
  xlab("Number of trees")
```

Holding the tuning parameters constant, random forest leads to a lower test error compared to the equivalent bagging model.

# Session Info {.toc-ignore}

```{r child = here::here("R", "_session-info.Rmd")}
```

# References {.toc-ignore}

* @james2013introduction
* @friedman2001elements
