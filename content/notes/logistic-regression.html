---
title: Logistic regression
date: 2019-01-16T13:30:00-06:00  # Schedule page publish date.
    
draft: false
type: docs

bibliography: [../../static/bib/sources.bib]
csl: [../../static/bib/apa.csl]
link-citations: true

menu:
  notes:
    parent: Classification
    weight: 1
---

<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<div id="TOC">
<ul>
<li><a href="#classification-problems"><span class="toc-section-number">1</span> Classification problems</a><ul>
<li><a href="#a-linear-regression-approach"><span class="toc-section-number">1.1</span> A linear regression approach</a></li>
</ul></li>
<li><a href="#logistic-regression"><span class="toc-section-number">2</span> Logistic regression</a><ul>
<li><a href="#probability"><span class="toc-section-number">2.1</span> Probability</a><ul>
<li><a href="#probability-of-surviving-the-titanic"><span class="toc-section-number">2.1.1</span> Probability of surviving the Titanic</a></li>
<li><a href="#generating-predicted-probabilities"><span class="toc-section-number">2.1.2</span> Generating predicted probabilities</a></li>
</ul></li>
<li><a href="#odds"><span class="toc-section-number">2.2</span> Odds</a><ul>
<li><a href="#odds-of-surviving-the-titanic"><span class="toc-section-number">2.2.1</span> Odds of surviving the Titanic</a></li>
</ul></li>
<li><a href="#log-odds"><span class="toc-section-number">2.3</span> Log-odds</a><ul>
<li><a href="#log-odds-of-surviving-the-titanic"><span class="toc-section-number">2.3.1</span> Log-odds of surviving the Titanic</a></li>
</ul></li>
<li><a href="#estimating-the-parameters"><span class="toc-section-number">2.4</span> Estimating the parameters</a></li>
<li><a href="#generating-predicted-probabilities-using-add_predictions"><span class="toc-section-number">2.5</span> Generating predicted probabilities using <code>add_predictions()</code></a></li>
<li><a href="#multiple-predictors"><span class="toc-section-number">2.6</span> Multiple predictors</a></li>
<li><a href="#predicted-probabilities-and-first-differences-in-multiple-variable-models"><span class="toc-section-number">2.7</span> Predicted probabilities and first differences in multiple variable models</a></li>
<li><a href="#interactive-terms"><span class="toc-section-number">2.8</span> Interactive terms</a></li>
</ul></li>
<li><a href="#evaluating-model-accuracy"><span class="toc-section-number">3</span> Evaluating model accuracy</a><ul>
<li><a href="#error-rate"><span class="toc-section-number">3.1</span> Error rate</a></li>
<li><a href="#proportional-reduction-in-error"><span class="toc-section-number">3.2</span> Proportional reduction in error</a></li>
<li><a href="#receiver-operating-characteristics-roc-curve"><span class="toc-section-number">3.3</span> Receiver operating characteristics (ROC) curve</a><ul>
<li><a href="#types-of-error"><span class="toc-section-number">3.3.1</span> Types of error</a></li>
<li><a href="#confusion-matrix"><span class="toc-section-number">3.3.2</span> Confusion matrix</a></li>
<li><a href="#alternative-thresholds"><span class="toc-section-number">3.3.3</span> Alternative thresholds</a></li>
<li><a href="#roc-curve"><span class="toc-section-number">3.3.4</span> ROC curve</a></li>
</ul></li>
</ul></li>
<li><a href="#session-info"><span class="toc-section-number">4</span> Session Info</a></li>
<li><a href="#references"><span class="toc-section-number">5</span> References</a></li>
</ul>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(broom)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(patchwork)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
<span class="kw">theme_set</span>(<span class="kw">theme_minimal</span>())</code></pre></div>
<div id="classification-problems" class="section level1">
<h1><span class="header-section-number">1</span> Classification problems</h1>
<p>The sinking of <a href="https://en.wikipedia.org/wiki/RMS_Titanic">RMS Titanic</a> provided the world with many things:</p>
<ul>
<li>A fundamental shock to the world as its faith in supposedly indestructible technology was shattered by a chunk of ice</li>
<li><p>Perhaps the best romantic ballad of all time</p>
<iframe width="853" height="480" src="https://www.youtube.com/embed/WNIPqafd4As?start=175" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe></li>
<li><p>A tragic love story</p>
<div class="figure">
<img src="https://i.giphy.com/KSeT85Vtym7m.gif" alt="Titanic (1997)" />
<p class="caption"><a href="https://en.wikipedia.org/wiki/Titanic_(1997_film)">Titanic (1997)</a></p>
</div></li>
</ul>
<p>Why did Jack have to die? Why couldn’t he have made it onto a lifeboat like Cal? We may never know the answer, but we can generalize the question a bit: why did some people survive the sinking of the Titanic while others did not?</p>
<p>In essence, we have a <strong>classification</strong> problem. The response is a <strong>qualitative variable</strong>, in this case a binary variable indicating whether a specific passenger survived. This differs from a <strong>regression</strong> problem. In a regression problem, the response variable is <strong>quantitative</strong> and could take on potentially an infinite range of values. In classification problems, we want to develop a model that assigns observations to <strong>categories</strong> or <strong>classes</strong> of the response variable. Given our knowledge of survivors and diers on the Titanic, if we combine this with predictors that describe each passenger we might be able to estimate a general model of survival.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Kaggle is an online platform for predictive modeling and analytics. They run regular competitions where they provide the public with a question and data, and anyone can estimate a predictive model to answer the question. They’ve run a popular contest based on a <a href="https://www.kaggle.com/c/titanic/data">dataset of passengers from the Titanic</a>. The datasets have been conveniently stored in a package called <code>titanic</code>. Let’s load the package and convert the desired data frame to a tibble.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(titanic)
titanic &lt;-<span class="st"> </span>titanic_train <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># remove missing values</span>
<span class="st">  </span><span class="kw">na.omit</span>()

titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="right">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left"></td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">McCarthy, Mr. Timothy J</td>
<td align="left">male</td>
<td align="right">54</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">17463</td>
<td align="right">51.8625</td>
<td align="left">E46</td>
<td align="left">S</td>
</tr>
</tbody>
</table>
<p>The codebook contains the following information on the variables:</p>
<pre><code>VARIABLE DESCRIPTIONS:
Survived        Survival
                (0 = No; 1 = Yes)
Pclass          Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
Name            Name
Sex             Sex
Age             Age
SibSp           Number of Siblings/Spouses Aboard
Parch           Number of Parents/Children Aboard
Ticket          Ticket Number
Fare            Passenger Fare
Cabin           Cabin
Embarked        Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = Southampton)

SPECIAL NOTES:
Pclass is a proxy for socio-economic status (SES)
 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower

Age is in Years; Fractional if Age less than One (1)
 If the Age is Estimated, it is in the form xx.5

With respect to the family relation variables (i.e. sibsp and parch)
some relations were ignored.  The following are the definitions used
for sibsp and parch.

Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic
Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)
Parent:   Mother or Father of Passenger Aboard Titanic
Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic

Other family relatives excluded from this study include cousins,
nephews/nieces, aunts/uncles, and in-laws.  Some children travelled
only with a nanny, therefore parch=0 for them.  As well, some
travelled with very close friends or neighbors in a village, however,
the definitions do not support such relations.</code></pre>
<p>So if this is our data, <code>Survived</code> is our <strong>response variable</strong>, and the remaining variables are <strong>predictors</strong>, how can we determine who survives and who dies?</p>
<div id="a-linear-regression-approach" class="section level2">
<h2><span class="header-section-number">1.1</span> A linear regression approach</h2>
<p>Let’s concentrate first on the relationship between age and survival. Using the methods we previously learned, we could estimate a linear regression model:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_{1}X\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;???&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/titanic_ols-1.png" width="672" /></p>
<p>Hmm. Not terrible, but you can immediately notice a couple of things. First, the only possible values for <code>Survival</code> are <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Yet the linear regression model gives us predicted values such as <span class="math inline">\(.4\)</span> and <span class="math inline">\(.25\)</span>. How should we interpret those?</p>
<p>One possibility is that these values are <strong>predicted probabilities</strong>. That is, the estimated probability a passenger will survive given their age. So someone with a predicted probability of <span class="math inline">\(.4\)</span> has a 40% chance of surviving. Okay, but notice that because the line is linear and continuous, it extends infinitely in both directions of age.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlim</span>(<span class="dv">0</span>, <span class="dv">200</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;???&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/titanic_ols_old-1.png" width="672" /></p>
<p>What happens if a 200 year old person is on the Titanic?<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> They would have a <span class="math inline">\(-.1\)</span> probability of surviving. <strong>But you cannot have a probability outside of the <span class="math inline">\([0,1]\)</span> interval!</strong> Admittedly this is a trivial example, but in other circumstances this can become a more realistic scenario.</p>
<p>Or what if we didn’t want to predict survival, but instead predict the port from which an individual departed (Cherbourg, Queenstown, or Southampton). We could try and code this as a numeric response variable:</p>
<table>
<thead>
<tr class="header">
<th>Numeric value</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Cherbourg</td>
</tr>
<tr class="even">
<td>2</td>
<td>Queenstown</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Southampton</td>
</tr>
</tbody>
</table>
<p>But why not instead code it:</p>
<table>
<thead>
<tr class="header">
<th>Numeric value</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Queenstown</td>
</tr>
<tr class="even">
<td>2</td>
<td>Cherbourg</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Southampton</td>
</tr>
</tbody>
</table>
<p>Or even:</p>
<table>
<thead>
<tr class="header">
<th>Numeric value</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Southampton</td>
</tr>
<tr class="even">
<td>2</td>
<td>Cherbourg</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Queenstown</td>
</tr>
</tbody>
</table>
<p><strong>There is no inherent ordering to this variable.</strong> Any claimed linear relationship between a predictor and port of embarkation is completely dependent on how we convert the classes to numeric values.</p>
</div>
</div>
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">2</span> Logistic regression</h1>
<div id="probability" class="section level2">
<h2><span class="header-section-number">2.1</span> Probability</h2>
<p>Rather than modeling the response <span class="math inline">\(Y\)</span> directly, logistic regression instead models the <strong>probability</strong> that <span class="math inline">\(Y\)</span> belongs to a particular category. In our first Titanic example, the probability of survival can be written as:</p>
<p><span class="math display">\[\Pr(Y) = \Pr(\text{survival} = \text{yes} | \text{age})\]</span></p>
<p>The linear regression model above attempted to predict or explain <span class="math inline">\(\Pr(Y)\)</span> using the functional form</p>
<p><span class="math display">\[f(X) = \beta_0 + \beta_{1}X\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate fake binary data</span>
sim_logit &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">1000</span>, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>),
                        <span class="dt">y =</span> <span class="dv">0</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>x)

<span class="co"># graph it</span>
<span class="kw">ggplot</span>(sim_logit, <span class="kw">aes</span>(x, y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;The linear function&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(<span class="kw">f</span>(X)))</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/linear-demo-1.png" width="672" /></p>
<p>But we now know that by using this functional form, we will always have for at least some subset of <span class="math inline">\(\text{age}\)</span> predicted values outside the <span class="math inline">\([0,1]\)</span> range. Instead, we need to model the functional form to prevent values from exceeding this range. One such function is the <strong>logistic function</strong>:<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display">\[f(X) = \frac{e^{\beta_0 + \beta_{1}X}}{1 + e^{\beta_0 + \beta_{1}X}}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2prob &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">exp</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_logit &lt;-<span class="st"> </span>sim_logit <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob =</span> <span class="kw">logit2prob</span>(y))

<span class="co"># graph it</span>
<span class="kw">ggplot</span>(sim_logit, <span class="kw">aes</span>(x, prob)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;The logistic function&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="kw">expression</span>(<span class="kw">Pr</span>(Y)))</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/logit-demo-1.png" width="672" /></p>
<p>The logistic transformation produces an S-shaped curve that preserves ordering; that is, larger values of <span class="math inline">\(X\)</span> will also result in larger values of <span class="math inline">\(\Pr(X)\)</span>, but the relationship is now <strong>curvilinear</strong>. <span class="math inline">\(\Pr(X)\)</span> will never decrease below 0 and never exceed 1, so the predicted probabilities will always have inherent meaning.</p>
<div id="probability-of-surviving-the-titanic" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Probability of surviving the Titanic</h3>
<p><span class="math display">\[\Pr(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age}}}{1 + e^{\beta_0 + \beta_{1}\text{Age}}}\]</span></p>
<p>The values of <span class="math inline">\(\Pr(\text{survival} = \text{Yes} | \text{age})\)</span> (or simply <span class="math inline">\(\Pr(\text{survival})\)</span> will range between 0 and 1. Given that predicted probability, we could predict anyone with for whom <span class="math inline">\(\Pr(\text{survival}) &gt; .5\)</span> will survive the sinking, and anyone else will die.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>We can estimate the logistic regression model using the <code>glm()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age, <span class="dt">data =</span> titanic, <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1488  -1.0361  -0.9544   1.3159   1.5908  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.05672    0.17358  -0.327   0.7438  
## Age         -0.01096    0.00533  -2.057   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.23  on 712  degrees of freedom
## AIC: 964.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Which produces a line that looks like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate predicted values</span>
survive_age_pred &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># predicted values are in the log-odds form - convert to probabilities</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob =</span> <span class="kw">logit2prob</span>(pred))

<span class="kw">ggplot</span>(survive_age_pred, <span class="kw">aes</span>(Age)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> Survived)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> prob), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Probability of surviving the Titanic&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/titanic_age_glm_plot-1.png" width="672" /></p>
<p>It’s hard to tell, but the line is not perfectly linear. Let’s expand the range of the x-axis to prove this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="co"># use geom_smooth for out-of-sample range plotting</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">method.args =</span> <span class="kw">list</span>(<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>),
              <span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">fullrange =</span> <span class="ot">TRUE</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlim</span>(<span class="dv">0</span>,<span class="dv">200</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Probability of surviving the Titanic&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/titanic_age_glm_plot_wide-1.png" width="672" /></p>
<p>No more predictions that a 200 year old has a <span class="math inline">\(-.1\)</span> probability of surviving!</p>
</div>
<div id="generating-predicted-probabilities" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Generating predicted probabilities</h3>
<p>To generate predicted probabilities, just substitute your specified values for <span class="math inline">\(X\)</span> in the original <span class="math inline">\(\Pr(\text{Survival})\)</span> equation. So for instance, the predicted probability of surviving the Titanic for a 30 year old is</p>
<p><span class="math display">\[
\begin{align}
\Pr(\text{Survival}) &amp;= \frac{e^{\beta_0 + \beta_{1} \times 30}}{1 + e^{\beta_0 + \beta_{1} \times 30}} \\
&amp;= \frac{e^{-0.0567236 + -0.0109635 \times 30}}{1 + e^{-0.0567236 + -0.0109635 \times 30}} \\
&amp;= 0.405
\end{align}
\]</span></p>
</div>
</div>
<div id="odds" class="section level2">
<h2><span class="header-section-number">2.2</span> Odds</h2>
<p>We can rewrite the probability function in the alternative form</p>
<p><span class="math display">\[\frac{\Pr(Y)}{1 - \Pr(Y)} = e^{\beta_0 + \beta_{1}X}\]</span></p>
<p>This quantity is called the <strong>odds</strong> and its range is <span class="math inline">\([0,\infty]\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob2odds &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  x <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_logit &lt;-<span class="st"> </span>sim_logit <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">odds =</span> <span class="kw">prob2odds</span>(prob))

<span class="co"># graph it</span>
<span class="kw">ggplot</span>(sim_logit, <span class="kw">aes</span>(x, odds)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;The logistic function&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="st">&quot;Odds of Y&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/odds-demo-1.png" width="672" /></p>
<p>Essentially it is the probability of one outcome divided by the probability of the other outcome. So in this example, it is</p>
<p><span class="math display">\[\frac{\Pr(\text{Survived})}{\Pr(\text{Died})}\]</span></p>
<p>So an odds of <span class="math inline">\(4\)</span> for an individual means they are 4 times more likely to survive than die, whereas an odds of <span class="math inline">\(\frac{1}{4}\)</span> means the odds of surviving are <span class="math inline">\(.25\)</span> as large as the odds of dying (i.e. the odds are higher you will die than survive).</p>
<div id="odds-of-surviving-the-titanic" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Odds of surviving the Titanic</h3>
<p>We can convert the probability of surviving the Titanic to the odds of surviving the Titanic.</p>
<p><span class="math display">\[\frac{\Pr(\text{Survival})}{1 - \Pr(\text{Survival})} = e^{\beta_0 + \beta_{1}\text{Age}}\]</span></p>
<p>The resulting graph is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_pred &lt;-<span class="st"> </span>survive_age_pred <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">odds =</span> <span class="kw">prob2odds</span>(prob))

<span class="kw">ggplot</span>(survive_age_pred, <span class="kw">aes</span>(Age, odds)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Odds of surviving the Titanic&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/titanic-odds-plot-1.png" width="672" /></p>
<p>Regardless of age, the odds of surviving the Titanic are always below 1. Considering the probability of even a 1 year old surviving was less than <span class="math inline">\(.50\)</span>, this should be expected. The relationship between age and the odds of survival is still curvilinear.</p>
</div>
</div>
<div id="log-odds" class="section level2">
<h2><span class="header-section-number">2.3</span> Log-odds</h2>
<p>By taking the logarithm of both sides of the odds function, we get</p>
<p><span class="math display">\[\log\left(\frac{\Pr(X)}{1 - \Pr(x)}\right) = \beta_0 + \beta_{1}X\]</span></p>
<p>The result is the <strong>log-odds</strong> or <strong>logit</strong>. Notice that this function now imposes a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(\Pr(Y)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob2logodds &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">log</span>(<span class="kw">prob2odds</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_logit &lt;-<span class="st"> </span>sim_logit <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">logodds =</span> <span class="kw">prob2logodds</span>(prob))

<span class="co"># graph it</span>
<span class="kw">ggplot</span>(sim_logit, <span class="kw">aes</span>(x, logodds)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;The logistic function&quot;</span>,
       <span class="dt">x =</span> <span class="kw">expression</span>(X),
       <span class="dt">y =</span> <span class="st">&quot;Log-odds of Y&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/log-odds-demo-1.png" width="672" /></p>
<p>A one-unit increase in <span class="math inline">\(X\)</span> corresponds to a monotonic increase or decrease in the log-odds of <span class="math inline">\(Y\)</span>. But because of the exponential transformation between log-odds and odds, <strong>a one-unit increase in <span class="math inline">\(X\)</span> corresponds to a differential increase or decrease in the odds of <span class="math inline">\(Y\)</span></strong>. The amount of the change in the odds of <span class="math inline">\(Y\)</span> depends on the initial value of <span class="math inline">\(X\)</span>. The directionality of the relationship is always the same, but not the magnitude. The same thing applies to the relationship between <span class="math inline">\(X\)</span> and the probability of <span class="math inline">\(Y\)</span>. This explains why we see curvilinear shapes for probabilities and odds, but not for log-odds.</p>
<div id="log-odds-of-surviving-the-titanic" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Log-odds of surviving the Titanic</h3>
<p>When you estimate a logistic regression model the log-odds function is actually the function for which you are estimating parameters.</p>
<p><span class="math display">\[\log\left(\frac{\Pr(\text{Survival})}{1 - \Pr(\text{Survival})}\right) = \beta_0 + \beta_{1}\text{Age}\]</span></p>
<p>Hence the parameters for the Titanic survival model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tidy</span>(survive_age)</code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  -0.0567   0.174      -0.327  0.744 
## 2 Age          -0.0110   0.00533    -2.06   0.0397</code></pre>
<p>are actually expressed in terms of log-odds – <strong>for every one-unit increase in age, we expect the log-odds of survival to decrease by 0.011</strong>.</p>
<p><span class="math display">\[\log\left(\frac{\Pr(\text{Survival})}{1 - \Pr(\text{Survival})}\right) = -0.0567236 + -0.0109635 \times \text{Age}\]</span></p>
<p>Generating predicted values from logistic regression models using <code>add_predictions()</code> always returns the log-odds of the outcome, so we can plot that directly to see the relationship between age and the log-odds of survival.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(survive_age_pred, <span class="kw">aes</span>(Age, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Log-odds of surviving the Titanic&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/log-odds-plot-1.png" width="672" /></p>
<p>While we can graph the log-odds, they don’t make any inherent sense. Nobody discusses the likelihood of events occuring in log-odds term. They are necessary to understanding and estimating logistic regression models, but are not directly interpretable.</p>
<div id="discussing-the-results-in-terms-of-probabilities" class="section level4">
<h4><span class="header-section-number">2.3.1.1</span> Discussing the results in terms of probabilities</h4>
<p>To interpret them in terms of probabilities, you need to specify your initial value for age. So for example, an increase in age from 20 to 30 is calculated by taking the <strong>first difference</strong>, or the expected change in probability given a particular change in the predictor. While it looks messy, all you do is calculate the predicted probability of survival for age 20 and 30 and calculate the difference.</p>
<p><span class="math display">\[
\begin{align}
\Pr(\text{Survival}_{30 - 20}) &amp;= \frac{e^{\beta_0 + \beta_{1}30}}{1 + e^{\beta_0 + \beta_{1}30}} - \frac{e^{\beta_0 + \beta_{1}20}}{1 + e^{\beta_0 + \beta_{1}20}} \\
&amp;= \frac{e^{-0.0567236 + -0.0109635 \times 30}}{1 + e^{-0.0567236 + -0.0109635 \times 30}} - \frac{e^{-0.0567236 + -0.0109635 \times 20}}{1 + e^{-0.0567236 + -0.0109635 \times 20}} \\
&amp;= 0.4047704 - 0.4314365 \\
&amp;= -0.0267
\end{align}
\]</span></p>
<p>However, compare this to the change in predicted probability of survival from age 40 to 50.</p>
<p><span class="math display">\[
\begin{align}
\Pr(\text{Survival}_{50 - 40}) &amp;= \frac{e^{\beta_0 + \beta_{1}50}}{1 + e^{\beta_0 + \beta_{1}50}} - \frac{e^{\beta_0 + \beta_{1}40}}{1 + e^{\beta_0 + \beta_{1}40}} \\
 &amp;= \frac{e^{-0.0567236 + -0.0109635 \times 50}}{1 + e^{-0.0567236 + -0.0109635 \times 50}} - \frac{e^{-0.0567236 + -0.0109635 \times 40}}{1 + e^{-0.0567236 + -0.0109635 \times 40}} \\
&amp;= 0.3532243 - 0.3786548 \\
&amp;= -0.0254
\end{align}
\]</span></p>
<p>Again, the relationship between age and the probability of survival is only weakly curvilinear, but you can already see that the first difference will depend on your initial starting value for age.</p>
</div>
</div>
</div>
<div id="estimating-the-parameters" class="section level2">
<h2><span class="header-section-number">2.4</span> Estimating the parameters</h2>
<p>Logistic regression is typically estimated using <a href="https://css18.github.io/parametric-inference.html#maximum_likelihood"><strong>maximum likelihood estimation</strong></a>.</p>
</div>
<div id="generating-predicted-probabilities-using-add_predictions" class="section level2">
<h2><span class="header-section-number">2.5</span> Generating predicted probabilities using <code>add_predictions()</code></h2>
<p>To visualise the predictions from a model, we start by generating an evenly spaced grid of values that covers the region where our data lies. First we use <code>data_grid()</code> from the <code>modelr</code> package to create a cleaned data frame of potential values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_age &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(Age)
titanic_age</code></pre></div>
<pre><code>## # A tibble: 88 x 1
##      Age
##    &lt;dbl&gt;
##  1  0.42
##  2  0.67
##  3  0.75
##  4  0.83
##  5  0.92
##  6  1   
##  7  2   
##  8  3   
##  9  4   
## 10  5   
## # … with 78 more rows</code></pre>
<p>Next we use the <code>add_predictions()</code> function to produce the predicted probabilities. This worked very well for linear models; unfortunately it is not perfect for logistic regression because as you I mentioned above, logistic regression directly estimates the <a href="https://wiki.lesswrong.com/wiki/Log_odds"><strong>log-odds</strong></a> for the outcome. Instead, we want the plain old predicted probability. To do this, we use this custom function to convert from log-odds to predicted probabilties:<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2prob &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">exp</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)

titanic_age &lt;-<span class="st"> </span>titanic_age <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred))
titanic_age</code></pre></div>
<pre><code>## # A tibble: 88 x 2
##      Age  pred
##    &lt;dbl&gt; &lt;dbl&gt;
##  1  0.42 0.485
##  2  0.67 0.484
##  3  0.75 0.484
##  4  0.83 0.484
##  5  0.92 0.483
##  6  1    0.483
##  7  2    0.480
##  8  3    0.478
##  9  4    0.475
## 10  5    0.472
## # … with 78 more rows</code></pre>
<p>With this information, we can now plot the logistic regression line using the estimated model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age, <span class="kw">aes</span>(Age, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Relationship between age and surviving the Titanic&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of survival&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/plot_pred-1.png" width="672" /></p>
</div>
<div id="multiple-predictors" class="section level2">
<h2><span class="header-section-number">2.6</span> Multiple predictors</h2>
<p>But as the old principle of the sea goes, <a href="https://en.wikipedia.org/wiki/Women_and_children_first">“women and children first”</a>. What if age isn’t the only factor effecting survival? Fortunately logistic regression handles multiple predictors in the form:</p>
<p><span class="math display">\[\Pr(Y) = \frac{e^{\beta_0 + \beta_{1}X_1 + \dots + \beta_{p}X_{p}}}{1 + e^{\beta_0 + \beta_{1}X_1 + \dots + \beta_{p}X_{p}}}\]</span></p>
<p>In our next model, let’s consider the relationship between age, sex, and survival. The logistic function takes the form:</p>
<p><span class="math display">\[\Pr(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                         <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age + Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7405  -0.6885  -0.6558   0.7533   1.8989  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.277273   0.230169   5.549 2.87e-08 ***
## Age         -0.005426   0.006310  -0.860     0.39    
## Sexmale     -2.465920   0.185384 -13.302  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 749.96  on 711  degrees of freedom
## AIC: 755.96
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The parameters essentially tell us the relationship between each individual predictor and the response, <strong>independent of other predictors</strong>. So this model tells us the relationship between age and survival, after controlling for the effects of sex. Likewise, it also tells us the relationship between sex and survival, after controlling for the effects of age. To get a better visualization of this, let’s use <code>data_grid()</code> and <code>add_predictions()</code> again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_age_sex &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(Age, Sex) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred))
titanic_age_sex</code></pre></div>
<pre><code>## # A tibble: 176 x 3
##      Age Sex     pred
##    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;
##  1  0.42 female 0.782
##  2  0.42 male   0.233
##  3  0.67 female 0.781
##  4  0.67 male   0.233
##  5  0.75 female 0.781
##  6  0.75 male   0.233
##  7  0.83 female 0.781
##  8  0.83 male   0.233
##  9  0.92 female 0.781
## 10  0.92 male   0.233
## # … with 166 more rows</code></pre>
<p>With these predicted probabilities, we can now plot the separate effects of age and sex:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age_sex, <span class="kw">aes</span>(Age, pred, <span class="dt">color =</span> Sex)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Probability of surviving the Titanic&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Age + Sex&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of survival&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Sex&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/survive_age_woman_plot-1.png" width="672" /></p>
<p>Substantively, this graph illustrates a key fact about surviving the sinking of the Titanic - age was not really a dominant factor. Instead, one’s sex was much more important. Females survived at much higher rates than males, regardless of age.</p>
</div>
<div id="predicted-probabilities-and-first-differences-in-multiple-variable-models" class="section level2">
<h2><span class="header-section-number">2.7</span> Predicted probabilities and first differences in multiple variable models</h2>
<p>Mathematically, the result is two lines that are parallel to each other <strong>in the log-odds functional form</strong>. The addition of the sex parameter acts like an second intercept when we compare age to probability of survival. All that happens is the line shifts up or down based on the estimated sex parameter.</p>
<p>In terms of predicted probabilities and first differences, <strong>this is not true</strong>. Remember the logistic function:</p>
<p><span class="math display">\[\Pr(Y) = \frac{e^{\beta_0 + \beta_{1}X_1 + \beta_{2}X_2}}{1 + e^{\beta_0 + \beta_{1}X_1 + \beta_{2}X_2}}\]</span></p>
<p>Because of the exponential transformation, <span class="math inline">\(\beta_2\)</span> now has a non-linear impact on the predicted probabilities. For clarity, let’s estimate a logistic regression model of the effect of age and fare on survival:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate logistic regression model of age and fare</span>
age_fare &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Fare, <span class="dt">data =</span> titanic, <span class="dt">family =</span> binomial)
<span class="kw">tidy</span>(age_fare)</code></pre></div>
<pre><code>## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  -0.417    0.186       -2.24 2.49e- 2
## 2 Age          -0.0176   0.00567     -3.10 1.92e- 3
## 3 Fare          0.0173   0.00262      6.60 4.23e-11</code></pre>
<p>And generate predicted log-odds and probabilities of survival for a series of hypothetical passengers, varying the fare amount paid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate predicted values</span>
age_fare_pred &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(Age, <span class="dt">Fare =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="kw">max</span>(Fare), <span class="dt">by =</span> <span class="dv">100</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(age_fare) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">Fare =</span> <span class="kw">factor</span>(Fare, <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">seq</span>(<span class="dv">0</span>, <span class="kw">max</span>(Fare), <span class="dt">by =</span> <span class="dv">100</span>))))</code></pre></div>
<p>Below is the plot of the log-odds for this regression model. To draw the line, all we did was draw the line for the relationship between age and expected log-odds of survival, holding fare constant at different levels.</p>
<p><span class="math display">\[\log\left(\frac{\Pr(\text{Survival})}{1 - \Pr(\text{Survival})}\right) = \beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Fare}\]</span></p>
<p><span class="math display">\[\log\left(\frac{\Pr(\text{Survival})}{1 - \Pr(\text{Survival})}\right) = (\beta_0 + \beta_{2}\text{Fare}) + \beta_{1}\text{Age}\]</span></p>
<p>Notice that each line is parallel to each other, because all that <span class="math inline">\(\hat{\beta}_{2}\)</span> did was change the intercept for the controlled relationship between age and log-odds of survival.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the new log-odds lines</span>
<span class="kw">ggplot</span>(age_fare_pred, <span class="kw">aes</span>(Age, pred, <span class="dt">group =</span> Fare, <span class="dt">color =</span> Fare)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Log-odds of survival&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Age + Fare model&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Log-odds of survival&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/fd-non-parallel-logodds-1.png" width="672" /></p>
<p>Now see what happens when we do the same thing but using predicted probabilities instead.</p>
<p><span class="math display">\[\Pr(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Fare}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Fare}}}\]</span></p>
<p><span class="math display">\[\Pr(\text{Survival}) = \frac{e^{(\beta_0 + \beta_{2}\text{Fare}) + \beta_{1}\text{Age}}}{1 + e^{(\beta_0 + \beta_{2}\text{Fare}) + \beta_{1}\text{Age}}}\]</span></p>
<p>Because of the exponential transformation, even if we hold fare constant, <strong>the resulting probability curve will non-linearly change for different fare values</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the new probability lines</span>
<span class="kw">ggplot</span>(age_fare_pred, <span class="kw">aes</span>(Age, prob, <span class="dt">group =</span> Fare, <span class="dt">color =</span> Fare)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Predicted probability of survival&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Age + Fare model&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of survival&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/fd-non-parallel-prob-1.png" width="672" /></p>
<p>So when calculating first differences and predicted probabilities for a single variable in a multiple variable logistic regression models, <strong>the values at which you hold the other variables constant will directly influence your results</strong>. The general rule of thumb is to hold the other variables constant at their <strong>median</strong> (if continuous) or <strong>modal</strong> (if discrete) values.</p>
</div>
<div id="interactive-terms" class="section level2">
<h2><span class="header-section-number">2.8</span> Interactive terms</h2>
<p>The additive assumption of linear regression also holds true for logistic regression: the relationships between predictors and responses are independent from one another. So for the age and sex example, we assume our function <span class="math inline">\(f\)</span> is:</p>
<p><span class="math display">\[\Pr(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex}}}\]</span></p>
<p>However once again, that is an assumption. What if the relationship between age and the probability of survival is actually dependent on whether or not the individual is a female? That is, the parameter for age is different for men and women? This possibility would take the functional form:</p>
<p><span class="math display">\[\Pr(\text{Survival}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex} + \beta_{3} \times \text{Age} \times \text{Sex}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{2}\text{Sex} + \beta_{3} \times \text{Age} \times \text{Sex}}}\]</span></p>
<p>This is considered an <strong>interaction</strong> between age and sex. To estimate this in R, we simply specify <code>Age * Sex</code> in our formula for the <code>glm()</code> function:<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman_x &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Age <span class="op">*</span><span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                           <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman_x)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9401  -0.7136  -0.5883   0.7626   2.2455  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.59380    0.31032   1.913  0.05569 . 
## Age          0.01970    0.01057   1.863  0.06240 . 
## Sexmale     -1.31775    0.40842  -3.226  0.00125 **
## Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 740.40  on 710  degrees of freedom
## AIC: 748.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>How can we interpret these parameters? Because the sex variable is binary (0 if female, 1 if male), it acts as a switch on the regression equation. So for women, the relationship between age and probability of survival is:<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p><span class="math display">\[\Pr(\text{Survival}_{female}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 0}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 0}}\]</span></p>
<p>which reduces to</p>
<p><span class="math display">\[\Pr(\text{Survival}_{female}) = \frac{e^{\beta_0 + \beta_{1}\text{Age}}}{1 + e^{\beta_0 + \beta_{1}\text{Age}}}\]</span></p>
<p>However for men, the resulting equation looks different:</p>
<p><span class="math display">\[\Pr(\text{Survival}_{male}) = \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 1}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age} \times 1}}\]</span></p>
<p>which reduces to</p>
<p><span class="math display">\[
\begin{align}
\Pr(\text{Survival}_{male}) &amp;= \frac{e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age}}}{1 + e^{\beta_0 + \beta_{1}\text{Age} + \beta_{3} \times \text{Age}}} \\
&amp;= \frac{e^{\beta_0 + (\beta_{1} + \beta_{3})\text{Age}}}{1 + e^{\beta_0 + (\beta_{1} + \beta_{3})\text{Age}}}
\end{align}
\]</span></p>
<p>Let’s plot the interactive effects of age and sex by estimating the log-odds and predicted probability of survival.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_age_sex_x &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(Age, Sex) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman_x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob =</span> <span class="kw">logit2prob</span>(pred))
titanic_age_sex_x</code></pre></div>
<pre><code>## # A tibble: 176 x 4
##      Age Sex      pred  prob
##    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1  0.42 female  0.602 0.646
##  2  0.42 male   -0.733 0.325
##  3  0.67 female  0.607 0.647
##  4  0.67 male   -0.738 0.323
##  5  0.75 female  0.609 0.648
##  6  0.75 male   -0.740 0.323
##  7  0.83 female  0.610 0.648
##  8  0.83 male   -0.742 0.323
##  9  0.92 female  0.612 0.648
## 10  0.92 male   -0.744 0.322
## # … with 166 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age_sex_x, <span class="kw">aes</span>(Age, pred, <span class="dt">color =</span> Sex)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Log-odds of surviving the Titanic&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Log-odds of survival&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Sex&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/age_woman_plot_logodds-1.png" width="672" /></p>
<p>For the log-odds (i.e. based directly on the parameter values), we can see that the interactive term changes the <strong>slope</strong> of the line for age. For women the slope of the line is positive (0.594), whereas for men the slope is negative (-0.724).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># join data frames of interactive and non-interactive model</span>
<span class="kw">bind_rows</span>(<span class="kw">list</span>(<span class="st">&quot;Age + Sex&quot;</span> =<span class="st"> </span>titanic_age_sex <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">rename</span>(<span class="dt">prob =</span> pred),
               <span class="st">&quot;Age x Sex&quot;</span> =<span class="st"> </span>titanic_age_sex_x), <span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># plot the two models</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Age, prob, <span class="dt">color =</span> Sex, <span class="dt">linetype =</span> id)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Probability of surviving the Titanic&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Age&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of survival&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Sex&quot;</span>,
       <span class="dt">linetype =</span> <span class="st">&quot;Model&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/age_woman_plot_prob-1.png" width="672" /></p>
<p>And now our minds are blown once again! For women, as age increases the probability of survival also increases. However for men, we see the opposite relationship: as age increases the probability of survival <strong>decreases</strong>. Again, the basic principle of saving women and children first can be seen empirically in the estimated probability of survival. Male children are treated similarly to female children, and their survival is prioritized. Even still, the probability of survival is always worse for men than women, but the regression function clearly shows a difference from our previous results.</p>
<p>You may think then that it makes sense to throw in interaction terms (and potentially quadratic terms) willy-nilly to all your regression models since we never know for sure if the relationship is strictly linear and independent. You could do that, but once you start adding more predictors (3, 4, 5, etc.) that will get very difficult to keep track of (five-way interactions are extremely difficult to interpret - even three-way get to be problematic). The best advice is to use theory and your domain knowledge as your guide. Do you have a reason to believe the relationship should be interactive? If so, test for it. If not, don’t.</p>
</div>
</div>
<div id="evaluating-model-accuracy" class="section level1">
<h1><span class="header-section-number">3</span> Evaluating model accuracy</h1>
<p>We need a method to evaluate the overall accuracy of a model. This allows us to determine if we have a good or bad model (at least based on predictive power), and compare alternative model specifications to each other. For classification methods such as logistic regression, there are a few different metrics we can use:</p>
<ul>
<li>Error rate</li>
<li>Proportional reduction in error</li>
<li>Receiver operating characteristics (ROC) curve and area under the curve (AUC)</li>
</ul>
<blockquote>
<p>For the purposes of simplification, here we use the training data to evaluate model accuracy. <a href="/notes/model-accuracy/#optimism-of-training-error">We know this is biased.</a> We will return to the issue of training/test error when we discuss resampling methods.</p>
</blockquote>
<div id="error-rate" class="section level2">
<h2><span class="header-section-number">3.1</span> Error rate</h2>
<p>One evalation criteria simply asks: how accurate are the predictions? For instance, how often did our basic model just using age correctly predict who survived and died? Typically this statistic is framed in terms of the <strong>error rate</strong>, as we want a quantity to be <strong>minimized</strong>. First we need to get the predicted probabilities for each individual in the original dataset and convert the probability to a prediction - that is, classify the individual as either a <strong>survivor</strong> or <strong>dead</strong>. The standard threshold for binary classification problems is <span class="math inline">\(.5\)</span>; any individual with a predicted probability greater than or equal to <span class="math inline">\(.5\)</span> would be classified as a 1 (or survivor), and any predicted probability less than <span class="math inline">\(.5\)</span> would be classified as a 0 (or dead).<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> Once we make our predictions, then we calculate what percentage of predictions were wrong.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_accuracy &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(age_accuracy<span class="op">$</span>Survived <span class="op">!=</span><span class="st"> </span>age_accuracy<span class="op">$</span>pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.4061625</code></pre>
<p><span class="math inline">\(40.6\%\)</span> of the predictions based on age only were not correct. Is this good or bad? Well, that depends on what you consider your <strong>baseline</strong>. It’s certainly better than an error rate of <span class="math inline">\(100\%\)</span>, or even <span class="math inline">\(50\%\)</span>, but is that what we should compare it to?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a function to calculate the modal value of a vector</span>
getmode &lt;-<span class="st"> </span><span class="cf">function</span>(v) {
   uniqv &lt;-<span class="st"> </span><span class="kw">unique</span>(v)
   uniqv[<span class="kw">which.max</span>(<span class="kw">tabulate</span>(<span class="kw">match</span>(v, uniqv)))]
}</code></pre></div>
<p>Instead we should compare the model’s error rate to a simple but useless classifier that predicts <strong>no individuals will survive the sinking of the Titanic</strong>. Because death is the <a href="https://en.wikipedia.org/wiki/Mode_(statistics)">modal</a> category, if we predict that every single person will die we would achieve an error rate of <span class="math inline">\(40.6\%\)</span>. Which coincidentially is exactly the accuracy rate for our age-only model. What gives?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic_age, <span class="kw">aes</span>(Age, pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Relationship between age and surviving the Titanic&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Predicted probability of survival&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/plot-pred2-1.png" width="672" /></p>
<p>Oh yeah. If you look at the graph, the predicted probability curve for age is always below <span class="math inline">\(.5\)</span> for the potential age range, so the model actually does act like a modal model - it always predicts death. So our fancy logistic regression model does no better than the useless classifier. What about our interactive age and sex model?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_accuracy &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman_x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">prob =</span> pred,
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(x_accuracy<span class="op">$</span>Survived <span class="op">!=</span><span class="st"> </span>x_accuracy<span class="op">$</span>pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.219888</code></pre>
<p>This model is much better. Just by knowing an individual’s age and sex, we can predict with 22% error whether he/she lives or dies.</p>
</div>
<div id="proportional-reduction-in-error" class="section level2">
<h2><span class="header-section-number">3.2</span> Proportional reduction in error</h2>
<p><strong>Proportional reduction in error</strong> (PRE) is a formalized way of comparing a model’s accuracy rate to the baseline. It is defined by</p>
<p><span class="math display">\[PRE = \frac{E_1 - E_2}{E_1}\]</span></p>
<p>where <span class="math inline">\(E_1\)</span> is the number of prediction errors in the null model (i.e. useless classifier) and <span class="math inline">\(E_2\)</span> is the number of prediction errors in the statistical learning model. It will range in value between <span class="math inline">\([0,1]\)</span> or <span class="math inline">\([0\%,100\%]\)</span>. <span class="math inline">\(0\%\)</span> means the statistical model reduced none of the prediction error, and <span class="math inline">\(100\%\)</span> means the statistical model eliminated all of the prediction error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to calculate PRE for a logistic regression model</span>
PRE &lt;-<span class="st"> </span><span class="cf">function</span>(model){
  <span class="co"># get the actual values for y from the data</span>
  y &lt;-<span class="st"> </span>model<span class="op">$</span>y
  
  <span class="co"># get the predicted values for y from the model</span>
  y.hat &lt;-<span class="st"> </span><span class="kw">round</span>(model<span class="op">$</span>fitted.values)
  
  <span class="co"># calculate the errors for the null model and your model</span>
  E1 &lt;-<span class="st"> </span><span class="kw">sum</span>(y <span class="op">!=</span><span class="st"> </span><span class="kw">median</span>(y))
  E2 &lt;-<span class="st"> </span><span class="kw">sum</span>(y <span class="op">!=</span><span class="st"> </span>y.hat)
  
  <span class="co"># calculate the proportional reduction in error</span>
  PRE &lt;-<span class="st"> </span>(E1 <span class="op">-</span><span class="st"> </span>E2) <span class="op">/</span><span class="st"> </span>E1
  <span class="kw">return</span>(PRE)
}</code></pre></div>
<p>So for the age-only model, the PRE is</p>
<p><span class="math display">\[
\begin{align}
PRE_{\text{Age}} &amp;= \frac{290 - 290}{290} \\
&amp;= \frac{0}{290} \\
&amp;= 0\%
\end{align}
\]</span></p>
<p>For the interactive age and sex model, the PRE is</p>
<p><span class="math display">\[
\begin{align}
PRE_{\text{Age x Sex}} &amp;= \frac{290 - 157}{290} \\
&amp;= \frac{133}{290} \\
&amp;= 45.9\%
\end{align}
\]</span></p>
<p>We’ve reduced the error from the useless classifier by <span class="math inline">\(45.9\%\)</span></p>
</div>
<div id="receiver-operating-characteristics-roc-curve" class="section level2">
<h2><span class="header-section-number">3.3</span> Receiver operating characteristics (ROC) curve</h2>
<div id="types-of-error" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Types of error</h3>
<p>Binary classifiers such as logistic regression make two types of error.</p>
<div class="figure">
<img src="https://marginalrevolution.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg" />

</div>
<p>Personally I hate the terms type I and type II because I never remember which is which. The above diagram may help, but instead let’s adopt the terms <strong>false positive</strong> and <strong>false negative</strong>.</p>
<p>In the context of the Titanic model, we could make two mistakes:</p>
<ol style="list-style-type: decimal">
<li>False positive - predict an individual survived the Titanic, when in fact the individual died.</li>
<li>False negative - predict an individual died on the Titanic, when in fact the individual survived.</li>
</ol>
</div>
<div id="confusion-matrix" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Confusion matrix</h3>
<p>Previously all we cared about was the overall accuracy rate of the model, but we may also care about the accuracy or error rate for both types of errors. To do that, we generate a <strong>confusion matrix</strong> using the <code>confusionMatrix()</code> function from the <code>caret</code> library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)  <span class="co"># load the caret package to use the confusionMatrix function</span></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cm_<span class="dv">5</span> &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(x_accuracy<span class="op">$</span>pred), <span class="kw">factor</span>(x_accuracy<span class="op">$</span>Survived))
cm_<span class="dv">5</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 360  93
##          1  64 197
##                                         
##                Accuracy : 0.7801        
##                  95% CI : (0.7479, 0.81)
##     No Information Rate : 0.5938        
##     P-Value [Acc &gt; NIR] : &lt; 2e-16       
##                                         
##                   Kappa : 0.5369        
##  Mcnemar&#39;s Test P-Value : 0.02544       
##                                         
##             Sensitivity : 0.8491        
##             Specificity : 0.6793        
##          Pos Pred Value : 0.7947        
##          Neg Pred Value : 0.7548        
##              Prevalence : 0.5938        
##          Detection Rate : 0.5042        
##    Detection Prevalence : 0.6345        
##       Balanced Accuracy : 0.7642        
##                                         
##        &#39;Positive&#39; Class : 0             
## </code></pre>
<p>The rows define the predicted outcomes and the columns define the actual (or reference) outcomes for individuals in the dataset. Ideally we want the off-diagonal cells to be 0 because we perfectly predicted all observations. However we can see that isn’t always the case with this model.</p>
</div>
<div id="alternative-thresholds" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Alternative thresholds</h3>
<p>Perhaps instead of boosting overall model accuracy, we instead care more about improving class-specific accuracy. For example, if we want to increase <strong>sensitivity/recall</strong> we need to increase the true positive rate (TPR):</p>
<p><span class="math display">\[TPR = \frac{\text{Number of actual positives correctly predicted}}{\text{Number of actual positives}}\]</span></p>
<p>However if we are concerned with increasing <strong>specificity</strong> we need to increase the true negative rate (TNR):</p>
<p><span class="math display">\[TNR = \frac{\text{Number of actual negatives correctly predicted}}{\text{Number of actual negatives}}\]</span></p>
<p>While the only way to achieve higher sensitivity <strong>and</strong> specificity is to estimate a better model, we can use our existing model to meet one of these goals. We do this by adjusting our <strong>threshold</strong>, or the cut-off point for classifying individuals as survivors or dead. The confusion matrix above also calculates the sensitivity and specificity of the predictive model. For the original threshold of <span class="math inline">\(.5\)</span>, our true positive rate (or sensitivity) was 0.849. What about for a threshold of <span class="math inline">\(.8\)</span>?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">threshold_<span class="dv">8</span> &lt;-<span class="st"> </span>titanic <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(survive_age_woman_x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">8</span>))

cm_<span class="dv">8</span> &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(threshold_<span class="dv">8</span><span class="op">$</span>pred), <span class="kw">factor</span>(threshold_<span class="dv">8</span><span class="op">$</span>Survived))
cm_<span class="dv">8</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 413 253
##          1  11  37
##                                           
##                Accuracy : 0.6303          
##                  95% CI : (0.5937, 0.6658)
##     No Information Rate : 0.5938          
##     P-Value [Acc &gt; NIR] : 0.02556         
##                                           
##                   Kappa : 0.1171          
##  Mcnemar&#39;s Test P-Value : &lt; 2e-16         
##                                           
##             Sensitivity : 0.9741          
##             Specificity : 0.1276          
##          Pos Pred Value : 0.6201          
##          Neg Pred Value : 0.7708          
##              Prevalence : 0.5938          
##          Detection Rate : 0.5784          
##    Detection Prevalence : 0.9328          
##       Balanced Accuracy : 0.5508          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>With a threshold of <span class="math inline">\(.8\)</span>, sensitivity increases to 0.974, but notice the trade-off: overall accuracy has decreased from 0.78 to 0.63, and specificity has also declined substantially. This reveals the trade-off we have to make between sensitivity and specificity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to calculate key stats for the Titanic data</span>
threshold_compare &lt;-<span class="st"> </span><span class="cf">function</span>(threshold, data, model){
  <span class="co"># generate predictions</span>
  pred &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">add_predictions</span>(model) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
           <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred <span class="op">&gt;</span><span class="st"> </span>threshold))
  
  <span class="co"># get confusion matrix</span>
  cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(pred<span class="op">$</span>pred), <span class="kw">factor</span>(pred<span class="op">$</span>Survived))
  
  <span class="co"># extract sensitivity and threshold to tibble</span>
  <span class="kw">tibble</span>(<span class="dt">threshold =</span> threshold,
             <span class="dt">sensitivity =</span> cm<span class="op">$</span>byClass[[<span class="st">&quot;Sensitivity&quot;</span>]],
             <span class="dt">specificity =</span> cm<span class="op">$</span>byClass[[<span class="st">&quot;Specificity&quot;</span>]],
             <span class="dt">accuracy =</span> cm<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]])
}

threshold_x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">by =</span> .<span class="dv">001</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">map_df</span>(threshold_compare, titanic, survive_age_woman_x) 

threshold_x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(measure, value, <span class="op">-</span>threshold) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">measure =</span> <span class="kw">factor</span>(measure, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Accuracy&quot;</span>, <span class="st">&quot;Sensitivity&quot;</span>, <span class="st">&quot;Specificity&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(threshold, value, <span class="dt">color =</span> measure, <span class="dt">linetype =</span> measure)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Threshold&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Accuracy rate&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;Measure&quot;</span>,
       <span class="dt">linetype =</span> <span class="st">&quot;Measure&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/threshold-compare-1.png" width="672" /></p>
<p>By graphing across many alternative thresholds, we can select a threshold level at the trade-off for which we are comfortable based on a visual inspection. But is that really the best approach? In order to make that determination we must have <strong>domain knowledge</strong> about about the benefits and costs to making that decision.</p>
</div>
<div id="roc-curve" class="section level3">
<h3><span class="header-section-number">3.3.4</span> ROC curve</h3>
<p>A receiver operating characteristic (ROC) curve is an alternative graphical method for comparing the types of errors for differing thresholds. In this graph, you plot the false positive rate vs. the true positive rate (i.e. 1- specificity vs. sensitivity) at different threshold values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(threshold_x, <span class="kw">aes</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity, sensitivity)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;grey&quot;</span>)</code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/roc-ggplot-1.png" width="672" /></p>
<p>The overall performance of the classifier across all potential thresholds is the <strong>area under the (ROC) curve</strong> (AUC). The ideal AUC curve hugs the top left corner of the graph, so a larger AUC indicates a better classifier. An AUC of <span class="math inline">\(1\)</span> means perfect prediction for any threshold value. The dashed line represents the null model where we randomly guess whether individuals are survivors or dead (i.e. flipped a coin) and would have an AUC of <span class="math inline">\(.5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)</code></pre></div>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">roc_x &lt;-<span class="st"> </span><span class="kw">roc</span>(x_accuracy<span class="op">$</span>Survived, x_accuracy<span class="op">$</span>prob)
<span class="kw">plot</span>(roc_x)   <span class="co"># use pROC to draw the ROC curve</span></code></pre></div>
<p><img src="/notes/logistic-regression_files/figure-html/roc-auc-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate the AUC</span>
auc_x &lt;-<span class="st"> </span><span class="kw">auc</span>(x_accuracy<span class="op">$</span>Survived, x_accuracy<span class="op">$</span>prob)
auc_x</code></pre></div>
<pre><code>## Area under the curve: 0.787</code></pre>
<p>With an AUC of 0.787, this model performs decently.</p>
</div>
</div>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1><span class="header-section-number">4</span> Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">session_info</span>()</code></pre></div>
<pre><code>## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.5.2 (2018-12-20)
##  os       macOS Mojave 10.14.2        
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2019-01-22                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package      * version    date       lib
##  assertthat     0.2.0      2017-04-11 [2]
##  backports      1.1.3      2018-12-14 [2]
##  bindr          0.1.1      2018-03-13 [2]
##  bindrcpp       0.2.2      2018-03-29 [1]
##  blogdown       0.9.4      2018-11-26 [1]
##  bookdown       0.9        2018-12-21 [1]
##  broom        * 0.5.1      2018-12-05 [2]
##  callr          3.1.1      2018-12-21 [2]
##  caret        * 6.0-81     2018-11-20 [1]
##  cellranger     1.1.0      2016-07-27 [2]
##  class          7.3-15     2019-01-01 [2]
##  cli            1.0.1      2018-09-25 [1]
##  codetools      0.2-16     2018-12-24 [2]
##  colorspace     1.3-2      2016-12-14 [2]
##  crayon         1.3.4      2017-09-16 [2]
##  data.table     1.11.8     2018-09-30 [2]
##  desc           1.2.0      2018-05-01 [2]
##  devtools       2.0.1      2018-10-26 [1]
##  digest         0.6.18     2018-10-10 [1]
##  dplyr        * 0.7.8      2018-11-10 [1]
##  e1071          1.7-0      2018-07-28 [1]
##  evaluate       0.12       2018-10-09 [2]
##  forcats      * 0.3.0      2018-02-19 [2]
##  foreach        1.4.4      2017-12-12 [2]
##  fs             1.2.6      2018-08-23 [1]
##  generics       0.0.2      2018-11-29 [1]
##  ggplot2      * 3.1.0      2018-10-25 [1]
##  glue           1.3.0      2018-07-17 [2]
##  gower          0.1.2      2017-02-23 [2]
##  gtable         0.2.0      2016-02-26 [2]
##  haven          2.0.0      2018-11-22 [2]
##  here           0.1        2017-05-28 [2]
##  hms            0.4.2      2018-03-10 [2]
##  htmltools      0.3.6      2017-04-28 [1]
##  httr           1.4.0      2018-12-11 [2]
##  ipred          0.9-8      2018-11-05 [1]
##  iterators      1.0.10     2018-07-13 [2]
##  jsonlite       1.6        2018-12-07 [2]
##  knitr        * 1.21       2018-12-10 [2]
##  lattice      * 0.20-38    2018-11-04 [2]
##  lava           1.6.4      2018-11-25 [2]
##  lazyeval       0.2.1      2017-10-29 [2]
##  lubridate      1.7.4      2018-04-11 [2]
##  magrittr       1.5        2014-11-22 [2]
##  MASS           7.3-51.1   2018-11-01 [2]
##  Matrix         1.2-15     2018-11-01 [2]
##  memoise        1.1.0      2017-04-21 [2]
##  ModelMetrics   1.2.2      2018-11-03 [2]
##  modelr       * 0.1.2      2018-05-11 [2]
##  munsell        0.5.0      2018-06-12 [2]
##  nlme           3.1-137    2018-04-07 [2]
##  nnet           7.3-12     2016-02-02 [2]
##  patchwork    * 0.0.1      2018-09-06 [1]
##  pillar         1.3.1      2018-12-15 [2]
##  pkgbuild       1.0.2      2018-10-16 [1]
##  pkgconfig      2.0.2      2018-08-16 [2]
##  pkgload        1.0.2      2018-10-29 [1]
##  plyr           1.8.4      2016-06-08 [2]
##  prettyunits    1.0.2      2015-07-13 [2]
##  pROC         * 1.13.0     2018-09-24 [1]
##  processx       3.2.1      2018-12-05 [2]
##  prodlim        2018.04.18 2018-04-18 [2]
##  ps             1.3.0      2018-12-21 [2]
##  purrr        * 0.2.5      2018-05-29 [2]
##  R6             2.3.0      2018-10-04 [1]
##  Rcpp           1.0.0      2018-11-07 [1]
##  readr        * 1.3.1      2018-12-21 [2]
##  readxl         1.2.0      2018-12-19 [2]
##  recipes        0.1.4      2018-11-19 [1]
##  remotes        2.0.2      2018-10-30 [1]
##  reshape2       1.4.3      2017-12-11 [2]
##  rlang          0.3.0.1    2018-10-25 [1]
##  rmarkdown      1.11       2018-12-08 [2]
##  rpart          4.1-13     2018-02-23 [1]
##  rprojroot      1.3-2      2018-01-03 [2]
##  rstudioapi     0.8        2018-10-02 [1]
##  rvest          0.3.2      2016-06-17 [2]
##  scales         1.0.0      2018-08-09 [1]
##  sessioninfo    1.1.1      2018-11-05 [1]
##  stringi        1.2.4      2018-07-20 [2]
##  stringr      * 1.3.1      2018-05-10 [2]
##  survival       2.43-3     2018-11-26 [2]
##  testthat       2.0.1      2018-10-13 [2]
##  tibble       * 2.0.0      2019-01-04 [2]
##  tidyr        * 0.8.2      2018-10-28 [2]
##  tidyselect     0.2.5      2018-10-11 [1]
##  tidyverse    * 1.2.1      2017-11-14 [2]
##  timeDate       3043.102   2018-02-21 [2]
##  usethis        1.4.0      2018-08-14 [1]
##  withr          2.1.2      2018-03-15 [2]
##  xfun           0.4        2018-10-23 [1]
##  xml2           1.2.0      2018-01-24 [2]
##  yaml           2.2.0      2018-07-25 [2]
##  source                              
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  Github (rstudio/blogdown@b2e1ed4)   
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.2)                      
##  Github (thomasp85/patchwork@7fb35b1)
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.1)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.2)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
##  CRAN (R 3.5.0)                      
## 
## [1] /Users/soltoffbc/Library/R/3.5/library
## [2] /Library/Frameworks/R.framework/Versions/3.5/Resources/library</code></pre>
</div>
<div id="references" class="section level1 toc-ignore">
<h1><span class="header-section-number">5</span> References</h1>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>General at least as applied to the Titanic. I’d like to think technology has advanced some since the early 20th century that the same patterns do not apply today. <a href="https://en.wikipedia.org/wiki/Costa_Concordia_disaster">Not that sinking ships have gone away.</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><code>library(titanic)</code> contains both <code>titanic_train</code> and <code>titanic_test</code>. We want to use only <code>titanic_train</code> today. We’ll discuss the importance of <code>titanic_test</code> next week.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Given Rose’s condition in the present-day of the film, it’s not outside the realm of possibility.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Alternatively, you may see the function written</p>
<p><span class="math display">\[\Pr(X) = \frac{1}{1 + e^{-(\beta_0 + \beta_{1}X)}}\]</span><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>The threshold can be adjusted depending on how conservative or risky of a prediction you wish to make.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Alternatively, we can use <code>broom::augment()</code> to add predicted probabilities for the original dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)

<span class="kw">augment</span>(survive_age, <span class="dt">newdata =</span> titanic, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>()</code></pre></div>
<pre><code>## # A tibble: 714 x 14
##    PassengerId Survived Pclass Name  Sex     Age SibSp Parch Ticket  Fare
##          &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;
##  1           1        0      3 Brau… male     22     1     0 A/5 2…  7.25
##  2           2        1      1 Cumi… fema…    38     1     0 PC 17… 71.3 
##  3           3        1      3 Heik… fema…    26     0     0 STON/…  7.92
##  4           4        1      1 Futr… fema…    35     1     0 113803 53.1 
##  5           5        0      3 Alle… male     35     0     0 373450  8.05
##  6           7        0      1 McCa… male     54     0     0 17463  51.9 
##  7           8        0      3 Pals… male      2     3     1 349909 21.1 
##  8           9        1      3 John… fema…    27     0     2 347742 11.1 
##  9          10        1      2 Nass… fema…    14     1     0 237736 30.1 
## 10          11        1      3 Sand… fema…     4     1     1 PP 95… 16.7 
## # … with 704 more rows, and 4 more variables: Cabin &lt;chr&gt;, Embarked &lt;chr&gt;,
## #   .fitted &lt;dbl&gt;, .se.fit &lt;dbl&gt;</code></pre>
<ul>
<li><code>newdata = titanic</code> - produces a data frame containing all the original variables + the predicted probability for the observation</li>
<li><code>type.predict = &quot;response&quot;</code> - ensures we get the predicted probabilities, not the logged version</li>
</ul>
<a href="#fnref6">↩</a></li>
<li id="fn7"><p>R automatically includes constituent terms, so this turns into <code>Age + Sex + Age * Sex</code>. <a href="https://pan-oxfordjournals-org.proxy.uchicago.edu/content/14/1/63.full.pdf+html">Generally you always want to include constituent terms in a regression model with an interaction.</a><a href="#fnref7">↩</a></p></li>
<li id="fn8"><p><span class="math inline">\(\beta_{2}\text{sex}\)</span> can be factored out since it does not contain age as a constituent term.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>We will discuss later on the effect of different threshold values.<a href="#fnref9">↩</a></p></li>
</ol>
</div>
