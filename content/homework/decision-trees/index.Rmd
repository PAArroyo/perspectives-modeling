---
title: "Homework 08: Tree-Based Inference"
date: 2019-02-27T13:30:00-06:00  # Schedule page publish date

draft: false
type: post

output:
  blogdown::html_page:
    number_sections: false

summary: "Implement methods for non-linear regression."
---

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}}

# Overview

Due by 11:59pm March 4th.

# Fork the `hw08` repository

Go [here](https://github.com/css-model/hw08) to fork the repo for homework 08.

# Submission format

For each of the following questions, produce brief written answers and/or the required graphs. Your responses must be readable on GitHub (i.e. we should not have to fork your repo to view the responses). The document should be **reproducible**. This means you need to commit and push all your code, output, and written text to complete the exercises. If necessary, I should be able to clone your repository and run all the code without any errors. Recommended document formats are Jupyter Notebook (`.ipynb`) or R Markdown (`.Rmd`) rendered as `pdf_document`.^[`html_document` cannot be viewed directly on GitHub.com, and GitHub does not properly render mathematical equations in `.md` documents `r emo::ji("sad")`]

# Conceptual exercises (4 points)

## Cost functions for classification trees

Consider the Gini index, classification error, and cross-entropy in a simple classification setting with two classes.

1. Create a single plot in Python or R that displays each of these quantities as a function of $\hat{p}_{m1}$. The $x$-axis should display $\hat{p}_{m1}$, ranging from 0 to 1, and the $y$-axis should display the value of the Gini index, classification error, and cross-entropy.
1. Of these three possible cost functions, which would be best to use when growing a decision tree? Which would be best to use when pruning a decision tree? Why?

## Predictions from tree-aggregation methods

Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each of the bootstrapped samples and, for a specific value of $X$, produce 10 estimates of $\Pr(\text{Class is Red} | X)$:
    
$$0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75$$
    
There are two common ways to combine these results together into a single class prediction. One is the majority vote approach, and the second is to classify based on the average probability.
    
1. What is the final classification under the majority vote approach?
1. What is the final classification based on the average probability?
1. What is $\Pr(\text{Class is Red} | X)$? That is, what is the probability this specific observation is "Red" given the 10 classification trees? Compare the result if you calculate the probability using the voting proportions from the majority vote approach versus averaging each of the individual classification tree predicted probabilities. Which approach is correct?

## Standardization using `xgboost`

Evaluate the performance of `xgboost` using `gss_train.csv`, predicting `egalit_scale` as a function of all the other covariates. Estimate two separate models with 1000 trees using 5-fold cross-validation (leave all other hyperparameter settings at their default values):

1. One with unstandardized predictors
1. One with standardized predictors

Record the time it takes your computer to estimate each model and compare the performance of each model based on the CV MSE.

1. Do the models perform substantially differently in terms of error? Why or why not?
1. Do the models perform substantially differently in terms of computational efficiency? Why or why not?

# Predicting attitudes towards racist college professors (6 points)

The [General Social Survey](http://gss.norc.org/) is a biannual survey of the American public.^[Conducted by NORC at the University of Chicago.]

> [The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years. The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.](http://gss.norc.org/About-The-GSS)

In this problem set, you are going to predict attitudes towards racist college professors. Specifically, each respondent was asked "Should a person who believes that Blacks are genetically inferior be allowed to teach in a college or university?" Given the kerfuffle over Richard J. Herrnstein and Charles Murray's [*The Bell Curve*](https://en.wikipedia.org/wiki/The_Bell_Curve) and the ostracization of Nobel Prize laureate [James Watson](https://en.wikipedia.org/wiki/James_Watson) over his controversial views on race and intelligence, this analysis will provide further insight into the public debate over this issue.

`gss_*.csv` contain a selection of variables from the 2012 GSS. The outcome of interest `colrac` is a binary variable coded as either `ALLOWED` or `NOT ALLOWED`. Documentation for the other predictors (if the variable is not clearly coded) can be viewed [here](https://gssdataexplorer.norc.org/variables/vfilter). Some data pre-processing has been done in advance for you to ease your model fitting:

1. Missing values have been imputed.
1. Categorical variables with low-frequency classes had those classes collapsed into an "other" category.
1. Nominal variables with more than two classes have been converted to dummy variables.
1. Remaining categorical variables have been converted to integer values, stripping their original labels.

Your mission is to construct a series of statistical/machine learning models to accurately predict an individual's attitude towards permitting individuals who view blacks to be racially inferior to teach in a college classroom. The learning objectives of this exercise are:

1. Implement statistical learning methods, including tree-based methods
1. Conduct hyperparameter tuning
1. Substantively interpret non-linear/non-regression models

## Estimate statistical models

Estimate statistical models using:

* Logistic regression
* Naive Bayes
* $K$-nearest neighbors
* Ridge regression
* Lasso regression
* Elastic net
* MARS
* Decision tree
* Bagging
* Random forest
* Boosting

Tune the relevant hyperparameters for each model as necessary. Only use the tuned model with the best performance for the remaining exercises. **Be sure to leave sufficient time for hyperparameter tuning.** Grid searches can be computationally taxing and take quite a while, especially for tree-aggregation methods.

## Evaluate statistical models

Compare each model's performance based on

* $5$-fold cross-validated error rate
* ROC/AUC
* Which is the best model? Defend your choice.

## Interpret the best model

Provide a substantive interpretation of the best model, using feature importance, PDPs/ICE, and local explanations (LIME). This should include (at minimum) 3-5 graphs, plus written analysis.

## Evaluate the best model

Evaluate the final model's performance on the test set using the classification error rate and ROC/AUC. Does the model generalize to this test set?
