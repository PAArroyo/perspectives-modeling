---
title: "Homework 09: Support Vector Machines"
date: 2019-03-05T13:30:00-06:00  # Schedule page publish date

draft: false
type: post

output:
  blogdown::html_page:
    number_sections: false

summary: "Implement support vector machines."
---

\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\se}{\text{se}}

# Overview

Due by 11:59pm March 11th.

# Fork the `hw09` repository

Go [here](https://github.com/css-model/hw09) to fork the repo for homework 09.

# Submission format

For each of the following questions, produce brief written answers and/or the required graphs. Your responses must be readable on GitHub (i.e. we should not have to fork your repo to view the responses). The document should be **reproducible**. This means you need to commit and push all your code, output, and written text to complete the exercises. If necessary, I should be able to clone your repository and run all the code without any errors. Recommended document formats are Jupyter Notebook (`.ipynb`) or R Markdown (`.Rmd`) rendered as `pdf_document`.^[`html_document` cannot be viewed directly on GitHub.com, and GitHub does not properly render mathematical equations in `.md` documents `r emo::ji("sad")`]

# Conceptual exercises (5 points)

## Non-linear separation

Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with $d > 1$) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training/test error rates in order to support your conclusions.

## SVM vs. logistic regression

We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.

1. Generate a data set with $n = 500$ and $p = 2$, such that the observations belong to two classes with a quadratic decision boundary between them.
1. Plot the observations, colored according to their class labels. Your plot should display $X_1$ on the $x$-axis and $X_2$ on the $y$-axis.
1. Fit a logistic regression model to the data, using $X_1$ and $X_2$ as predictors.
1. Apply this model to the **training** data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the **predicted** class labels. The decision boundary should be linear.
1. Now fit a logistic regression model to the data using non-linear functions of $X_1$ and $X_2$ as predictors (e.g. $X_1^2, X_1 \times X_2, \log(X_2)$, and so forth).
1. Apply this model to the **training** data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the **predicted** class labels. The decision boundary should be obviously non-linear. If it is not, then repeat steps 1-5 until you come up with an example in which the predicted class labels are obviously non-linear.
1. Fit a support vector classifier to the data with $X_1$ and $X_2$ as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the **predicted** class labels.
1. Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the **predicted** class labels.
1. Comment on your results.

## Tuning the cost parameter

In ISL, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of `cost` that misclassifies a couple of training observations may perform better on test data than one with a huge value of `cost` that does not misclassify any training observations. You will now investigate that claim.

1. Generate two-class data with $p=2$ in such a way that the classes are just barely linearly separable.
1. Compute the cross-validation error rates for support vector classifiers with a range of `cost` values. How many training errors are made for each value of `cost` considered, and how does this relate to the cross-validation errors obtained?
1. Generate an appropriate test data set, and compute the test errors corresponding to each of the values of `cost` considered. Which value of `cost` leads to the fewest test errors, and how does this compare to the values of `cost` that yield the fewest training errors and the fewest cross-validation errors?
1. Discuss your results.

# Predicting attitudes towards racist college professors (5 points)

The [General Social Survey](http://gss.norc.org/) is a biannual survey of the American public.^[Conducted by NORC at the University of Chicago.]

> [The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years. The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.](http://gss.norc.org/About-The-GSS)

In this problem set, you are going to predict attitudes towards racist college professors. Specifically, each respondent was asked "Should a person who believes that Blacks are genetically inferior be allowed to teach in a college or university?" Given the kerfuffle over Richard J. Herrnstein and Charles Murray's [*The Bell Curve*](https://en.wikipedia.org/wiki/The_Bell_Curve) and the ostracization of Nobel Prize laureate [James Watson](https://en.wikipedia.org/wiki/James_Watson) over his controversial views on race and intelligence, this analysis will provide further insight into the public debate over this issue.

`gss_*.csv` contain a selection of variables from the 2012 GSS. The outcome of interest `colrac` is a binary variable coded as either `ALLOWED` or `NOT ALLOWED`. Documentation for the other predictors (if the variable is not clearly coded) can be viewed [here](https://gssdataexplorer.norc.org/variables/vfilter). Some data pre-processing has been done in advance for you to ease your model fitting:

1. Missing values have been imputed.
1. Categorical variables with low-frequency classes had those classes collapsed into an "other" category.
1. Nominal variables with more than two classes have been converted to dummy variables.
1. Remaining categorical variables have been converted to integer values, stripping their original labels.

Your mission is to apply SVM-based models to the problem at hand.

1. Fit a support vector classifier to `colrac` using all available predictors, using 10-fold cross-validation to select an optimal value for `cost`. Report the CV errors associated with different values of parameters, and comment on your results.
1. Repeat (1) using SVMs with radial and polynomial basis kernels, with different values for `gamma` and `degree` and `cost`. Comment on your results.
1. Provide a substantive interpretation of the best model, using feature importance, PDPs/ICE, and local explanations (LIME). This should include (at minimum) 3-5 graphs, plus written analysis.
